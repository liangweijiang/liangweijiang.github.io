{"meta":{"title":"Hexo","subtitle":"","description":"","author":"liangweijiang","url":"https://liangweijiang.github.io","root":"/"},"pages":[{"title":"categories","date":"2020-02-19T09:53:30.000Z","updated":"2020-02-19T09:54:44.015Z","comments":true,"path":"categories/index.html","permalink":"https://liangweijiang.github.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2020-02-19T10:00:45.000Z","updated":"2020-02-19T10:01:19.886Z","comments":true,"path":"tags/index.html","permalink":"https://liangweijiang.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"每日一道算法之--颜色分类","slug":"algorithm-75","date":"2020-04-07T07:09:44.312Z","updated":"2020-04-07T07:10:34.680Z","comments":true,"path":"2020/04/07/algorithm-75/","link":"","permalink":"https://liangweijiang.github.io/2020/04/07/algorithm-75/","excerpt":"","text":"颜色分类力扣第75题 : https://leetcode-cn.com/problems/sort-colors/ 给定一个包含红色、白色和蓝色，一共 n 个元素的数组，原地对它们进行排序，使得相同颜色的元素相邻，并按照红色、白色、蓝色顺序排列。 此题中，我们使用整数 0、 1 和 2 分别表示红色、白色和蓝色。 注意:不能使用代码库中的排序函数来解决这道题。 示例: 输入: [2,0,2,1,1,0]输出: [0,0,1,1,2,2] 1. 计数排序这道题其实说白了就是排序,可以按照普通的排序直接做,但是它这里只会出现0, 1, 2 这三种数字,所以使用计数排序是一个很好的选择。 1234567891011class Solution: def sortColors(self, nums: List[int]) -&gt; None: bucket = [0] * 3 sorted_index = 0 for num in nums: bucket[num] += 1 for index, count in enumerate(bucket): while count &gt; 0: nums[sorted_index] = index sorted_index += 1 count -= 1 复杂度分析时间复杂度: 扫描一遍数组算出各个数字的次数,时间复杂度为O(N),然后在重写数组,所以总的时间为O(N) 空间复杂度: 用了一个列表来作为桶,长度为3,所以空间复杂度为O(1) 2. 一次遍历因为只有0, 1, 2 这三个数字,所以0肯定为最左边,2为最右边,我们使用两个指针，一个指向左，一个指向右边，当我们遍历数组时,如果当前数位0,则将其与左边的数交换,如果为2就和右边的数交换。我后来看了题解，是叫荷兰国旗问题。 12345678910111213141516171819class Solution: def sortColors(self, nums: List[int]) -&gt; None: \"\"\" Do not return anything, modify nums in-place instead. \"\"\" head, tail = 0, len(nums) - 1 index = 0 # 这里退出的条件是index而不是head while index &lt;= tail: if nums[index] == 0: nums[head], nums[index] = nums[index], nums[head] head += 1 # 这里索引也要加1,因为数组是从头开始排序的 index += 1 elif nums[index] == 2: nums[tail], nums[index] = nums[index], nums[tail] tail -= 1 else: index += 1 复杂度分析时间复杂度:由于对长度 N 的数组进行了一次遍历，时间复杂度为O(N)。 空间复杂度: 用了一个列表来作为桶,长度为3,所以空间复杂度为O(1)。 3. 三路快排 参考自 https://leetcode-cn.com/problems/sort-colors/solution/kuai-su-pai-xu-partition-guo-cheng-she-ji-xun-huan/作者：liweiwei1419 如果循环不变量是这样定义的：所有在子区间 [0, zero) 的元素都等于 0；所有在子区间 [zero, i) 的元素都等于 1；所有在子区间 [two, len - 1] 的元素都等于 2。 编码的时候，zero 和 two 初始化的值就应该保证上面的三个子区间全为空。 在遍历的过程中，“索引先加减再交换”、还是“先交换再加减”就看初始化的时候变量在哪里。 退出循环的条件也看上面定义的循环不变量，在 i == two 成立的时候，上面的三个子区间就正好“不重不漏”地覆盖了整个数组，并且给出的性质成立，题目的任务也就完成了。 1234567891011121314151617181920212223242526272829303132333435363738394041from typing import Listclass Solution: def sortColors(self, nums: List[int]) -&gt; None: \"\"\" Do not return anything, modify nums in-place instead. \"\"\" # all in [0, zero) = 0 # all in [zero, i) = 1 # all in [two, len - 1] = 2 def swap(nums, index1, index2): nums[index1], nums[index2] = nums[index2], nums[index1] size = len(nums) if size &lt; 2: return # 为了保证初始化的时候 [0, zero) 为空，设置 zero = 0， # 所以下面遍历到 0 的时候，先交换，再加 zero = 0 # 为了保证初始化的时候 [two, len - 1] 为空，设置 two = len # 所以下面遍历到 2 的时候，先减，再交换 two = size i = 0 # 当 i == two 上面的三个子区间正好覆盖了全部数组 # 因此，循环可以继续的条件是 i &lt; two while i &lt; two: if nums[i] == 0: swap(nums, i, zero) i += 1 zero += 1 elif nums[i] == 1: i += 1 else: two -= 1 swap(nums, i, two)","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://liangweijiang.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"排序","slug":"排序","permalink":"https://liangweijiang.github.io/tags/%E6%8E%92%E5%BA%8F/"}]},{"title":"数据结构学习笔记---散列表","slug":"data-structure-08","date":"2020-04-01T14:50:25.413Z","updated":"2020-04-01T14:51:11.791Z","comments":true,"path":"2020/04/01/data-structure-08/","link":"","permalink":"https://liangweijiang.github.io/2020/04/01/data-structure-08/","excerpt":"","text":"学习自极客时间的《数据结构与算法之美》 作者：王争 散列表散列表，又叫哈希表（Hash Table），是能够通过给定的关键字的值直接访问到具体对应的值的一个数据结构。也就是说，把关键字映射到一个表中的位置来直接访问记录，以加快访问速度。散列表用的就是数组支持按照下标随机访问的时候，时间复杂度是 O(1) 的特性。我们通过散列函数把元素的键值映射为下标，然后将数据存储在数组中对应下标的位置。当我们按照键值查询元素时，我们用同样的散列函数，将键值转化数组下标，从对应的数组下标的位置取数据。 1. 散列思想散列的思想, 其实就是利用数组的随机访问特性, 将key-value形式的数据, 其中的key转换成数组下标, 即可实现将其存放到数组中, 进而实现随机访问.散列表用的是数组支持按照下标随机访问数据的特性，所以散列表其实就是数组的一种扩展，由数组演化而来。可以说，如果没有数组，就没有散列表。 2. 散列函数散列函数，顾名思义，它是一个函数。我们可以把它定义成 hash(key)，其中 key 表示元素的键值，hash(key) 的值表示经过散列函数计算得到的散列值。散列函数设计的基本要求： 散列函数计算得出的值是一个正整数(数组下标嘛) 若key相等, 则计算后的哈希值相等 若key不相等, 则计算后的哈希值不相等，在真实的情况下，要想找到一个不同的 key 对应的散列值都不一样的散列函数，几乎是不可能的。即便像业界著名的MD5、SHA、CRC等哈希算法，也无法完全避免这种散列冲突。而且，因为数组的存储空间有限，也会加大散列冲突的概率。 2.1 构造散列函数的方法 散列函数的设计不能太复杂。过于复杂的散列函数，势必会消耗很多计算时间，也就间接的影响到散列表的性能。其次，散列函数生成的值要尽可能随机并且均匀分布，这样才能避免或者最小化散列冲突，而且即便出现冲突，散列到每个槽里的数据也会比较平均，不会出现某个槽内数据特别多的情况。 直接定址法：取关键字或关键字的某个线性函数值为散列地址。即 hash(k) = k 或 hash(k) = a * k + b, 其中ab为常数（这种散列函数叫做自身函数） 数字分析法：通过对数据的分析，发现数据中冲突较少的部分，并构造散列地址。假设关键字是以r为基的数，并且哈希表中可能出现的关键字都是事先知道的，则可取关键字的若干数位组成哈希地址。例如同学们的学号，通常同一届学生的学号，其中前面的部分差别不太大，所以用后面的部分来构造散列地址。 平方取中法：当无法确定关键字里哪几位的分布相对比较均匀时，可以先求出关键字的平方值，然后按需要取平方值的中间几位作为散列地址。这是因为：计算平方之后的中间几位和关键字中的每一位都相关，所以不同的关键字会以较高的概率产生不同的散列地址。 取随机数法：使用一个随机函数，取关键字的随机值作为散列地址，这种方式通常用于关键字长度不同的场合。 折叠法：将关键字分割成位数相同的几部分（最后一部分的位数可以不同），然后取这几部分的叠加和（舍去进位）作为哈希地址。 除留余数法：取关键字被某个不大于散列表表长m的数p除后所得的余数为散列地址。即 hash(k) = k mod p, p &lt;= m。不仅可以对关键字直接取模，也可在折叠法、平方取中法等运算之后取模。对的选择很重要，一般取素数或，若选择不好，容易产生冲突。 3. 散列冲突(一) 开放寻址法开放寻址法的核心思想是，如果出现了散列冲突，我们就重新探测一个空闲位置，将其插入。 (1) 线性探测法 插入: 当我们往散列表中插入数据时，如果某个数据经过散列函数散列之后，存储位置已经被占用了，我们就从当前位置开始，依次往后查找，看是否有空闲位置，直到找到为止。 查找: 我们通过散列函数求出要查找元素的键值对应的散列值，然后比较数组中下标为散列值的元素和要查找的元素。如果相等，则说明就是我们要找的元素；否则就顺序往后依次查找。如果遍历到数组中的空闲位置，还没有找到，就说明要查找的元素并没有在散列表中。 删除: 但是, 删除操作就比较麻烦了, 因为查找是通过空位来判断的, 若直接删除key, 就会在下次查找时出现空位而打断本来应该继续的查找. 对于这种情况, 我们可以将删除的空位标记为delete, 查找时遇到delete不会中断就好了. 线性探测法其实存在很大问题。当散列表中插入的数据越来越多时，散列冲突发生的可能性就会越来越大，空闲位置会越来越少，线性探测的时间就会越来越久。极端情况下，我们可能需要探测整个散列表，所以最坏情况下的时间复杂度为 O(n)。同理，在删除和查找时，也有可能会线性探测整张散列表，才能找到要查找或者删除的数据。 对于开放寻址冲突解决方法，除了线性探测方法之外，还有另外两种比较经典的探测方法，二次探测（Quadratic probing）和双重散列（Double hashing）。 (2) 二次探测（Quadratic probing）所谓二次探测，跟线性探测很像，线性探测每次探测的步长是 1，那它探测的下标序列就是 hash(key)+0，hash(key)+1，hash(key)+2……而二次探测探测的步长就变成了原来的“二次方”，也就是说，它探测的下标序列就是 hash(key)+0，hash(key)+12，hash(key)+22…… (3) 双重散列（Double hashing）所谓双重散列，意思就是不仅要使用一个散列函数。我们使用一组散列函数 hash1(key)，hash2(key)，hash3(key)……我们先用第一个散列函数，如果计算得到的存储位置已经被占用，再用第二个散列函数，依次类推，直到找到空闲的存储位置。 (二) 链表法使用链表法来解决哈希冲突相对来说更为常见一些, Java中的HashMap就是这么处理的。 当插入的时候，我们只需要通过散列函数计算出对应的散列槽位，将其插入到对应链表中即可，所以插入的时间复杂度是 O(1)。当查找、删除一个元素时，我们同样通过散列函数计算出对应的槽，然后遍历链表查找或者删除。 查找或删除操作的时间复杂度跟链表的长度 k 成正比，也就是 O(k)。对于散列比较均匀的散列函数来说，理论上讲，k=n/m，其中 n 表示散列中数据的个数，m 表示散列表中“槽”的个数。 （三） 再哈希法在产生冲突之后，使用关键字的其他部分继续计算地址，如果还是有冲突，则继续使用其他部分再计算地址。这种方式的缺点是时间增加了。 （四） 建立一个公共溢出区这种方式是建立一个公共溢出区，当地址存在冲突时，把新的地址放在公共溢出区里。 装载因子当散列表中空闲位置不多的时候，散列冲突的概率就会大大提高。为了尽可能保证散列表的操作效率，一般情况下，我们会尽可能保证散列表中有一定比例的空闲槽位。我们用装载因子（load factor）来表示空位的多少。 散列表的装载因子=填入表中的元素个数/散列表的长度装载因子越大，说明空闲位置越少，冲突越多，散列表的性能会下降。 问题思考1， Word 文档中单词拼写检查功能是如何实现的？ 常用的英文单词有 20 万个左右，假设单词的平均长度是 10 个字母，平均一个单词占用 10 个字节的内存空间，那 20 万英文单词大约占 2MB 的存储空间，就算放大 10 倍也就是 20MB。对于现在的计算机来说，这个大小完全可以放在内存里面。所以我们可以用散列表来存储整个英文单词词典。 当用户输入某个英文单词时，我们拿用户输入的单词去散列表中查找。如果查到，则说明拼写正确；如果没有查到，则说明拼写可能有误，给予提示。借助散列表这种数据结构，我们就可以轻松实现快速判断是否存在拼写错误 2， 假设我们有 10 万条 URL 访问日志，如何按照访问次数给 URL 排序？ 遍历 10 万条数据，以 URL 为 key，访问次数为 value，存入散列表，同时记录下访问次数的最大值 K，时间复杂度 O(N)。 如果 K 不是很大，可以使用桶排序，时间复杂度 O(N)。如果 K 非常大（比如大于 10 万），就使用快速排序，复杂度 O(NlogN)。 3， 有两个字符串数组，每个数组大约有 10 万条字符串，如何快速找出两个数组中相同的字符串？ 以第一个字符串数组构建散列表，key 为字符串，value 为出现次数。再遍历第二个字符串数组，以字符串为 key 在散列表中查找，如果 value 大于零，说明存在相同字符串。时间复杂度 O(N)。 4. 如何解决装载因子过大的问题 对于动态散列表来说，数据集合是频繁变动的，我们事先无法预估将要加入的数据个数，所以我们也无法事先申请一个足够大的散列表。随着数据慢慢加入，装载因子就会慢慢变大。当装载因子大到一定程度之后，散列冲突就会变得不可接受。 针对散列表，当装载因子过大时，我们也可以进行动态扩容，重新申请一个更大的散列表，将数据搬移到这个新散列表中。假设每次扩容我们都申请一个原来散列表大小两倍的空间。如果原来散列表的装载因子是 0.8，那经过扩容之后，新散列表的装载因子就下降为原来的一半，变成了 0.4。 针对数组的扩容，数据搬移操作比较简单。但是，针对散列表的扩容，数据搬移操作要复杂很多。因为散列表的大小变了，数据的存储位置也变了，所以我们需要通过散列函数重新计算每个数据的存储位置。 插入一个数据，最好情况下，不需要扩容，最好时间复杂度是 O(1)。最坏情况下，散列表装载因子过高，启动扩容，我们需要重新申请内存空间，重新计算哈希位置，并且搬移数据，所以时间复杂度是 O(n)。用摊还分析法，均摊情况下，时间复杂度接近最好情况，就是 O(1)。对于动态散列表，随着数据的删除，散列表中的数据会越来越少，空闲空间会越来越多。如果我们对空间消耗非常敏感，我们可以在装载因子小于某个值之后，启动动态缩容。 高效扩容如果散列表当前大小为 1GB，要想扩容为原来的两倍大小，那就需要对 1GB 的数据重新计算哈希值，并且从原来的散列表搬移到新的散列表，这会非常的耗时。 为了解决一次性扩容耗时过多的情况，我们可以将扩容操作穿插在插入操作的过程中，分批完成。当装载因子触达阈值之后，我们只申请新空间，但并不将老的数据搬移到新散列表中。 当有新数据要插入时，我们将新数据插入新散列表中，并且从老的散列表中拿出一个数据放入到新散列表。每次插入一个数据到散列表，我们都重复上面的过程。经过多次插入操作之后，老的散列表中的数据就一点一点全部搬移到新散列表中了。这样没有了集中的一次性数据搬移，插入操作就都变得很快了。 5. 选择冲突解决方法1， 开放寻址法 用开放寻址法解决冲突的散列表，删除数据的时候比较麻烦，需要特殊标记已经删除掉的数据。而且，在开放寻址法中，所有的数据都存储在一个数组中，比起链表法来说，冲突的代价更高。所以，使用开放寻址法解决冲突的散列表，装载因子的上限不能太大。 当数据量比较小、装载因子小的时候，适合采用开放寻址法。这也是 Java 中的ThreadLocalMap使用开放寻址法解决散列冲突的原因。 2，链表法 链表法比起开放寻址法，对大装载因子的容忍度更高。开放寻址法只能适用装载因子小于 1 的情况。接近 1 时，就可能会有大量的散列冲突，导致大量的探测、再散列等，性能会下降很多。但是对于链表法来说，只要散列函数的值随机均匀，即便装载因子变成 10，也就是链表的长度变长了而已，虽然查找效率有所下降，但是比起顺序查找还是快很多。 ** 对链表法稍加改造，可以实现一个更加高效的散列表。那就是，我们将链表法中的链表改造为其他高效的动态数据结构，比如跳表、红黑树**。这样，即便出现散列冲突，极端情况下，所有的数据都散列到同一个桶内，那最终退化成的散列表的查找时间也只不过是 O(logn)。 基于链表的散列冲突处理方法比较适合存储大对象、大数据量的散列表，而且，比起开放寻址法，它更加灵活，支持更多的优化策略，比如用红黑树代替链表。 Java中的HashMap1. 初始大小HashMap 默认的初始大小是 16，当然这个默认值是可以设置的，如果事先知道大概的数据量有多大，可以通过修改默认初始大小，减少动态扩容的次数，这样会大大提高 HashMap 的性能。 2. 装载因子和动态扩容最大装载因子默认是 0.75，当 HashMap 中元素个数超过 0.75*capacity（capacity 表示散列表的容量）的时候，就会启动扩容，每次扩容都会扩容为原来的两倍大小。 3. 散列冲突解决方法HashMap 底层采用链表法来解决冲突。即使负载因子和散列函数设计得再合理，也免不了会出现拉链过长的情况，一旦出现拉链过长，则会严重影响 HashMap 的性能。当链表长度太长（默认超过 8）时，链表就转换为红黑树。我们可以利用红黑树快速增删改查的特点，提高 HashMap 的性能。当红黑树结点个数少于 8 个的时候，又会将红黑树转化为链表。因为在数据量较小的情况下，红黑树要维护平衡，比起链表来，性能上的优势并不明显。 4. 散列函数 12345static final int hash(Object key) &#123; int hash; return key == null ? 0 : (hash = key.hashCode()) ^ hash &gt;&gt;&gt; 16; &#125; 其中，hashCode() 返回的是 Java 对象的 hash code。比如 String 类型的对象的 hashCode() 就是下面这样： 123456789101112public int hashCode() &#123; int var1 = this.hash; if(var1 == 0 &amp;&amp; this.value.length &gt; 0) &#123; char[] var2 = this.value; for(int var3 = 0; var3 &lt; this.value.length; ++var3) &#123; var1 = 31 * var1 + var2[var3]; &#125; this.hash = var1; &#125; return var1;&#125; 在插入或查找的时候，计算Key被映射到桶的位置： 1int index = hash(key) &amp; (capacity - 1) 首先hashcode本身是个32位整型值，在系统中，这个值对于不同的对象必须保证唯一（JAVA规范），这也是大家常说的，重写equals必须重写hashcode的重要原因。 获取对象的hashcode以后，先进行移位运算，然后再和自己做异或运算，即：hashcode ^ (hashcode &gt;&gt;&gt; 16)，这一步甚是巧妙，是将高16位移到低16位，这样计算出来的整型值将“具有”高位和低位的性质 最后，用hash表当前的容量减去一，再和刚刚计算出来的整型值做位与运算。进行位与运算，很好理解，是为了计算出数组中的位置。但这里有个问题：为什么要用容量减去一？因为 A % B = A &amp; (B - 1)，所以，(h ^ (h &gt;&gt;&gt; 16)) &amp; (capitity -1) = (h ^ (h &gt;&gt;&gt; 16)) % capitity，可以看出这里本质上是使用了「除留余数法」 综上，可以看出，hashcode的随机性，加上移位异或算法，得到一个非常随机的hash值，再通过「除留余数法」，得到index，","categories":[{"name":"随笔","slug":"随笔","permalink":"https://liangweijiang.github.io/categories/%E9%9A%8F%E7%AC%94/"}],"tags":[{"name":"散列表","slug":"散列表","permalink":"https://liangweijiang.github.io/tags/%E6%95%A3%E5%88%97%E8%A1%A8/"}]},{"title":"每日一道算法之--不同路径","slug":"algorithm-62-63","date":"2020-04-01T08:05:08.133Z","updated":"2020-04-07T07:27:10.559Z","comments":true,"path":"2020/04/01/algorithm-62-63/","link":"","permalink":"https://liangweijiang.github.io/2020/04/01/algorithm-62-63/","excerpt":"","text":"不同路径力扣第62题 : https://leetcode-cn.com/problems/unique-paths/ 一个机器人位于一个 m x n 网格的左上角 （起始点在下图中标记为“Start” ）。 机器人每次只能向下或者向右移动一步。机器人试图达到网格的右下角（在下图中标记为“Finish”）。 问总共有多少条不同的路径？ 示例 1: 输入: m = 3, n = 2输出: 3解释:从左上角开始，总共有 3 条路径可以到达右下角。 向右 -&gt; 向右 -&gt; 向下 向右 -&gt; 向下 -&gt; 向右 向下 -&gt; 向右 -&gt; 向右 示例 2: 输入: m = 7, n = 3输出: 28 动态规划我们可以假设dp[row][col]是到达第row行第col列的最多可能,那么达到第row行第col列只有两种可能 第row行col- 1列从左边向右移动一位 第row- 1行col列从上到下移动一位 所以状态转移方程为: dp[row][col] = dp[row][col - 1] + dp[row - 1][col]注意，对于第一行 dp[0][j]，或者第一列 dp[i][0]，由于都是在边界，所以只能为 1 12345678910class Solution: def uniquePaths(self, m: int, n: int) -&gt; int: # 边界处只能为1 dp = [[1]*n] + [[1]+[0] * (n-1) for _ in range(m-1)] print(dp) #print(dp) for row in range(1, m): for col in range(1, n): dp[row][col] = dp[row-1][col] + dp[row][col-1] return dp[-1][-1] 复杂度分析时间复杂度：O(m*n),每一种可能都要遍历到 空间复杂度：O(m * n) 优化动态规划可以发现二维数组占用了太多内存,分析题目发现,其实每一行的子问题只和最后一个结果相关,所以可以将二维数组简化为一维数组 1234567class Solution: def uniquePaths(self, m: int, n: int) -&gt; int: cur = [1] * n for row in range(1, m): for col in range(1, n): cur[col] += cur[col - 1] return cur[-1] 空间复杂度可以降为 O(n) 不同路径II力扣第63题 : https://leetcode-cn.com/problems/unique-paths-ii/ 一个机器人位于一个 m x n 网格的左上角 （起始点在下图中标记为“Start” ）。 机器人每次只能向下或者向右移动一步。机器人试图达到网格的右下角（在下图中标记为“Finish”）。 现在考虑网格中有障碍物。那么从左上角到右下角将会有多少条不同的路径？ 示例 1: 输入:[ [0,0,0], [0,1,0], [0,0,0]]输出: 2解释:3x3 网格的正中间有一个障碍物。从左上角到右下角一共有 2 条不同的路径： 向右 -&gt; 向右 -&gt; 向下 -&gt; 向下 向下 -&gt; 向下 -&gt; 向右 -&gt; 向右 动态规划这题和上面唯一的区别就是有了障碍物,这样我们就需要判断是否存在障碍物了,而且边界也要判断。 最初的版本123456789101112class Solution: def uniquePathsWithObstacles(self, obstacleGrid: List[List[int]]) -&gt; int: rows = len(obstacleGrid) cols = len(obstacleGrid[0]) dp = [1] + [0]* (cols - 1) for row in range(rows): for col in range(cols): if obstacleGrid[row][col] == 1: dp[col] = 0 else: dp[col] += dp[col - 1] return dp[-1] 这里会出现一个问题,就是当col 为 0 时, col - 1 为 -1 ,这样会取到了倒数第一个数据,从而造成错误 改进版本12345678910111213class Solution: def uniquePathsWithObstacles(self, obstacleGrid: List[List[int]]) -&gt; int: rows = len(obstacleGrid) cols = len(obstacleGrid[0]) # 在最后在加一个元素0,就可以解决边界问题 dp = [1] + [0]* cols for row in range(rows): for col in range(cols): if obstacleGrid[row][col] == 1: dp[col] = 0 else: dp[col] += dp[col - 1] return dp[-2] 力扣题解123456789101112131415161718192021222324252627282930313233class Solution(object): def uniquePathsWithObstacles(self, obstacleGrid): m = len(obstacleGrid) n = len(obstacleGrid[0]) ＃如果起始单元格有障碍物，则只需返回即可 ＃没有到达目的地的路径 if obstacleGrid[0][0] == 1: return 0 ＃到达起始像元的方式数量为 1 obstacleGrid[0][0] = 1 ＃填充第一列的值 for i in range(1,m): obstacleGrid[i][0] = int(obstacleGrid[i][0] == 0 and obstacleGrid[i-1][0] == 1) ＃填充第一行的值 for j in range(1, n): obstacleGrid[0][j] = int(obstacleGrid[0][j] == 0 and obstacleGrid[0][j-1] == 1) ＃从cell（1,1）开始填充值 ＃到达单元格[i] [j] =单元格[i-1] [j] +单元格[i] [j-1]的方式数量 ＃即从上方和左侧。 for i in range(1,m): for j in range(1,n): if obstacleGrid[i][j] == 0: obstacleGrid[i][j] = obstacleGrid[i-1][j] + obstacleGrid[i][j-1] else: obstacleGrid[i][j] = 0 ＃返回值存储在最右边的最底部单元格中。 那是目的地。 return obstacleGrid[m-1][n-1] 复杂度分析时间复杂度 ： O(M×N) 。长方形网格的大小是 M×N，而访问每个格点恰好一次。 空间复杂度 ： O(1)。我们利用 obstacleGrid 作为 DP 数组，因此不需要额外的空间。","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://liangweijiang.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"动态规划","slug":"动态规划","permalink":"https://liangweijiang.github.io/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"}]},{"title":"每日一道算法之--跳跃游戏","slug":"algorithm-55","date":"2020-04-01T07:30:30.186Z","updated":"2020-04-01T07:31:50.483Z","comments":true,"path":"2020/04/01/algorithm-55/","link":"","permalink":"https://liangweijiang.github.io/2020/04/01/algorithm-55/","excerpt":"","text":"跳跃游戏力扣第55题 : https://leetcode-cn.com/problems/jump-game/ 给定一个非负整数数组，你最初位于数组的第一个位置。 数组中的每个元素代表你在该位置可以跳跃的最大长度。 判断你是否能够到达最后一个位置。 示例 1: 输入: [2,3,1,1,4]输出: true解释: 我们可以先跳 1 步，从位置 0 到达 位置 1, 然后再从位置 1 跳 3 步到达最后一个位置。 示例 2: 输入: [3,2,1,0,4]输出: false解释: 无论怎样，你总会到达索引为 3 的位置。但该位置的最大跳跃长度是 0 ， 所以你永远不可能到达最后一个位置。 1. 回溯使用回溯法,可以将所有的情况列举出来,如果有一种情况满足条件,则返回真 1234567891011121314151617class Solution: tag = False def canJump(self, nums: List[int]) -&gt; bool: if not nums:return False lenght = len(nums) def backtrack(step,nums): if step &gt;= len(nums) - 1: # 如果有一个条件满足就返回真 self.tag = True return True for i in range(1, step + 1): backtrack(nums[i], nums[i:]) tag = backtrack(nums[0], nums) return True if self.tag else False 明显的,回溯方法没有剪枝,超出了时间限制 2. 动态规划可以使用动态规划,去除回溯 1234567891011121314151617class Solution: def canJump(self, nums: List[int]) -&gt; bool: if len(nums) &lt;= 1: return True dp = [0 for _ in range(len(nums) + 1)] dp[0] = nums[0] for pos in range(1, len(nums)): # 如果上一个索引的步数能够到达当前的索引位置 if dp[pos - 1] &gt;= pos: dp[pos] = max(dp[pos - 1], pos + nums[pos]) # 上一个索引的步数不能够到达当前的索引位置,则证明肯定达到不了最后一个元素 else: return False if dp[pos] &gt;= len(nums) - 1: return True return False 3. 贪心算法如果能到达某个位置，那一定能到达它前面的所有位置。 解法一:初始化最远位置为0，然后遍历数组，如果当前位置能到达，并且当前位置+跳数&gt;最远位置，就更新最远位置。最后比较最远位置和数组长度。 12345678class Solution: def canJump(self, nums: List[int]) -&gt; bool: max_i = 0 #初始化当前能到达最远的位置 for i, jump in enumerate(nums): #i为当前位置，jump是当前位置的跳数 if max_i &gt;= i and i+jump &gt; max_i: #如果当前位置能到达，并且当前位置+跳数&gt;最远位置 max_i = i+jump #更新最远能到达位置 return max_i&gt;=i 解法二:用current记录还能往前进多少格，每次进一current则为current-1和当前所处位置的值中的较大值。当current为0时，代表无法前进，返回False。反之则为True 12345678class Solution: def canJump(self, nums: List[int]) -&gt; bool: current=0 for i in range(len(nums)-1):#最后一格不用检测 current=max(current-1,nums[i]) if current==0:return False return True","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://liangweijiang.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"动态规划","slug":"动态规划","permalink":"https://liangweijiang.github.io/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"},{"name":"回溯","slug":"回溯","permalink":"https://liangweijiang.github.io/tags/%E5%9B%9E%E6%BA%AF/"},{"name":"贪心","slug":"贪心","permalink":"https://liangweijiang.github.io/tags/%E8%B4%AA%E5%BF%83/"}]},{"title":"数据结构学习笔记---跳表","slug":"data-structure-07","date":"2020-03-31T13:14:56.074Z","updated":"2020-03-31T13:15:45.065Z","comments":true,"path":"2020/03/31/data-structure-07/","link":"","permalink":"https://liangweijiang.github.io/2020/03/31/data-structure-07/","excerpt":"","text":"学习自极客时间的《数据结构与算法之美》 作者：王争 跳表跳表是一种动态数据结构，支持快速的插入、删除、查找操作，时间复杂度都是 O(logn)。目前开源软件 Redis 和 LevelDB 都有用到它，它的效率和红黑树以及 AVL 树不相上下，但跳表的原理相当简单，可以支持快速的插入、删除、查找操作。Redis 中的有序集合（Sorted Set）就是用跳表来实现的。 跳表的实现1. 单链表对于一个单链表来讲，即便链表中存储的数据是有序的，如果我们要想在其中查找某个数据，也只能从头到尾遍历链表。这样查找效率就会很低，时间复杂度会很高，是 O(n)。 2. 建立一级索引如果是说链表是排序的，并且节点中还存储了指向前面第二个节点的指针的话，那么在查找一个节点时，仅仅需要遍历N/2个节点即可。 3. 多层索引在第一级索引的基础之上，每两个结点就抽出一个结点到第二级索引。现在我们再来查找莫个数，只需要遍历需要遍历的结点数量又减少了。 通过建立多级索引，从而使得链表能够实现二分查找。 跳表的时间复杂度在一个单链表中查询某个数据的时间复杂度是 O(n)。那在一个具有多级索引的跳表中，查询某个数据的时间复杂度是多少呢？ 每两个结点会抽出一个结点作为上一级索引的结点，那第一级索引的结点个数大约就是 n/2，第二级索引的结点个数大约就是 n/4，第三级索引的结点个数大约就是 n/8，依次类推，也就是说，第 k 级索引的结点个数是第 k-1 级索引的结点个数的 1/2，那第 k级索引结点的个数就是 n/(2k)。 假设索引有 h 级，最高级的索引有 2 个结点。通过上面的公式，我们可以得到 n/(2h)=2，从而求得 h=log2n-1。如果包含原始链表这一层，整个跳表的高度就是 log2n。我们在跳表中查询某个数据的时候，如果每一层都要遍历 m 个结点，那在跳表中查询一个数据的时间复杂度就是 O(m*logn)。 假设我们要查找的数据是 x，在第 k 级索引中，我们遍历到 y 结点之后，发现 x 大于 y，小于后面的结点 z，所以我们通过 y 的 down 指针，从第 k 级索引下降到第 k-1 级索引。在第 k-1 级索引中，y 和 z 之间只有 3 个结点（包含 y 和 z），所以，我们在 K-1 级索引中最多只需要遍历 3 个结点，依次类推，每一级索引都最多只需要遍历 3 个结点，也就是说** m=3。** 通过上面的分析，我们得到 m=3，所以在跳表中查询任意数据的时间复杂度就是 O(logn)。这个查找的时间复杂度跟二分查找是一样的。 3. 跳表的空间复杂度假如原始链表包含 n 个元素，则一级索引元素个数为 n/2、二级索引元素个数为 n/4、三级索引元素个数为 n/8 以此类推。所以，索引节点的总和是：n/2 + n/4 + n/8 + … + 8 + 4 + 2 = n-2，空间复杂度是 O(n)。 如下图所示：如果每三个结点抽一个结点做为索引，索引总和数就是 n/3 + n/9 + n/27 + … + 9 + 3 + 1= n/2，减少了一半。 在实际的软件开发中，原始链表中存储的有可能是很大的对象，而索引结点只需要存储关键值和几个指针，并不需要存储对象，所以当对象比索引结点大很多时，那索引占用的额外空间就可以忽略了。 举个例子：我们现在需要用跳表来给所有学生建索引，学生有很多属性：学号、姓名、性别、身份证号、年龄、家庭住址、身高、体重等。学生的各种属性只需要在原始链表中存储一份即可，我们只需要用学生的学号（int 类型的数据）建立索引，所以索引相对原始数据而言，占用的空间可以忽略。 4. 插入数据实际上，跳表这个动态数据结构，不仅支持查找操作，还支持动态的插入、删除操作，而且插入、删除操作的时间复杂度也是 O(logn)。 在单链表中，一旦定位好要插入的位置，插入结点的时间复杂度是很低的，就是 O(1)。但是，这里为了保证原始链表中数据的有序性，我们需要先找到要插入的位置，这个查找操作就会比较耗时。 对于跳表来说，查找某个结点的的时间复杂度是 O(logn)，所以这里查找某个数据应该插入的位置，方法也是类似的，时间复杂度也是 O(logn)。 5. 删除数据跳表中，每一层索引其实都是一个有序的单链表，单链表删除元素的时间复杂度为 O(1)，索引层数为 logn 表示最多需要删除 logn 个元素，所以删除元素的总时间包含 查找元素的时间 加 删除 logn个元素的时间 为 O(logn) + O(logn) = 2 O(logn)，忽略常数部分，删除元素的时间复杂度为 O(logn)。 6. 跳表索引动态更新当我们不停地往跳表中插入数据时，如果我们不更新索引，就有可能出现某 2 个索引结点之间数据非常多的情况。极端情况下，跳表还会退化成单链表。 红黑树、AVL 树这样平衡二叉树，是通过左右旋的方式保持左右子树的大小平衡，而跳表是通过随机函数来维护前面提到的“平衡性”。 我们通过一个随机函数，来决定将这个结点插入到哪几级索引中，比如随机函数生成了值 K，那我们就将这个结点添加到第一级到第 K 级这 K 级索引中。 过程如下： 跳表的实现代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118import randomclass SkipListNode(object): def __init__(self, val, high=1): # 节点存储的值 self.data = val # 节点对应索引层的深度 self.deeps = [None] * highclass SkipList(object): \"\"\" An implementation of skip list. The list stores positive integers without duplicates. 跳表的一种实现方法。 跳表中储存的是正整数，并且储存的是不重复的。 Author: Ben \"\"\" def __init__(self): # 索引层的最大深度 self.__MAX_LEVEL = 16 # 跳表的高度 self._high = 1 # 每一索引层的首节点, 默认值为None self._head = SkipListNode(None, self.__MAX_LEVEL) def find(self, val): cur = self._head # 从索引的顶层, 逐层定位要查找的值 # 索引层上下是对应的, 下层的起点是上一个索引层中小于插入值的最大值对应的节点 for i in range(self._high - 1, -1, -1): # 同一索引层内, 查找小于插入值的最大值对应的节点 while cur.deeps[i] and cur.deeps[i].data &lt; val: cur = cur.deeps[i] if cur.deeps[0] and cur.deeps[0].data == val: return cur.deeps[0] return None def insert(self, val): ''' 新增时, 通过随机函数获取要更新的索引层数, 要对低于给定高度的索引层添加新结点的指针 ''' high = self.randomLevel() if self._high &lt; high: self._high = high # 申请新结点 newNode = SkipListNode(val, high) # cache用来缓存对应索引层中小于插入值的最大节点 cache = [self._head] * high cur = self._head # 在低于随机高度的每一个索引层寻找小于插入值的节点 for i in range(high - 1, -1, -1): # 每个索引层内寻找小于带插入值的节点 # ! 索引层上下是对应的, 下层的起点是上一个索引层中小于插入值的最大值对应的节点 while cur.deeps[i] and cur.deeps[i].data &lt; val: cur = cur.deeps[i] cache[i] = cur # 在小于高度的每个索引层中插入新结点 for i in range(high): # new.next = prev.next \\ prev.next = new.next newNode.deeps[i] = cache[i].deeps[i] cache[i].deeps[i] = newNode def delete(self, val): ''' 删除时, 要将每个索引层中对应的节点都删掉 ''' # cache用来缓存对应索引层中小于插入值的最大节点 cache = [None] * self._high cur = self._head # 缓存每一个索引层定位小于插入值的节点 for i in range(self._high - 1, -1, -1): while cur.deeps[i] and cur.deeps[i].data &lt; val: cur = cur.deeps[i] cache[i] = cur # 如果给定的值存在, 更新索引层中对应的节点 if cur.deeps[0] and cur.deeps[0].data == val: for i in range(self._high): if cache[i].deeps[i] and cache[i].deeps[i].data == val: cache[i].deeps[i] = cache[i].deeps[i].deeps[i] def randomLevel(self, p=0.25): ''' #define ZSKIPLIST_P 0.25 /* Skiplist P = 1/4 */ https://github.com/antirez/redis/blob/unstable/src/t_zset.c ''' high = 1 for _ in range(self.__MAX_LEVEL - 1): if random.random() &lt; p: high += 1 return high def __repr__(self): vals = [] p = self._head while p.deeps[0]: vals.append(str(p.deeps[0].data)) p = p.deeps[0] return '-&gt;'.join(vals)if __name__ == '__main__': sl = SkipList() for i in range(100): sl.insert(i) print(sl) p = sl.find(7) print(p.data) sl.delete(37) print(sl) sl.delete(37.5) print(sl)","categories":[{"name":"随笔","slug":"随笔","permalink":"https://liangweijiang.github.io/categories/%E9%9A%8F%E7%AC%94/"}],"tags":[{"name":"跳表","slug":"跳表","permalink":"https://liangweijiang.github.io/tags/%E8%B7%B3%E8%A1%A8/"}]},{"title":"数据结构学习笔记---线性排序","slug":"data-structure-06","date":"2020-03-30T10:37:12.557Z","updated":"2020-03-30T10:38:48.565Z","comments":true,"path":"2020/03/30/data-structure-06/","link":"","permalink":"https://liangweijiang.github.io/2020/03/30/data-structure-06/","excerpt":"","text":"学习自极客时间的《数据结构与算法之美》 作者：王争 1. 桶排序（Bucket sort）桶排序，顾名思义，会用到“桶”，核心思想是将要排序的数据分到几个有序的桶里，每个桶里的数据再单独进行排序。桶内排完序之后，再把每个桶里的数据按照顺序依次取出，组成的序列就是有序的了。 12345678910111213141516def bucketSort(nums): # 选择一个最大的数 max_num = max(nums) # 创建一个元素全是0的列表, 当做桶 bucket = [0]*(max_num+1) # 把所有元素放入桶中, 即把对应元素个数加一 for i in nums: bucket[i] += 1 # 存储排序好的元素 sort_nums = [] # 取出桶中的元素 for j in range(len(bucket)): if bucket[j] != 0: for y in range(bucket[j]): sort_nums.append(j) return sort_nums 桶排序的时间复杂度如果要排序的数据有 n 个，我们把它们均匀地划分到 m 个桶内，每个桶里就有 k=n/m 个元素。每个桶内部使用快速排序，时间复杂度为 O(k * logk)。m 个桶排序的时间复杂度就是 O(m * k * logk)，因为 k=n/m，所以整个桶排序的时间复杂度就是 O(n*log(n/m))。当桶的个数 m 接近数据个数 n 时，log(n/m) 就是一个非常小的常量，这个时候桶排序的时间复杂度接近 O(n)。 桶排序的缺点桶排序对要排序数据的要求是非常苛刻的。 首先，要排序的数据需要很容易就能划分成 m 个桶，并且，桶与桶之间有着天然的大小顺序。这样每个桶内的数据都排序完之后，桶与桶之间的数据不需要再进行排序。 其次，数据在各个桶之间的分布是比较均匀的。如果数据经过桶的划分之后，有些桶里的数据非常多，有些非常少，很不平均，那桶内数据排序的时间复杂度就不是常量级了。在极端情况下，如果数据都被划分到一个桶里，那就退化为 O(nlogn) 的排序算法了。 桶排序比较适合用在外部排序中。所谓的外部排序就是数据存储在外部磁盘中，数据量比较大，内存有限，无法将数据全部加载到内存中。 2. 计数排序（Counting sort）计数排序其实是桶排序的一种特殊情况。当要排序的 n 个数据，所处的范围并不大的时候，比如最大值是 k，我们就可以把数据划分成 k 个桶。每个桶内的数据值都是相同的，省掉了桶内排序的时间。 12345678910111213141516def countingSort(arr): max_value = max(arr) bucket_len = max_value + 1 # 每一个数都申请一个桶 bucket = [0] * bucket_len sorted_index = 0 arr_len = len(arr) for i in range(arr_len): # 索引号为该数字时,计数加一 bucket[arr[i]] += 1 for j in range(bucket_len): while bucket[j] &gt; 0: arr[sorted_index] = j sorted_index += 1 bucket[j] -= 1 return arr 计数排序的优化如何快速计算出，每个数在有序数组中对应的存储位置呢？这个处理方法非常巧妙，很不容易想到。假设只有 8 个数字，分数在 0 到 5 分之间。这 8 个数字的值我们放在一个数组 A[8]中，它们分别是：2，5，3，0，2，3，0，3。 我们使用大小为 6 的数组 C[6]表示桶，其中下标对应分数。不过，C[6]内存储的并不是考生，而是对应的考生个数。像我刚刚举的那个例子，我们只需要遍历一遍考生分数，就可以得到 C[6]的值。 分数为 3 分的考生有 3 个，小于 3 分的考生有 4 个，所以，成绩为 3 分的考生在排序之后的有序数组 R[8]中，会保存下标 4，5，6 的位置。 我们对 C[6]数组顺序求和，C[6]存储的数据就变成了下面这样子。C[k]里存储小于等于分数 k 的考生个数。 从后到前依次扫描数组 A。比如，当扫描到 3 时，我们可以从数组 C 中取出下标为 3 的值 7，也就是说，到目前为止，包括自己在内，分数小于等于 3 的考生有 7 个，也就是说 3 是数组 R 中的第 7 个元素（也就是数组 R 中下标为 6 的位置）。当 3 放入到数组 R 中后，小于等于 3 的元素就只剩下了 6 个了，所以相应的 C[3]要减 1，变成 6。 12345678910111213141516171819202122from typing import Listimport itertoolsdef counting_sort(a: List[int]): if len(a) &lt;= 1: return # a中有counts[i]个数不大于i counts = [0] * (max(a) + 1) for num in a: counts[num] += 1 # 对数组顺序求和 counts = list(itertools.accumulate(counts)) # 临时数组，储存排序之后的结果 a_sorted = [0] * len(a) for num in reversed(a): index = counts[num] - 1 a_sorted[index] = num counts[num] -= 1 a[:] = a_sorted 计数排序只能用在数据范围不大的场景中，如果数据范围 k 比要排序的数据 n 大很多，就不适合用计数排序了。而且，计数排序只能给非负整数排序，如果要排序的数据是其他类型的，要将其在不改变相对大小的情况下，转化为非负整数。 3. 基数排序（Radix sort）基数排序是按照低位先排序，然后收集；再按照高位排序，然后再收集；依次类推，直到最高位。有时候有些属性是有优先级顺序的，先按低优先级排序，再按高优先级排序。最后的次序就是高优先级高的在前，高优先级相同的低优先级高的在前。 123456789101112def radix_sort(res, d): for k in range(d): # d轮排序 # 每一轮生成10个列表 s = [[] for i in range(10)] # 因为每一位数字都是0~9，故建立10个桶 for i in res: # 按第k位放入到桶中 s[i // (10 ** k) % 10].append(i) # 按当前桶的顺序重排列表 cur = [j for i in s for j in i] # 更新原数组 res[:] = cur return res 基数排序对要排序的数据是有要求的，需要可以分割出独立的“位”来比较，而且位之间有递进的关系，如果 a 数据的高位比 b 数据大，那剩下的低位就不用比较了。除此之外，每一位的数据范围不能太大，要可以用线性排序算法来排序，否则，基数排序的时间复杂度就无法做到 O(n) 了。解答开篇 总结","categories":[{"name":"随笔","slug":"随笔","permalink":"https://liangweijiang.github.io/categories/%E9%9A%8F%E7%AC%94/"}],"tags":[{"name":"排序","slug":"排序","permalink":"https://liangweijiang.github.io/tags/%E6%8E%92%E5%BA%8F/"}]},{"title":"《趣谈网络协议》学习笔记之--陌生的数据中心(上)","slug":"NetworkProtocol-08","date":"2020-03-30T07:23:58.866Z","updated":"2020-03-30T08:47:05.872Z","comments":true,"path":"2020/03/30/NetworkProtocol-08/","link":"","permalink":"https://liangweijiang.github.io/2020/03/30/NetworkProtocol-08/","excerpt":"","text":"学习自极客时间《趣谈网络协议》 作者：刘超 1. DNS协议1.1 DNS域名结构一. 域名的层次结构 像Linux目录结构一样，现代因特网采用层次树状结构的命名方法，任何一个连接在因特网上的主机或路由器，都有一个唯一的层次结构的名字，该名字称为域名。 每一个域名（英文域名）都是一个标号序列（labels），用字母（A-Z，a-z，大小写等价）、数字（0-9）和连接符（-）组成，标号序列总长度不能超过255个字符，它由点号分割成一个个的标号（label），每个标号应该在63个字符之内，每个标号都可以看成一个层次的域名。级别最低的域名写在左边，级别最高的域名写在右边。 域名服务主要是基于UDP实现的，服务器的端口号为53。 二. 域名的分级域名可以划分为各个子域，子域还可以继续划分为子域的子域，这样就形成了顶级域、二级域、三级域等。 其中顶级域名分为：国家顶级域名、通用顶级域名、反向域名。 国家顶级域名：中国:cn， 美国:us，英国uk… 通用顶级域名：com 公司企业 edu教育机构 gov政府部门 int国际组织 mil军事部门 net网络 org非盈利组织… 反向域名：只有一个arpa，用于PTR查询（IP地址转换为域名） 。 1.2 DNS 服务器在网络世界，也是这样的。你肯定记得住网站的名称，但是很难记住网站的 IP 地址，因而也需要一个地址簿，就是 DNS 服务器。域名是分层结构，域名服务器也是对应的层级结构。有了域名结构，还需要有一个东西去解析域名，域名需要由遍及全世界的域名服务器去解析，域名服务器实际上就是装有域名系统的主机。 每个人上网，都需要访问它，但是同时，这对它来讲也是非常大的挑战。一旦它出了故障，整个互联网都将瘫痪。另外，上网的人分布在全世界各地，如果大家都去同一个地方访问某一台服务器，时延将会非常大。因而，DNS 服务器，一定要设置成高可用、高并发和分布式的。 根 DNS 服务器 ：返回顶级域 DNS 服务器的 IP 地址, 最高层次的域名服务器，本地域名服务器解析不了的域名就会向其求助，全球共有13个不同IP地址的根域名服务器，它们的名称用一个英文字母命名，从a一直到m。 顶级域 DNS 服务器：返回权威 DNS 服务器的 IP 地址, 负责管理在该顶级域名服务器下注册的二级域名 权威 DNS 服务器 ：返回相应主机的 IP 地址, 负责一个区的域名解析工作 本地域名服务器 : 可以看成是默认域名服务器，DNS客户进程收到主机发送过来的域名后，就会最初向该域名服务器发送查询请求 1.3 域名解析过程 电脑客户端会发出一个 DNS 请求，并发给本地域名服务器 (本地 DNS),进行递归查询。如果是通过 DHCP 配置，本地 DNS 由你的网络服务商（ISP），如电信、移动等自动分配，它通常就在你网络服务商的某个机房。 本地 DNS 收到来自客户端的请求。你可以想象这台服务器上缓存了一张域名与之对应 IP 地址的大表格。如果能找到，它直接就返回 IP 地址。如果没有，本地 DNS 会迭代询问根域名服务器。 根域名服务器收到来自本地 DNS 的请求，告诉本地域名服务器，下一次应该查询的顶级域名服务器的IP地址。 本地域名服务器向顶级域名服务器进行查询。 顶级域名服务器告诉本地域名服务器，下一步查询权限服务器的IP地址。 本地域名服务器向权限服务器进行查询。 权限服务器告诉本地域名服务器所查询的主机的IP地址。 本地域名服务器最后把查询结果告诉主机 1.4 负载均衡站在客户端角度，这是一次 DNS 递归查询过程。因为本地 DNS 全权为它效劳，它只要坐等结果即可。在这个过程中，DNS 除了可以通过名称映射为 IP 地址，它还可以做另外一件事，就是负载均衡。 一. 内部负载均衡DNS 首先可以做内部负载均衡。 一个应用要访问数据库，在这个应用里面应该配置这个数据库的 IP 地址，还是应该配置这个数据库的域名呢？显然应该配置域名，因为一旦这个数据库，因为某种原因，换到了另外一台机器上，而如果有多个应用都配置了这台数据库的话，一换 IP 地址，就需要将这些应用全部修改一遍。但是如果配置了域名，则只要在 DNS 服务器里，将域名映射为新的 IP 地址，这个工作就完成了，大大简化了运维。 在这个基础上，我们可以再进一步。例如，某个应用要访问另外一个应用，如果配置另外一个应用的 IP 地址，那么这个访问就是一对一的。但是当被访问的应用撑不住的时候，我们其实可以部署多个。但是，访问它的应用，如何在多个之间进行负载均衡？只要配置成为域名就可以了。在域名解析的时候，我们只要配置策略，这次返回第一个 IP，下次返回第二个 IP，就可以实现负载均衡了。 二. 全局负载均衡为了保证我们的应用高可用，往往会部署在多个机房，每个地方都会有自己的 IP 地址。当用户访问某个域名的时候，这个 IP 地址可以轮询访问多个数据中心。如果一个数据中心因为某种原因挂了，只要在 DNS 服务器里面，将这个数据中心对应的 IP 地址删除，就可以实现一定的高可用。 DNS 访问数据中心中对象存储上的静态资源假设全国有多个数据中心，托管在多个运营商，每个数据中心三个可用区（Available Zone）。对象存储通过跨可用区部署，实现高可用性。在每个数据中心中，都至少部署两个内部负载均衡器，内部负载均衡器后面对接多个对象存储的前置服务器（Proxy-server）。 当一个客户端要访问 object.yourcompany.com 的时候，需要将域名转换为 IP 地址进行访问，所以它要请求本地 DNS 解析器 本地 DNS 解析器先查看看本地的缓存是否有这个记录。如果有则直接使用，因为上面的过程太复杂了，如果每次都要递归解析，就太麻烦了。 如果本地无缓存，则需要请求本地的 DNS 服务器。 本地的 DNS 服务器一般部署在你的数据中心或者你所在的运营商的网络中，本地 DNS 服务器也需要看本地是否有缓存，如果有则返回，因为它也不想把上面的递归过程再走一遍。 至 7. 如果本地没有，本地 DNS 才需要递归地从根 DNS 服务器，查到.com 的顶级域名服务器，最终查到 yourcompany.com 的权威 DNS 服务器，给本地 DNS 服务器，权威 DNS 服务器按说会返回真实要访问的 IP 地址。 对于不需要做全局负载均衡的简单应用来讲，yourcompany.com 的权威 DNS 服务器可以直接将 object.yourcompany.com 这个域名解析为一个或者多个 IP 地址，然后客户端可以通过多个 IP 地址，进行简单的轮询，实现简单的负载均衡。 但是对于复杂的应用，尤其是跨地域跨运营商的大型应用，则需要更加复杂的全局负载均衡机制，因而需要专门的设备或者服务器来做这件事情，这就是全局负载均衡器（GSLB，Global Server Load Balance）。 在 yourcompany.com 的 DNS 服务器中，一般是通过配置 CNAME 的方式，给 object.yourcompany.com 起一个别名，例如 object.vip.yourcomany.com，然后告诉本地 DNS 服务器，让它请求 GSLB 解析这个域名，GSLB 就可以在解析这个域名的过程中，通过自己的策略实现负载均衡。 两层的 GSLB，是因为分运营商和地域,这样不同运营商的客户，可以访问该相同运营商机房中的资源，这样不跨运营商访问，有利于提高吞吐量，减少时延。 第一层 GSLB，通过查看请求它的本地 DNS 服务器所在的运营商，就知道用户所在的运营商。假设是移动，通过 CNAME 的方式，通过另一个别名 object.yd.yourcompany.com，告诉本地 DNS 服务器去请求第二层的 GSLB。 第二层 GSLB，通过查看请求它的本地 DNS 服务器所在的地址，就知道用户所在的地理位置，然后将距离用户位置比较近的 Region 里面，六个内部负载均衡（SLB，Server Load Balancer）的地址，返回给本地 DNS 服务器。 本地 DNS 服务器将结果返回给本地 DNS 解析器。 本地 DNS 解析器将结果缓存后，返回给客户端。 客户端开始访问属于相同运营商的距离较近的 Region 1 中的对象存储，当然客户端得到了六个 IP 地址，它可以通过负载均衡的方式，随机或者轮询选择一个可用区进行访问。对象存储一般会有三个备份，从而可以实现对存储读写的负载均衡。 1.5 总结 DNS 是网络世界的地址簿，可以通过域名查地址，因为域名服务器是按照树状结构组织的，因而域名查找是使用递归的方法，并通过缓存的方式增强性能； 在域名和 IP 的映射过程中，给了应用基于域名做负载均衡的机会，可以是简单的负载均衡，也可以根据地址和运营商做全局的负载均衡。 1.6 问题思考(1) 全局负载均衡为什么要分地址和运营商呢？分地址和运营商主要是为了返回最优的ip，也就是离用户最近的ip，提高用户访问的速度，分运营商也是返回最快的一条路径。gslb失灵一般是因为一个ns请求gslb的时候，看不到用户真实的ip，从而不一定是最优的，而且这个返回的结果可能给一个用户或者一万个用户，可以通过流量监测来缓解。 (2) 全局负载均衡使用过程中，常常遇到失灵的情况，你知道具体有哪些情况吗？对应应该怎么来解决呢？全局负载均衡失灵的时间，可以分情况来应对 全局负载均衡器因为流量过大，而导致的失灵，此情况，是因为流量已经超过了当前机器的极限所导致的，针对此只能通过扩容来解决。 全局负载均衡器因为机器故障了，导致的失灵。此情况的发生，说明机器存在负载均衡器有单点问题，须通过增加备机，或者更为可靠的集群来解决。 全局负载均衡器因为网络故障，导致的失灵。此情况，案例莫过于支付宝的光纤被挖掘机挖断，此问题可通过接入更多的线路来解决。 （3）如果权威 DNS 连不上，怎么办？ 一般情况下，DNS 是基于 UDP 协议的。在应用层设置一个超时器，如果 UDP 发出没有回应，则会进行重试。 DNS 服务器一般也是高可用的，很少情况下会挂。即便挂了，也会很快切换，重试一般就会成功。 对于客户端来讲，为了 DNS 解析能够成功，也会配置多个 DNS 服务器，当一个不成功的时候，可以选择另一个来尝试。 1.7 DNS存在的问题 域名缓存问题 DNS在本地做一个缓存，也就是说，不是每一个请求，它都会去访问权威 DNS 服务器，而是访问过一次就把结果缓存到自己本地，当其他人来问的时候，直接就返回这个缓存数据,但是如果IP地址更换了而缓存没有及时更新,就会出现错误。 另外，有的运营商会把一些静态页面，缓存到本运营商的服务器内，这样用户请求的时候，就不用跨运营商进行访问，这样既加快了速度，也减少了运营商之间流量计算的成本。在域名解析的时候，不会将用户导向真正的网站，而是指向这个缓存的服务器。当页面更新，用户会访问到老的页面。 本地做一个缓存，直接返回缓存数据。可能会导致全局负载均衡失败，因为上次进行的缓存，不一定是这次离客户最近的地方，可能会绕远路。 域名转发问题如果是A运营商将解析的请求转发给B运营商，B去权威DNS服务器查询的话，权威服务器会认为你是B运营商的，就返回了B运营商的网站地址，结果每次都会跨运营商，速度就会很慢。 出口NAT问题出口的时候，很多机房都会配置 NAT，也即网络地址转换，使得从这个网关出去的包，都换成新的 IP 地址，当然请求返回的时候，在这个网关，再将 IP 地址转换回去。一旦做了网络地址转化后，权威的DNS服务器，没法通过地址来判断客户到底是哪个运营商，极有可能误判运营商，导致跨运营商访问。 *域名更新问题 *本地DNS服务器是由不同地区，不同运营商独立部署的，对域名解析缓存的处理上，有区别，有的会偷懒忽略解析结果TTL的时间限制，在权威 DNS 服务器解析变更的时候，解析结果在全网生效的周期非常漫长,导致服务器没有更新新的ip而是指向旧的ip。 解析延迟问题DNS 的查询过程需要递归遍历多个 DNS 服务器，才能获得最终的解析结果，这会带来一定的时延，甚至会解析超时。 2. HttpDNS HttpDNS 其实就是，不走传统的 DNS 解析，而是自己搭建基于 HTTP 协议的 DNS 服务器集群，分布在多个地点和多个运营商。当客户端需要 DNS 解析的时候，直接通过 HTTP 协议进行请求这个服务器集群，得到就近的地址。 这就相当于每家基于 HTTP 协议，自己实现自己的域名解析，自己做一个自己的地址簿，而不使用统一的地址簿。但是默认的域名解析都是走 DNS 的，因而使用 HttpDNS 需要绕过默认的 DNS 路径，就不能使用默认的客户端。使用 HttpDNS 的，往往是手机应用，需要在手机端嵌入支持 HttpDNS 的客户端 SDK。 2.1 HttpDNS 的工作模式 在客户端的 SDK 里动态请求服务端，获取 HttpDNS 服务器的 IP 列表，缓存到本地。随着不断地解析域名，SDK 也会在本地缓存 DNS 域名解析的结果。 当手机应用要访问一个地址的时候，首先看是否有本地的缓存，如果有就直接返回。这个缓存和本地 DNS 的缓存不一样的是，这个是手机应用自己做的，而非整个运营商统一做的。如何更新、何时更新，手机应用的客户端可以和服务器协调来做这件事情。 如果本地没有，就需要请求 HttpDNS 的服务器，在本地 HttpDNS 服务器的 IP 列表中，选择一个发出 HTTP 的请求，会返回一个要访问的网站的 IP 列表。 请求方式如下: curl http://106.2.xxx.xxx/d?dn=c.m.163.com { &quot;dns&quot;:[{&quot;host&quot;:&quot;c.m.163.com&quot;,&quot;ips&quot;:[&quot;223.252.199.12&quot;],&quot;ttl&quot;:300,&quot;http2&quot;:0}], &quot;client&quot;:{&quot;ip&quot;:&quot;106.2.81.50&quot;,&quot;line&quot;:269692944} }手机客户端自然知道手机在哪个运营商、哪个地址。由于是直接的 HTTP 通信，HttpDNS 服务器能够准确知道这些信息，因而可以做精准的全局负载均衡。当然，当所有这些都不工作的时候，可以切换到传统的 LocalDNS 来解析，慢也比访问不到好。 那 HttpDNS 是如何解决上面DNS的问题的呢？其实归结起来就是两大问题。一是解析速度和更新速度的平衡问题，二是智能调度的问题，对应的解决方案是 HttpDNS 的缓存设计和调度设计。 2.2 HttpDNS 的缓存设计HttpDNS 就是将解析速度和更新速度全部掌控在自己手中。 一方面，解析的过程，不需要本地 DNS 服务递归的调用一大圈，一个 HTTP 的请求直接搞定，要实时更新的时候，马上就能起作用； 另一方面为了提高解析速度，本地也有缓存，缓存是在客户端 SDK 维护的，过期时间、更新时间，都可以自己控制。 HttpDNS 的缓存设计策略也是应用架构中常用的缓存设计模式，也即分为客户端、缓存、数据源三层。 对于应用架构来讲，就是应用、缓存、数据库。常见的是 Tomcat、Redis、MySQL。 对于 HttpDNS 来讲，就是手机客户端、DNS 缓存、HttpDNS 服务器 DNS 缓存在内存中，也可以持久化到存储上，从而 APP 重启之后，能够尽快从存储中加载上次累积的经常访问的网站的解析结果。解析可以同步进行，也就是直接调用 HttpDNS 的接口，返回最新的记录，更新缓存；也可以异步进行，添加一个解析任务到后台，由后台任务调用 HttpDNS 的接口。 同步解析 同步更新的优点是实时性好，缺点是如果有多个请求都发现过期的时候，同时会请求 HttpDNS 多次，其实是一种浪费。 同步更新的方式对应到应用架构中缓存的 Cache-Aside 机制，也即先读缓存，不命中读数据库，同时将结果写入缓存。 异步解析 异步更新的优点是，可以将多个请求都发现过期的情况，合并为一个对于 HttpDNS 的请求任务，只执行一次，减少 HttpDNS 的压力。同时可以在即将过期的时候，就创建一个任务进行预加载，防止过期之后再刷新，称为预加载。 它的缺点是当前请求拿到过期数据的时候，如果客户端允许使用过期数据，需要冒一次风险。如果过期的数据还能请求，就没问题；如果不能请求，则失败一次，等下次缓存更新后，再请求方能成功。 异步更新的机制对应到应用架构中缓存的 Refresh-Ahead 机制，即业务仅仅访问缓存，当过期的时候定期刷新。在著名的应用缓存** Guava Cache 中，有个 RefreshAfterWrite 机制，对于并发情况下，多个缓存访问不命中从而引发并发回源的情况，可以采取只有一个请求回源的模式。在应用架构的缓存中，也常常用数据预热或者预加载**的机制。 ##2.3 HttpDNS 的调度设计客户端 HTTPDNS服务端可以根据手机的国家，省市地点，运营商，选择最佳的服务节点。 如果有多个节点，还会考虑错误率、请求时间、服务器压力、网络状况等，进行综合选择，而非仅仅考虑地理位置。当有一个节点宕机或者性能下降的时候，可以尽快进行切换。 服务端应用可以通过调用 HttpDNS 的管理接口，配置不同服务质量的优先级、权重。HttpDNS 会根据这些策略综合地理位置和线路状况算出一个排序，优先访问当前那些优质的、时延低的 IP 地址。 HttpDNS 通过智能调度之后返回的结果，也会缓存在客户端。为了不让缓存使得调度失真，客户端可以根据不同的移动网络运营商 WIFI 的 SSID 来分维度缓存。不同的运营商或者 WIFI 解析出来的结果会不同。 2.4 总结 传统的 DNS 有很多问题，例如解析慢、更新不及时。因为缓存、转发、NAT 问题导致客户端误会自己所在的位置和运营商，从而影响流量的调度。 HttpDNS 通过客户端 SDK 和服务端，通过 HTTP 直接调用解析 DNS 的方式，绕过了传统 DNS 的这些缺点，实现了智能的调度。 2.5 问题思考使用 HttpDNS，需要向 HttpDNS 服务器请求解析域名，可是客户端怎么知道 HttpDNS 服务器的地址或者域名呢？ .httpdns服务器的地址一般不变 可以使用dns的方式获取httpdns服务器的ip地址 也可以直接把httpdns服务器的ip地址写死在客户端中。 3. CDN3.1 什么是CDN?CDN的全称是Content Delivery Network，即内容分发网络。CDN是一组分布在多个不同的地理位置的WEB服务器，用于更加有效的向用户发布内容，在优化性能时，会根据距离的远近来选择 。这些分布在各个地方的各个数据中心的节点，就称为边缘节点。 这就是 CDN 分发系统的架构。CDN 系统的缓存，也是一层一层的，能不访问后端真正的源，就不打扰它。 3.2 CDN分发客户端如何找到相应的边缘节点进行访问 ?基于 DNS 的全局负载均衡主要用来选择一个就近的同样运营商的服务器进行访问。CDN 分发网络也是一个分布在多个区域、多个运营商的分布式系统，也可以用相同的思路选择最合适的边缘节点。 有了 CDN 之后,在 web.com 这个权威 DNS 服务器上，会设置一个 CNAME 别名，指向另外一个域名 www.web.cdn.com，返回给本地 DNS 服务器。 当本地 DNS 服务器拿到这个新的域名时，需要继续解析这个新的域名。这个时候，再访问的就不是 web.com 的权威 DNS 服务器了，而是 web.cdn.com 的权威 DNS 服务器，这是 CDN 自己的权威 DNS 服务器。在这个服务器上，还是会设置一个 CNAME，指向另外一个域名，也即 CDN 网络的全局负载均衡器。 接下来，本地 DNS 服务器去请求 CDN 的全局负载均衡器解析域名，全局负载均衡器会为用户选择一台合适的缓存服务器提供服务，选择的依据包括： 根据用户 IP 地址，判断哪一台服务器距用户最近； 用户所处的运营商； 根据用户所请求的 URL 中携带的内容名称，判断哪一台服务器上有用户所需的内容； 查询各个服务器当前的负载情况，判断哪一台服务器尚有服务能力。 基于以上这些条件，进行综合分析之后，全局负载均衡器会返回一台缓存服务器的 IP 地址。 本地 DNS 服务器缓存这个 IP 地址，然后将 IP 返回给客户端，客户端去访问这个边缘节点，下载资源。缓存服务器响应用户请求，将用户所需内容传送到用户终端。如果这台缓存服务器上并没有用户想要的内容，那么这台服务器就要向它的上一级缓存服务器请求内容，直至追溯到网站的源服务器将内容拉到本地。 3.3 CDN缓存的内容 CDN能够缓存JavaScript脚本，css样式表，图片，图标，Flash等静态资源文件（不包括html页面），这些静态资源文件的访问频率很高，将其缓存在CDN可以极大地提高网站的访问速度，但由于CDN是部署在网络运营商的机房，所以在一般的网站很少用CDN加速。 在进入数据中心的时候，我们希望通过最外层接入层的缓存，将大部分静态资源的访问拦在边缘。而 CDN 则更进一步，将这些静态资源缓存到离用户更近的数据中心。越接近客户，访问性能越好，时延越低。 nginx是在网站的接入层缓存静态资源，CDN是在数据中心之外，离客户端很近的地方缓存静态资源 3.4 流媒体协议和CDNCDN 支持流媒体协议，例如前面讲过的 RTMP 协议。在很多情况下，这相当于一个代理，从上一级缓存读取内容，转发给用户。由于流媒体往往是连续的，因而可以进行预先缓存的策略，也可以预先推送到用户的客户端。 对于静态页面来讲，内容的分发往往采取拉取的方式，也即当发现未命中的时候，再去上一级进行拉取。但是，流媒体数据量大，如果出现回源，压力会比较大，所以往往采取主动推送的模式，将热点数据主动推送到边缘节点。 对于流媒体来讲，很多 CDN 还提供预处理服务，也即文件在分发之前，经过一定的处理。例如将视频转换为不同的码流，以适应不同的网络带宽的用户需求；再如对视频进行分片，降低存储压力，也使得客户端可以选择使用不同的码率加载不同的分片。这就是我们常见的，“超清、标清、流畅等”。 对于流媒体 CDN 来讲，有个关键的问题是防盗链问题。最常用也最简单的方法就是** HTTP 头的 referer 字段，** 当浏览器发送请求的时候，一般会带上 referer，告诉服务器是从哪个页面链接过来的，服务器基于此可以获得一些信息用于处理。如果 refer 信息不是来自本站，就阻止访问或者跳到其它链接。 时间戳防盗链时间戳防盗链就是使用 CDN 的管理员可以在配置界面上，和 CDN 厂商约定一个加密字符串。时间戳防盗链的流程如下: 客户端取出当前的时间戳，要访问的资源及其路径，连同加密字符串进行签名算法得到一个字符串，然后生成一个下载链接，带上这个签名字符串和截止时间戳去访问 CDN。 在 CDN 服务端，根据取出过期时间，和当前 CDN 节点时间进行比较，确认请求是否过期。然后 CDN 服务端有了资源及路径，时间戳，以及约定的加密字符串，根据相同的签名算法计算签名，如果匹配则一致，访问合法，才会将资源返回给客户。 3.5 动态数据的缓存现在也有动态 CDN，主要有两种模式。 一种为生鲜超市模式，也即边缘计算的模式。既然数据是动态生成的，所以数据的逻辑计算和存储，也相应的放在边缘的节点。其中定时从源数据那里同步存储的数据，然后在边缘进行计算得到结果。 另一种是冷链运输模式，也即路径优化的模式。数据不是在边缘计算生成的，而是在源站生成的，但是数据的下发则可以通过 CDN 的网络，对路径进行优化。因为 CDN 节点较多，能够找到离源站很近的边缘节点，也能找到离用户很近的边缘节点。中间的链路完全由 CDN 来规划，选择一个更加可靠的路径，使用类似专线的方式进行访问。 TCP连接和CDN加速 对于常用的 TCP 连接，在公网上传输的时候经常会丢数据，导致 TCP 的窗口始终很小，发送速度上不去。根据前面的 TCP 流量控制和拥塞控制的原理，在 CDN 加速网络中可以调整 TCP 的参数，使得 TCP 可以更加激进地传输数据。 可以通过多个请求复用一个连接，保证每次动态请求到达时。连接都已经建立了，不必临时三次握手或者建立过多的连接，增加服务器的压力。另外，可以通过对传输数据进行压缩，增加传输效率。 3.6 总结 CDN 和电商系统的分布式仓储系统一样，分为中心节点、区域节点、边缘节点，而数据缓存在离用户最近的位置。 CDN 最擅长的是缓存静态数据，除此之外还可以缓存流媒体数据，这时候要注意使用防盗链。它也支持动态数据的缓存，一种是边缘计算的生鲜超市模式，另一种是链路优化的冷链运输模式。 3.7 问题思考CDN 如何使用 HttpDNS ？参照阿里云CDN HTTPDNS方式；客户端请求服务URL:umc.danuoyi.alicdn.com xxx，参数是客户端ip地址和待解析的域名；然后返回多个ip地址，客户端轮训这些ip地址。","categories":[{"name":"随笔","slug":"随笔","permalink":"https://liangweijiang.github.io/categories/%E9%9A%8F%E7%AC%94/"}],"tags":[{"name":"网络协议","slug":"网络协议","permalink":"https://liangweijiang.github.io/tags/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/"}]},{"title":"数据结构学习笔记---经典排序","slug":"data-structure-05","date":"2020-03-29T12:23:26.378Z","updated":"2020-03-30T09:20:21.613Z","comments":true,"path":"2020/03/29/data-structure-05/","link":"","permalink":"https://liangweijiang.github.io/2020/03/29/data-structure-05/","excerpt":"","text":"学习自极客时间的《数据结构与算法之美》 作者：王争 排序排序的内存消耗原地排序（Sorted in place）。原地排序算法，就是特指空间复杂度是 O(1) 的排序算法。 排序算法的稳定性仅仅用执行效率和内存消耗来衡量排序算法的好坏是不够的。针对排序算法，我们还有一个重要的度量指标，稳定性。经过某种排序算法排序之后，如果两个相同元素的前后顺序没有改变，那我们就把这种排序算法叫作稳定的排序算法；如果前后顺序发生变化，那对应的排序算法就叫作不稳定的排序算法。 1. 冒泡排序（Bubble Sort） 12345678910def BubbleSort(arr): lenght = len(arr) # 这个循环负责设置冒泡排序进行的次数 for i in range(lenght): # 这个循环负责控制比较的元素个数 for j in range(lenght - 1 - i): if arr[j] &gt; arr[j + 1]: arr[j + 1], arr[j] = arr[j], arr[j + 1] return arr 冒泡排序的优化可以看到,数组中本来有一些元素已经是有序的, 但是还是进行了多余的比较,所以我们设置一个标志,如果没有数据交换,则证明数组已经有序,跳出循环。 1234567891011121314151617def BubbleSort(arr): lenght = len(arr) # 有序标记，每一轮的初始是true，用于判断元素间是否需要交换 isSorted = True # 这个循环负责设置冒泡排序进行的次数 for i in range(lenght): # 这个循环负责控制比较的元素个数 for j in range(lenght - 1 - i): if arr[j] &gt; arr[j + 1]: arr[j + 1], arr[j] = arr[j], arr[j + 1] # 有交换行为设为 False isSorted = False # 无交换行为（isSorted = True），直接跳过本次循环 if isSorted: break return arr 冒泡排序的分析一. 冒泡的过程只涉及相邻数据的交换操作，只需要常量级的临时空间，所以它的空间复杂度为 O(1)，是一个原地排序算法。 二. 在冒泡排序中，只有交换才可以改变两个元素的前后顺序。为了保证冒泡排序算法的稳定性，当有相邻的两个元素大小相等的时候，我们不做交换，相同大小的数据在排序前后不会改变顺序，所以冒泡排序是稳定的排序算法。 三. 时间复杂度:最好情况下，要排序的数据已经是有序的了，我们只需要进行一次冒泡操作，就可以结束了，所以最好情况时间复杂度是 O(n)。而最坏的情况是，要排序的数据刚好是倒序排列的，我们需要进行 n 次冒泡操作，所以最坏情况时间复杂度为 O(n2)。 那平均时间复杂度为多少? 这里要引进“有序度”和“逆序度”这两个概念来进行分析有序度是数组中具有有序关系的元素对的个数。有序元素对用数学表达式表示就是这样： 有序元素对：a[i] &lt;= a[j], 如果i &lt; j。 对于一个倒序排列的数组，比如 6，5，4，3，2，1，有序度是 0；对于一个完全有序的数组，比如 1，2，3，4，5，6，有序度就是 n*(n-1)/2，也就是 15。我们把这种完全有序的数组的有序度叫作满有序度。 逆序度的定义正好跟有序度相反（默认从小到大为有序）： 逆序元素对：a[i] &gt; a[j], 如果i &lt; j。 冒泡排序包含两个操作原子，比较和交换。每交换一次，有序度就加 1。不管算法怎么改进，交换次数总是确定的，即为逆序度，也就是n*(n-1)/2–初始有序度。此例中就是 15–3=12，要进行 12 次交换操作。 对于包含 n 个数据的数组进行冒泡排序，平均交换次数是多少呢？最坏情况下，初始状态的有序度是 0，所以要进行 n(n-1)/2 次交换。最好情况下，初始状态的有序度是 n(n-1)/2，就不需要进行交换。我们可以取个中间值 n*(n-1)/4，来表示初始有序度既不是很高也不是很低的平均情况。 换句话说，平均情况下，需要 n*(n-1)/4 次交换操作，比较操作肯定要比交换操作多，而复杂度的上限是 O(n^2)，所以平均情况下的时间复杂度就是 O(n^2)。 2. 插入排序（Insertion Sort) 我们将数组中的数据分为两个区间，已排序区间和未排序区间。初始已排序区间只有一个元素，就是数组的第一个元素。 插入算法的核心思想是取未排序区间中的元素，在已排序区间中找到合适的插入位置将其插入，并保证已排序区间数据一直有序。重复这个过程，直到未排序区间中元素为空，算法结束。 对于不同的查找插入点方法（从头到尾、从尾到头），元素的比较次数是有区别的。但对于一个给定的初始序列，移动操作的次数总是固定的，就等于逆序度。 1234567891011121314151617from typing import Listdef insertion_sort(a: List[int]): length = len(a) if length &lt;= 1: return for i in range(1, length): # 选出要插入的数据 current = a[i] j = i - 1 # 在已经排好序的部分从后往前一一比较,如果大于出入的数据,则往后退一个单位 while j &gt;= 0 and a[j] &gt; current: a[j + 1] = a[j] j -= 1 # 当没有数据比插入的数据大时,则将该数据插入数组中 a[j + 1] = current 插入排序的性能分析一. 冒泡的过程只涉及相邻数据的交换操作，只需要常量级的临时空间，所以它的空间复杂度为 O(1)，是一个原地排序算法。 二. 在插入排序中，对于值相同的元素，我们可以选择将后面出现的元素，插入到前面出现元素的后面，这样就可以保持原有的前后顺序不变，所以插入排序是稳定的排序算法。 三. 时间复杂度 如果要排序的数据已经是有序的，我们并不需要搬移任何数据。如果我们从尾到头在有序数据组里面查找插入位置，每次只需要比较一个数据就能确定插入的位置。所以这种情况下，最好是时间复杂度为 O(n)。 如果数组是倒序的，每次插入都相当于在数组的第一个位置插入新的数据，所以需要移动大量的数据，所以最坏情况时间复杂度为 O(n^2)。 在数组中插入一个数据的平均时间复杂度是O(n)。所以，对于插入排序来说，每次插入操作都相当于在数组中插入一个数据，循环执行 n 次插入操作，所以平均时间复杂度为 O(n^2)。 3. 希尔排序(Shell’s Sort)希尔排序(Shell’s Sort)是插入排序的一种又称“缩小增量排序”（Diminishing Increment Sort），是直接插入排序算法的一种更高效的改进版本。希尔排序是非稳定排序算法。该方法因D.L.Shell于1959年提出而得名。 希尔排序是基于插入排序的以下两点性质而提出改进方法的： 插入排序在对几乎已经排好序的数据操作时，效率高，即可以达到线性排序的效率； 但插入排序一般来说是低效的，因为插入排序每次只能将数据移动一位； 先将整个待排序的记录序列分割成为若干子序列分别进行直接插入排序，具体算法描述： 选择一个增量序列t1，t2，…，tk，其中ti&gt;tj，tk=1； 按增量序列个数k，对序列进行k 趟排序； 每趟排序，根据对应的增量ti，将待排序列分割成若干长度为m 的子序列，分别对各子表进行直接插入排序。仅增量因子为1 时，整个序列作为一个表来处理，表长度即为整个序列的长度。 12345678910111213141516171819def shell_sort(lists): #希尔排序 count = len(lists) step = 2 group = count // step while group &gt; 0: #通过group增量分组循环 for i in range(0, group): j = i + group while j &lt; count: #分组中key值的索引，通过增量自增 k = j - group key = lists[j] while k &gt;= 0: #分组中进行插入排序 if lists[k] &gt; key: lists[k + group], lists[k] = lists[k], key else: break k -= group j += group group //= step return lists 4. 选择排序（Selection Sort）选择排序算法的实现思路有点类似插入排序，也分已排序区间和未排序区间。但是选择排序每次会从未排序区间中找到最小的元素，将其放到已排序区间的末尾。 1234567891011121314151617from typing import Listdef selection_sort(a: List[int]): length = len(a) if length &lt;= 1: return for i in range(length): # 选取排序区域的末尾 min_index = i # 在未排序区域找到最小的元素 for j in range(i, length): if a[j] &lt; a[min_index]: min_index = j if i != min_index: a[i], a[min_index] = a[min_index], a[i] 选择排序的性能分析一. 选择排序空间复杂度为 O(1)，是一种原地排序算法。 二. 选择排序是一种不稳定的排序算法。**选择排序每次都要找剩余未排序元素中的最小值，并和前面的元素交换位置，这样破坏了稳定性。 三. 选择排序的最好情况时间复杂度、最坏情况和平均情况时间复杂度都为 O(n^2)。 5. 归并排序（Merge Sort）归并排序使用的就是分治思想。分治，顾名思义，就是分而治之，将一个大问题分解成小的子问题来解决。小的子问题解决了，大问题也就解决了。 归并排序的核心思想就是如果要排序一个数组，我们先把数组从中间分成前后两部分，然后对前后两部分分别排序，再将排好序的两部分合并在一起，这样整个数组就都有序了。 归并排序的递推公式为: 递推公式： merge_sort(p…r) = merge(merge_sort(p…q), merge_sort(q+1…r)) 终止条件： p &gt;= r 不用再继续分解 1234567891011121314151617181920212223242526272829303132333435from typing import Listdef merge_sort(a: List[int]): length = len(a) _merge_sort_between(a, 0, length - 1)def _merge_sort_between(a: List[int], low: int, high: int): if low &lt; high: mid = low + (high - low) // 2 # 分治思想 _merge_sort_between(a, low, mid) _merge_sort_between(a, mid + 1, high) _merge(a, low, mid, high)def _merge(a: List[int], low: int, mid: int, high: int): # 申请一个大小跟A[p...r]一样的临时数组 temp = [] i, j = low, mid + 1 while i &lt;= mid and j &lt;= high: if a[i] &lt;= a[j]: temp.append(a[i]) i += 1 else: temp.append(a[j]) j += 1 # 判断哪个子数组中有剩余的数据 start = i if i &lt;= mid else j end = mid if i &lt;= mid else high # 将剩余的数据拷贝到临时数组tmp temp.extend(a[start:end + 1]) # 将tmp中的数组拷贝回A[p...r] a[low:high + 1] = temp 归并排序的性能分析一. 归并排序稳不稳定关键要看 merge() 函数，也就是两个有序子数组合并成一个有序数组的那部分代码。在合并的过程中，如果 A[p…q]和 A[q+1…r]之间有值相同的元素，先把 A[p…q]中的元素放入 tmp 数组。这样就保证了值相同的元素，在合并前后的先后顺序不变。所以，归并排序是一个稳定的排序算法。 二. 时间复杂度我们假设对 n 个元素进行归并排序需要的时间是 T(n)，那分解成两个子数组排序的时间都是 T(n/2)。我们知道，merge() 函数合并两个有序子数组的时间复杂度是 O(n)。所以，套用前面的公式，归并排序的时间复杂度的计算公式就是： T(1) = C； n=1时，只需要常量级的执行时间，所以表示为C。 T(n) = 2*T(n/2) + n； n&gt;1 = 2*(2*T(n/4) + n/2) + n = 4*T(n/4) + 2*n = 4*(2*T(n/8) + n/4) + 2*n = 8*T(n/8) + 3*n = 8*(2*T(n/16) + n/8) + 3*n = 16*T(n/16) + 4*n ...... = 2^k * T(n/2^k) + k * n ......通过这样一步一步分解推导，我们可以得到 T(n) = 2^kT(n/2^k)+kn。当 T(n/2^k)=T(1) 时，也就是 n/2^k=1，我们得到 k=log2n 。我们将 k 值代入上面的公式，得到 T(n)=Cn+nlog2n 。如果我们用大 O 标记法来表示的话，T(n) 就等于 O(nlogn)。所以归并排序的时间复杂度是 O(nlogn)。 归并排序的执行效率与要排序的原始数组的有序程度无关，所以其时间复杂度是非常稳定的，不管是最好情况、最坏情况，还是平均情况，时间复杂度都是 O(nlogn)。 三. 空间复杂度归并排序不是原地排序算法。这是因为归并排序的合并函数，在合并两个有序数组为一个有序数组时，需要借助额外的存储空间。尽管每次合并操作都需要申请额外的内存空间，但在合并完成之后，临时开辟的内存空间就被释放掉了。在任意时刻，CPU 只会有一个函数在执行，也就只会有一个临时的内存空间在使用。临时内存空间最大也不会超过 n 个数据的大小，所以空间复杂度是 O(n)。 6. 快速排序（Quicksort）快排的思想是这样的：如果要排序数组中下标从 p 到 r 之间的一组数据，我们选择 p 到 r 之间的任意一个数据作为 pivot（分区点）。 归并排序中有一个 merge() 合并函数，我们这里有一个 partition() 分区函数。partition() 分区函数实际上我们前面已经讲过了，就是随机选择一个元素作为 pivot（一般情况下，可以选择 p 到 r 区间的最后一个元素），然后对 A[p…r]分区，函数返回 pivot 的下标。 如果我们不考虑空间消耗的话，partition() 分区函数可以写得非常简单。我们申请两个临时数组 X 和 Y，遍历 A[p…r]，将小于 pivot 的元素都拷贝到临时数组 X，将大于 pivot 的元素都拷贝到临时数组 Y，最后再将数组 X 和数组 Y 中数据顺序拷贝到 A[p…r]。但是这就会消耗过多的空间。 我们通过游标 i 把 A[p…r-1]分成两部分。A[p…i-1]的元素都是小于 pivot 的，我们暂且叫它“已处理区间”，A[i…r-1]是“未处理区间”。我们每次都从未处理的区间 A[i…r-1]中取一个元素 A[j]，与 pivot 对比，如果小于 pivot，则将其加入到已处理区间的尾部，也就是 A[i]的位置。只需要将 A[i]与 A[j]交换，就可以在 O(1) 时间复杂度内将 A[j]放到下标为 i 的位置。 1234567891011121314151617181920212223242526272829from typing import Listimport randomdef quick_sort(a: List[int]): _quick_sort_between(a, 0, len(a) - 1)def _quick_sort_between(a: List[int], low: int, high: int): if low &lt; high: # 随机找一个数字作为开始的分区点 k = random.randint(low, high) # 设置哨兵 a[low], a[k] = a[k], a[low] # 找到分区点 m = _partition(a, low, high) _quick_sort_between(a, low, m - 1) _quick_sort_between(a, m + 1, high)def _partition(a: List[int], low, high): pivot, j = a[low], low for i in range(low + 1, high + 1): if a[i] &lt;= pivot: j += 1 a[i], a[j] = a[j], a[i] # 当没有小于基准数的时候,就是基准数的位置 a[low], a[j] = a[j], a[low] return j 快速排序的性能分析一. 快排是一种原地、不稳定的排序算法。 二. 时间复杂度: O(logn) 归并排序和快排的区别快排和归并用的都是分治思想，递推公式和递归代码也非常相似，那它们的区别在哪里呢？ 归并排序的处理过程是由下到上的，先处理子问题，然后再合并。而快排正好相反，它的处理过程是由上到下的，先分区，然后再处理子问题。 归并排序虽然是稳定的、时间复杂度为 O(nlogn) 的排序算法，但是它是非原地排序算法。我们前面讲过，归并之所以是非原地排序算法，主要原因是合并函数无法在原地执行。 快速排序通过设计巧妙的原地分区函数，可以实现原地排序，解决了归并排序占用太多内存的问题。","categories":[{"name":"随笔","slug":"随笔","permalink":"https://liangweijiang.github.io/categories/%E9%9A%8F%E7%AC%94/"}],"tags":[{"name":"排序","slug":"排序","permalink":"https://liangweijiang.github.io/tags/%E6%8E%92%E5%BA%8F/"}]},{"title":"每日一道算法之--单词的压缩编码","slug":"algorithm-820","date":"2020-03-28T10:49:54.988Z","updated":"2020-03-28T10:50:56.135Z","comments":true,"path":"2020/03/28/algorithm-820/","link":"","permalink":"https://liangweijiang.github.io/2020/03/28/algorithm-820/","excerpt":"","text":"单词的压缩编码力扣第820题:https://leetcode-cn.com/problems/short-encoding-of-words/ 给定一个单词列表，我们将这个列表编码成一个索引字符串 S 与一个索引列表 A。 例如，如果这个列表是 [“time”, “me”, “bell”]，我们就可以将其表示为 S = “time#bell#” 和 indexes = [0, 2, 5]。 对于每一个索引，我们可以通过从字符串 S 中索引的位置开始读取字符串，直到 “#” 结束，来恢复我们之前的单词列表。 那么成功对给定单词列表进行编码的最小字符串长度是多少呢？ 示例： 输入: words = [“time”, “me”, “bell”]输出: 10说明: S = “time#bell#” ， indexes = [0, 2, 5] 。 1. 字符串翻转 + 排序首先读懂题意,如果某个单词为另一个单词的后缀,则该单词用另一个加上#表示。我首先想到的就是把后缀相同的单词排在一起,但是排序是从前缀开始的,所以需要翻转。 12345678910111213141516171819class Solution: def minimumLengthEncoding(self, words: List[str]) -&gt; int: if not words: return 0 # 将字符串安安后缀排序 words.sort(key = lambda x: x[::-1],reverse=True) # 记录新字符串的长度 lenght = 0 # 这个变量是更新比较的字符串 word_compare = '' for i in range(len(words)): # 如果该单词是比较字符串的后缀，直接忽略 if word_compare.endswith(words[i]): continue # 更新新字符串长度，包括一个‘#’ lenght += len(words[i]) + 1 # 更新比较的字符串 word_compare = words[i] return lenght 复杂度分析时间复杂度: 将字符串排序的时间复杂度为 O(nlogn), 然后只需遍历一次列表就能完成压缩,时间复杂度为 O(n),所以总的时间复杂度为 O(nlogn)。 空间复杂度: 用了两个变量,所以空间复杂度为O(1）。 2. 字典树/Trie树去找到是否不同的单词具有相同的后缀，我们可以将其反序之后插入字典树中。例如，我们有 “time” 和 “me”，可以将 “emit” 和 “em” 插入字典树中。 123456789101112class Solution: def minimumLengthEncoding(self, words: List[str]) -&gt; int: words = list(set(words)) Trie = lambda: collections.defaultdict(Trie) trie = Trie() nodes = [reduce(dict.__getitem__, word[::-1], trie) for word in words] print(words)s return sum(len(word) + 1 for i, word in enumerate(words) if len(nodes[i]) == 0) 参考至力扣题解:https://leetcode-cn.com/problems/short-encoding-of-words/solution/dan-ci-de-ya-suo-bian-ma-by-leetcode-solution/","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://liangweijiang.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"排序","slug":"排序","permalink":"https://liangweijiang.github.io/tags/%E6%8E%92%E5%BA%8F/"},{"name":"字典树","slug":"字典树","permalink":"https://liangweijiang.github.io/tags/%E5%AD%97%E5%85%B8%E6%A0%91/"}]},{"title":"数据结构学习笔记---队列","slug":"data-structure-04","date":"2020-03-27T13:50:04.222Z","updated":"2020-03-27T13:50:52.637Z","comments":true,"path":"2020/03/27/data-structure-04/","link":"","permalink":"https://liangweijiang.github.io/2020/03/27/data-structure-04/","excerpt":"","text":"学习自极客时间的《数据结构与算法之美》 作者：王争 队列1. 什么是队列?先进者先出，这就是典型的“队列”。队列跟栈非常相似，支持的操作也很有限，最基本的操作也是两个：入队 enqueue()，放一个数据到队列尾部；出队 dequeue()，从队列头部取一个元素。所以,队列跟栈一样，也是一种操作受限的线性表数据结构。 作为一种非常基础的数据结构，队列的应用也非常广泛，特别是一些具有某些额外特性的队列，比如循环队列、阻塞队列、并发队列。它们在很多偏底层系统、框架、中间件的开发中，起着关键性的作用。比如高性能队列 Disruptor、Linux 环形缓存，都用到了循环并发队列；Java concurrent 并发包利用 ArrayBlockingQueue 来实现公平锁等。 2. 顺序队列跟栈一样，队列可以用数组来实现，也可以用链表来实现。用数组实现的栈叫作顺序栈，用链表实现的栈叫作链式栈。同样，用数组实现的队列叫作顺序队列，用链表实现的队列叫作链式队列。 2.1 顺序队列数组实现队列用的是数组双指针的方法。 1234567891011121314151617181920212223242526272829from typing import Optionalclass ArrayQueue: def __init__(self, capacity: int): self._items = [] # 申请一个大小为capacity的数组 self._capacity = capacity # head表示队头下标，tail表示队尾下标 self._head = 0 self._tail = 0 def enqueue(self, item: str) -&gt; bool: if self._tail == self._capacity: return False self._items.insert(self._tail, item) self._tail += 1 return True def dequeue(self) -&gt; Optional[str]: if self._head != self._tail: item = self._items[self._head] self._head += 1 return item else: return None def __repr__(self) -&gt; str: return \" \".join(item for item in self._items[self._head : self._tail]) 队列需要两个指针：一个是 head 指针，指向队头；一个是 tail 指针，指向队尾。 当 a、b、c、d 依次入队之后，队列中的 head 指针指向下标为 0 的位置，tail 指针指向下标为 4 的位置。 当我们调用两次出队操作之后，队列中 head 指针指向下标为 2 的位置，tail 指针仍然指向下标为 4 的位置。 随着不停地进行入队、出队操作，head 和 tail 都会持续往后移动。当 tail 移动到最右边，即使数组中还有空闲空间，也无法继续往队列中添加数据了。在数组那里也可以提到，可以用数据搬移。但是，每次进行出队操作都相当于删除数组下标为 0 的数据，要搬移整个队列中的数据，这样出队操作的时间复杂度就会从原来的 O(1) 变为 O(n)。 实际上，我们在出队时可以不用搬移数据。如果没有空闲空间了，我们只需要在入队时，再集中触发一次数据的搬移操作。借助这个思想，出队函数 dequeue() 保持不变，我们稍加改造一下入队函数 enqueue() 的实现，就可以轻松解决刚才的问题了。下面是具体的代码： 123456789101112131415def enqueue(self, item: str) -&gt; bool: if self._tail == self._capacity: if self._head == 0: return False else: # 数据搬移 for i in range(0, self._tail - self._head): self._items[i] = self._items[i + self._head] # 搬移完之后重新更新head和tail self._tail = self._tail - self._head self._head = 0 self._items.insert(self._tail, item) self._tail += 1 return True 当队列的 tail 指针移动到数组的最右边后，如果有新的数据入队，我们可以将 head 到 tail 之间的数据，整体搬移到数组中 0 到 tail-head 的位置。 完整代码可以这样写: 123456789101112131415161718192021222324252627282930313233from typing import Optionalclass DynamicArrayQueue: def __init__(self, capacity: int): self._items = [] self._capacity = capacity self._head = 0 self._tail = 0 def enqueue(self, item: str) -&gt; bool: if self._tail == self._capacity: if self._head == 0: return False self._items[0 : self._tail - self._head] = self._items[self._head : self._tail] self._tail -= self._head self._head = 0 if self._tail == len(self._items): self._items.append(item) else: self._items[self._tail] = item self._tail += 1 return True def dequeue(self) -&gt; Optional[str]: if self._head != self._tail: item = self._items[self._head] self._head += 1 return item def __repr__(self) -&gt; str: return \" \".join(item for item in self._items[self._head:self._tail]) 3. 链式队列基于链表的实现，我们同样需要两个指针：head 指针和 tail 指针。它们分别指向链表的第一个结点和最后一个结点。如图所示，入队时，tail-&gt;next= new_node, tail = tail-&gt;next；出队时，head = head-&gt;next。 12345678910111213141516171819202122232425262728293031323334353637from typing import Optionalclass Node: def __init__(self, data: str, next=None): self.data = data self._next = nextclass LinkedQueue: def __init__(self): self._head: Optional[Node] = None self._tail: Optional[Node] = None def enqueue(self, value: str): new_node = Node(value) if self._tail: self._tail._next = new_node else: self._head = new_node self._tail = new_node def dequeue(self) -&gt; Optional[str]: if self._head: value = self._head.data self._head = self._head._next if not self._head: self._tail = None return value def __repr__(self) -&gt; str: values = [] current = self._head while current: values.append(current.data) current = current._next return \"-&gt;\".join(value for value in values) 4. 循环队列要想实现一个循环队列,最关键的是，确定好队空和队满的判定条件。循环队列为空的判断条件仍然是 head == tail。但队列满的判断条件就稍微有点复杂了。当队满时，(tail+1)%n=head。 当队列满时，图中的 tail 指向的位置实际上是没有存储数据的。所以，循环队列会浪费一个数组的存储空间。 123456789101112131415161718192021222324252627282930from typing import Optionalfrom itertools import chainclass CircularQueue: def __init__(self, capacity): self._items = [] self._capacity = capacity + 1 self._head = 0 self._tail = 0 def enqueue(self, item: str) -&gt; bool: if (self._tail + 1) % self._capacity == self._head: return False self._items.append(item) self._tail = (self._tail + 1) % self._capacity return True def dequeue(self) -&gt; Optional[str]: if self._head != self._tail: item = self._items[self._head] self._head = (self._head + 1) % self._capacity return item def __repr__(self) -&gt; str: if self._tail &gt;= self._head: return \" \".join(item for item in self._items[self._head : self._tail]) else: return \" \".join(item for item in chain(self._items[self._head:], self._items[:self._tail])) 5. 阻塞队列阻塞队列其实就是在队列基础上增加了阻塞操作。简单来说，就是在队列为空的时候，从队头取数据会被阻塞。因为此时还没有数据可取，直到队列中有了数据才能返回；如果队列已经满了，那么插入数据的操作就会被阻塞，直到队列中有空闲位置后再插入数据，然后再返回。 “生产者 - 消费者模型”就是利用的阻塞队列。这种基于阻塞队列实现的“生产者 - 消费者模型”，可以有效地协调生产和消费的速度。当“生产者”生产数据的速度过快，“消费者”来不及消费时，存储数据的队列很快就会满了。这个时候，生产者就阻塞等待，直到“消费者”消费了数据，“生产者”才会被唤醒继续“生产”。 基于阻塞队列，我们还可以通过协调“生产者”和“消费者”的个数，来提高数据的处理效率。比如前面的例子，我们可以多配置几个“消费者”，来应对一个“生产者”。 6. 并发队列线程安全的队列我们叫作并发队列。最简单直接的实现方式是直接在 enqueue()、dequeue() 方法上加锁，但是锁粒度大并发度会比较低，同一时刻仅允许一个存或者取操作。实际上，基于数组的循环队列，利用 CAS 原子操作，可以实现非常高效的并发队列。这也是循环队列比链式队列应用更加广泛的原因。在实战篇讲 Disruptor 的时候，我会再详细讲并发队列的应用。 问题思考一. 当我们向固定大小的线程池中请求一个线程时，如果线程池中没有空闲资源了，这个时候线程池如何处理这个请求？是拒绝请求还是排队请求？各种处理策略又是怎么实现的呢？ 我们一般有两种处理策略。第一种是非阻塞的处理方式，直接拒绝任务请求；另一种是阻塞的处理方式，将请求排队，等到有空闲线程时，取出排队的请求继续处理。那如何存储排队的请求呢？ 我们希望公平地处理每个排队的请求，先进者先服务，所以队列这种数据结构很适合来存储排队请求。我们前面说过，队列有基于链表和基于数组这两种实现方式。这两种实现方式对于排队请求又有什么区别呢？ 基于链表的实现方式，可以实现一个支持无限排队的无界队列（unbounded queue），但是可能会导致过多的请求排队等待，请求处理的响应时间过长。所以，针对响应时间比较敏感的系统，基于链表实现的无限排队的线程池是不合适的。 而基于数组实现的有界队列（bounded queue），队列的大小有限，所以线程池中排队的请求超过队列大小时，接下来的请求就会被拒绝，这种方式对响应时间敏感的系统来说，就相对更加合理。不过，设置一个合理的队列大小，也是非常有讲究的。队列太大导致等待的请求太多，队列太小会导致无法充分利用系统资源、发挥最大性能。 二. 除了线程池这种池结构会用到队列排队请求，你还知道有哪些类似的池结构或者场景中会用到队列的排队请求呢?分布式应用中的消息队列，也是一种队列结构。 三. 如何实现无锁并发队列?可以使用 cas + 数组的方式实现。在入队前，获取tail位置，入队时比较tail是否发生变化，如果否，则允许入队，反之，本次入队失败。出队则是获取head位置，进行cas。","categories":[{"name":"随笔","slug":"随笔","permalink":"https://liangweijiang.github.io/categories/%E9%9A%8F%E7%AC%94/"}],"tags":[{"name":"队列","slug":"队列","permalink":"https://liangweijiang.github.io/tags/%E9%98%9F%E5%88%97/"}]},{"title":"《趣谈网络协议》学习笔记之--最常用的运用层(下)","slug":"NetworkProtocol-07","date":"2020-03-26T13:10:49.872Z","updated":"2020-03-27T10:43:25.608Z","comments":true,"path":"2020/03/26/NetworkProtocol-07/","link":"","permalink":"https://liangweijiang.github.io/2020/03/26/NetworkProtocol-07/","excerpt":"","text":"学习自极客时间《趣谈网络协议》 作者：刘超 5. 流媒体协议5.1 什么是流媒体? 流媒体（Streaming media）是指将一连串的媒体数据压缩后，经过网络分段发送数据，在网络上即时传输影音以供观赏的一种技术与过程，此技术使得数据包得以像流水一样发送；如果不使用此技术，就必须在使用前下载整个媒体文件。流媒体文件一般定义在bit层次结构，因此流数据包并不一定必须按照字节对齐，虽然通常的媒体文件都是按照这种字节对齐的方式打包的。流媒体的三大操作平台是微软公司、RealNetworks、苹果公司提供的。 5.2 有哪些多媒体常用协议 TTP/TCP/UDP 涵盖传统的私有协议，以及基于HTTP的流式传输协议。 RTP/RTSP/RTCP/SRTP &amp; SRTCP/RTMP HLS MMS （Microsoft Media Server Protocol） DASH RTMP 是一个古老的协议。RMTP 最初由 Macromedia 开发，后被 Adobe 收购，至今仍被使用。由于 RTMP 播放视频需要依赖 Flash 插件。而 Flash 插件多年来一直受安全问题困扰，正在被迅速淘汰。因此，目前 RTMP 主要用于提取 stream。也就是，当设置解编码器将视频发送到托管平台时，视频将使用 RTMP 协议发送到 CDN，随后使用另一种协议（通常是HLS）传递给播放器。何时使用 RTMPRTMP 协议延迟非常低，但由于需要 Flash 插件，不建议使用该协议，但流提取是例外。在流提取方便，RTMP 非常强大，且几乎得到了普遍支持。 相对于常见的流媒体直播协议，例如RTMP协议、RTSP协议、MMS协议等，HLS直播最大的不同在于，直播客户端获取到的，并不是一个完整的数据流。HLS协议在服务器端将直播数据流存储为连续的、很短时长的媒体文件（MPEG-TS格式），而客户端则不断的下载并播放这些小文件，因为服务器端总是会将最新的直播数据生成新的小文件，这样客户端只要不停的按顺序播放从服务器获取到的文件，就实现了直播。由此可见，基本上可以认为，HLS是以点播的技术方式来实现直播。由于数据通过HTTP协议传输，所以完全不用考虑防火墙或者代理的问题，而且分段文件的时长很短，客户端可以很快的选择和切换码率，以适应不同带宽条件下的播放。不过HLS的这种技术特点，决定了它的延迟一般总是会高于普通的流媒体直播协议。但 Apple 在 WWDC 2019 发布了新的解决方案，可以将延迟从8秒降低到1至2秒。具体可以查看Introducing Low-Latency HLS。 5.3 什么是视频 ?视频其实就是快速播放一连串连续的图片。每一张图片，我们称为一帧。只要每秒钟帧的数据足够多，也即播放得足够快。比如每秒 30 帧，以人的眼睛的敏感程度，是看不出这是一张张独立的图片的，这就是我们常说的帧率（FPS）。 由于图片是由像素组成的,所以视频的数据量非常大,需要压缩。编码就是一种压缩的过程。 5.4 视频和图片的压缩过程的特点 空间冗余：图像的相邻像素之间有较强的相关性，一张图片相邻像素往往是渐变的，不是突变的，没必要每个像素都完整地保存，可以隔几个保存一个，中间的用算法计算出来。 时间冗余：视频序列的相邻图像之间内容相似。一个视频中连续出现的图片也不是突变的，可以根据已有的图片进行预测和推断。 时间冗余：视频序列的相邻图像之间内容相似。一个视频中连续出现的图片也不是突变的，可以根据已有的图片进行预测和推断。 编码冗余：不同像素值出现的概率不同，概率高的用的字节少，概率低的用的字节多，类似霍夫曼编码（Huffman Coding）的思路。 编码的过程如下: 5.5 视频编码的两大流派 流派一：ITU（International Telecommunications Union）的 VCEG（Video Coding Experts Group），这个称为国际电联下的 VCEG。既然是电信，可想而知，他们最初做视频编码，主要侧重传输。名词系列二，就是这个组织制定的标准。 流派二：ISO（International Standards Organization）的 MPEG（Moving Picture Experts Group），这个是 ISO 旗下的 MPEG，本来是做视频存储的。例如，编码后保存在 VCD 和 DVD 中。当然后来也慢慢侧重视频传输了。名词系列三，就是这个组织制定的标准。 后来，ITU-T（国际电信联盟电信标准化部门，ITU Telecommunication Standardization Sector）与 MPEG 联合制定了 H.264/MPEG-4 AVC。 5.6 看直播时到底发生了什么 网络协议将编码好的视频流，从主播端推送到服务器，在服务器上有个运行了同样协议的服务端来接收这些网络包，从而得到里面的视频流，这个过程称为接流。 服务端接到视频流之后，可以对视频流进行一定的处理，例如转码，也即从一个编码格式，转成另一种格式。因为观众使用的客户端千差万别，要保证他们都能看到直播。 流处理完毕之后，就可以等待观众的客户端来请求这些视频流。观众的客户端请求的过程称为拉流。如果有非常多的观众，同时看一个视频直播，那都从一个服务器上拉流，压力太大了，因而需要一个视频的分发网络，将视频预先加载到就近的边缘节点，这样大部分观众看的视频，是从边缘节点拉取的，就能降低服务器的压力。 当观众的客户端将视频流拉下来之后，就需要进行解码，也即通过上述过程的逆过程，将一串串看不懂的二进制，再转变成一帧帧生动的图片，在客户端播放出来。 5.6.1 编码的实现(1) 帧的分类虽然我们说视频是一张张图片的序列，但是如果每张图片都完整，就太大了，因而会将视频序列分成三种帧。 I 帧，也称关键帧。里面是完整的图片，只需要本帧数据，就可以完成解码。 P 帧，前向预测编码帧。P 帧表示的是这一帧跟之前的一个关键帧（或 P 帧）的差别，解码时需要用之前缓存的画面，叠加上和本帧定义的差别，生成最终画面。 B 帧，双向预测内插编码帧。B 帧记录的是本帧与前后帧的差别。要解码 B 帧，不仅要取得之前的缓存画面，还要解码之后的画面，通过前后画面的数据与本帧数据的叠加，取得最终的画面。 I 帧最完整，B 帧压缩率最高，而压缩后帧的序列，应该是在 IBBP 的间隔出现的。这就是通过时序进行编码。 在一帧中，分成多个片，每个片中分成多个宏块，每个宏块分成多个子块，这样将一张大的图分解成一个个小块，可以方便进行空间上的编码。 (2) 网络提取层单元（NALU)尽管时空非常立体的组成了一个序列，但是总归还是要压缩成一个二进制流。这个流是有结构的，是一个个的网络提取层单元（NALU，Network Abstraction Layer Unit）。变成这种格式就是为了传输，因为网络上的传输，默认的是一个个的包，因而这里也就分成了一个个的单元。 每一个 NALU 首先是一个起始标识符，用于标识 NALU 之间的间隔；然后是 NALU 的头，里面主要配置了 NALU 的类型；最终 Payload 里面是 NALU 承载的数据。 在 NALU 头里面，主要的内容是类型 NAL Type。 0x07 表示 SPS，是序列参数集， 包括一个图像序列的所有信息，如图像尺寸、视频格式等。 0x08 表示 PPS，是图像参数集，包括一个图像的所有分片的所有相关信息，包括图像类型、序列号等。 在传输视频流之前，必须要传输这两类参数，不然无法解码。为了保证容错性，每一个 I 帧前面，都会传一遍这两个参数集合 如果 NALU Header 里面的表示类型是 SPS 或者 PPS，则 Payload 中就是真正的参数集的内容。如果类型是帧，则 Payload 中才是正的视频数据，当然也是一帧一帧存放的，前面说了，一帧的内容还是挺多的，因而每一个 NALU 里面保存的是一片。 一个视频，可以拆分成一系列的帧，每一帧拆分成一系列的片，每一片都放在一个 NALU 里面，NALU 之间都是通过特殊的起始标识符分隔，在每一个 I 帧的第一片前面，要插入单独保存 SPS 和 PPS 的 NALU，最终形成一个长长的 NALU 序列。 5.6.2 推流显然上面所说的二进制流是不能直接在网上传输的,还需要将其打包成网络包进行发送,用到了 RTMP 协议。 RTMP 是基于 TCP 的，因而肯定需要双方建立一个 TCP 的连接。在有 TCP 的连接的基础上，还需要建立一个 RTMP 的连接，也即在程序里面，你需要调用 RTMP 类库的 Connect 函数，显示创建一个连接。 为什么RTMP要建立单独连接?主要就是协商两个事情: 一个是版本号，如果客户端、服务器的版本号不一致，则不能工作。 另一个就是时间戳，视频播放中，时间是很重要的，后面的数据流互通的时候，经常要带上时间戳的差值，因而一开始双方就要知道对方的时间戳。 RTMP建立连接的过程如图 首先，客户端发送 C0 表示自己的版本号，不必等对方的回复，然后发送 C1 表示自己的时间戳。 服务器只有在收到 C0 的时候，才能返回 S0，表明自己的版本号，如果版本不匹配，可以断开连接。 服务器发送完 S0 后，也不用等什么，就直接发送自己的时间戳 S1。客户端收到 S1 的时候，发一个知道了对方时间戳的 ACK C2。同理服务器收到 C1 的时候，发一个知道了对方时间戳的 ACK S2。于是，握手完成。 握手之后，双方需要互相传递一些控制信息，例如 Chunk 块的大小、窗口大小等。 传输数据真正传输数据的时候，还是需要创建一个流 Stream，然后通过这个 Stream 来推流 publish。推流的过程，就是将 NALU 放在 Message 里面发送，这个也称为 RTMP Packet 包。 Message 的格式就像这样。 发送的时候，去掉 NALU 的起始标识符。因为这部分对于 RTMP 协议来讲没有用。接下来，将 SPS 和 PPS 参数集封装成一个 RTMP 包发送，然后发送一个个片的 NALU。 RTMP 在收发数据的时候并不是以 Message 为单位的，而是把 Message 拆分成 Chunk 发送，而且必须在一个 Chunk 发送完成之后，才能开始发送下一个 Chunk。每个 Chunk 中都带有 Message ID，表示属于哪个 Message，接收端也会按照这个 ID 将 Chunk 组装成 Message。可以设置 Chunk 块大小,将大的消息变为小的块再发送，可以在低带宽的情况下，减少网络拥塞。 整个推流过程如下: 这个时候，大量观看直播的观众就可以通过 RTMP 协议从流媒体服务器上拉取,如果用户量巨大并且都在同一个服务器拉取,服务器的压力就很大,而且用户分布在全国甚至全球，如果都去统一的一个地方下载，也会时延比较长，需要有分发网络。 分发网络分为中心和边缘两层。边缘层服务器部署在全国各地及横跨各大运营商里，和用户距离很近。中心层是流媒体服务集群，负责内容的转发。智能负载均衡系统，根据用户的地理位置信息，就近选择边缘服务器，为用户提供推 / 拉流服务。中心层也负责转码服务，例如，把 RTMP 协议的码流转换为 HLS 码流。 5.6.3 拉流用户先读到的是 H.264 的解码参数，例如 SPS 和 PPS，然后对收到的 NALU 组成的一个个帧，进行解码，交给播发器播放，一个绚丽多彩的视频画面就出来了。 5.7 总结 视频名词比较多，编码两大流派达成了一致，都是通过时间、空间的各种算法来压缩数据； 压缩好的数据，为了传输组成一系列 NALU，按照帧和片依次排列； 排列好的 NALU，在网络传输的时候，要按照 RTMP 包的格式进行包装，RTMP 的包会拆分成 Chunk 进行传输； 推送到流媒体集群的视频流经过转码和分发，可以被客户端通过 RTMP 协议拉取，然后组合为 NALU，解码成视频格式进行播放。 参考文章五种常见流媒体协议 : https://www.jianshu.com/p/d71ceef679de 流媒体协议介绍 : https://blog.csdn.net/u013008311/article/details/80405241 6. P2P协议6.1 以前下载数据的方式方式一, 最简单的方式就是通过 HTTP 进行下载。但是通过浏览器下载的时候，只要文件稍微大点，下载的速度就奇慢无比。 方式二, 还有种下载文件的方式，就是通过 FTP，也即文件传输协议。FTP 采用两个 TCP 连接来传输一个文件。 控制连接：服务器以被动的方式，打开众所周知用于 FTP 的端口 21，客户端则主动发起连接。该连接将命令从客户端传给服务器，并传回服务器的应答。常用的命令有：list——获取文件目录；reter——取一个文件；store——存一个文件。 数据连接：每当一个文件在客户端与服务器之间传输时，就创建一个数据连接。 FTP 的两种工作模式每传输一个文件，都要建立一个全新的数据连接。FTP 有两种工作模式，分别是主动模式（PORT）和被动模式（PASV），这些都是站在 FTP 服务器的角度来说的。 主动模式下 客户端随机打开一个大于 1024 的端口 N，向服务器的命令端口 21 发起连接，同时开放 N+1 端口监听，并向服务器发出 “port N+1” 命令; 由服务器从自己的数据端口 20，主动连接到客户端指定的数据端口 N+1。 被动模式下 当开启一个 FTP 连接时，客户端打开两个任意的本地端口 N（大于 1024）和 N+1。第一个端口连接服务器的 21 端口，提交 PASV 命令。 然后，服务器会开启一个任意的端口 P（大于 1024），返回“227 entering passive mode”消息，里面有 FTP 服务器开放的用来进行数据传输的端口。 客户端收到消息取得端口号之后，会通过 N+1 号端口连接服务器的端口 P，然后在两个端口之间进行数据传输。 上面说了 HTTP 下载和 FTP 下载，这两种方式都有一个大缺点-难以解决单一服务器的带宽压力。因为它们使用的都是传统 C/S 结构，这种结构会随着客户端的增多，下载速度越来越慢。这在当今互联网世界显然是不合理的，我们期望能实现“下载人数越多，下载速度不变甚至更快”的愿望。 6.2 P2PP2P 就是 peer-to-peer。资源开始并不集中地存储在某些设备上，而是分散地存储在多台设备上,这些设备我们称为 peer。 在下载一个文件时，只要得到那些已经存在了文件的 peer 地址，并和这些 peer 建立点对点的连接，就可以就近下载文件，而不需要到中心服务器上。一旦下载了文件，你的设备也就称为这个网络的一个 peer，你旁边的那些机器也可能会选择从你这里下载文件。即你自己也加入了这个 P2P 的网络，自己从别人那里下载，同时也提供给其他人下载 通过这种方式解决上面 C/S 结构单一服务器带宽压力问题。如果使用过 P2P2 软件，例如 BitTorrent，你就会看到自己网络不仅有下载流量，还有上传流量，也就是说你加入了这个 P2P 网络，自己可以从这个网络里下载，同时别人也可以从你这里下载。这样就实现了，下载人数越多，下载速度越快的愿望。 6.3 种子（.torrent）文件当你想下载一个文件的时候，怎么知道哪些 peer 有这个文件呢？这就用到种子，也即.torrent 文件。.torrent 文件由两部分组成，分别是：announce（tracker URL）和文件信息。 文件信息里面有这些内容: info 区：这里指定的是该种子有几个文件、文件有多长、目录结构，以及目录和文件的名字。 Name 字段：指定顶层目录名字。 每个段的大小：BitTorrent（简称 BT）协议把一个文件分成很多个小段，然后分段下载。 段哈希值：将整个种子中，每个段的 SHA-1 哈希值拼在一起。 下载过程如下: 下载时，BT 客户端首先解析.torrent 文件，得到 tracker 地址，然后连接 tracker 服务器。tracker 服务器回应下载者的请求，将其他下载者（包括发布者）的 IP 提供给下载者。 下载者再连接其他下载者，根据.torrent 文件，两者分别对方告知自己已经有的块，然后交换对方没有的数据。此时不需要其他服务器参与，并分散了单个线路上的数据流量，因此减轻了服务器的负担。 下载者每得到一个块，需要算出下载块的 Hash 验证码，并与.torrent 文件中的对比。如果一样，则说明块正确，不一样则需要重新下载这个块。这种规定是为了解决下载内容的准确性问题。 这种方式特别依赖 tracker。tracker 需要收集下载者信息的服务器，并将此信息提供给其他下载者，使下载者们相互连接起来，传输数据。虽然下载的过程是非中心化的，但是加入这个 P2P 网络的时候，都需要借助 tracker 中心服务器，这个服务器是用来登记有哪些用户在请求哪些资源。所以,这种工作方式有一个弊端，一旦 tracker 服务器出现故障或者线路遭到屏蔽，BT 工具就无法正常工作了。 6.4 去中心化网络（DHT）那能不能彻底非中心化呢？是，后来就有了一种叫作DHT（Distributed Hash Table），这个网络中，每个加入 DHT 网络的人，都要负责存储这个网络里的资源信息和其他成员的联系信息，相当于所有人一起构成了一个庞大的分布式存储数据库。 而 *Kedemlia 协议 *就是一种著名的 DHT 协议。 任何一个 BitTorrent 启动之后，它都有两个角色。 一个是 peer，监听一个 TCP 端口，用来上传和下载文件，这个角色表明，我这里有某个文件。 另一个角色 DHT node，监听一个 UDP 的端口，通过这个角色，这个节点加入了一个 DHT 的网络。 在 DHT 网络里面，每一个 DHT Node 都有一个 ID。这个 ID 是一个长字符串。每个 DHT Node 都有责任掌握一些“知识”，也就是文件索引。也就是说，每个节点要知道哪些文件是保存哪些节点上的。注意，这里它只需要有这些“知识”就可以了，而它本身不一定就是保存这个文件的节点。 Node ID 和文件哈希值每个 DHT Node 不会有全局的“知识”，也就是说它不知道所有的文件保存位置，只需要知道一部分。这里的一部分，就是通过哈希算法计算出来的。每个文件可以计算出一个哈希值，而 DHT node 的 ID 是和哈希值相同长度的串。 DHT 算法是这样规定的：如果一个文件计算出一个哈希值，则和这个哈希值一样的那个 DHT node，就有责任知道从哪里下载这个文件，即便它自己没保存这个文件。当然不一定这么巧，总能找到和哈希值一模一样的，有可能一模一样的 DHT node 也下线了，所以 DHT 算法还规定：除了一模一样的那个 DHT node 应该知道，ID 和这个哈希值非常接近的 N 个 DHT node 也应该知道。 DHT网络如下: 文件 1 通过哈希运算，得到匹配 ID 的 DHT node 为 node C，当然还会有其他的，我这里没有画出来。所以，node C 有责任知道文件 1 的存放地址，虽然 node C 本身没有存放文件 1。 同理，文件 2 通过哈希运算，得到匹配 ID 的 DHT node 为 node E，但是 node D 和 E 的 ID 值很近，所以 node D 也知道。当然，文件 2 本身没有必要一定在 node D 和 E 里，但是碰巧这里就在 E 那有一份。 接下来一个新的节点 node new 上线了。如果想下载文件 1，它首先要加入 DHT 网络，如何加入呢？ 在这种模式下，种子.torrent 文件里面就不再是 tracker 的地址了，而是一个 list 的 node 的地址，而所有这些 node 都是已经在 DHT 网络里面的。当然随着时间的推移，很可能有退出的，有下线的，但是我们假设，不会所有的都联系不上，总有一个能联系上。node new 只要在种子里面找到一个 DHT node，就加入了网络。node new 会计算文件 1 的哈希值，并根据这个哈希值了解到，和这个哈希值匹配，或者很接近的 node 上知道如何下载这个文件， 但是 node new 不知道怎么联系上 node C，因为种子里面的 node 列表里面很可能没有 node C，但是它可以问，DHT 网络特别像一个社交网络，Node new 会去它能联系上的 Node 问，你们知道 Node C 的联系方式吗？ 在 DHT 网络中，每个 node 都保存了一定的联系方式，但是肯定没有 node 的所有联系方式。DHT 网络中，节点之间通过互相通信，也会交流联系方式，也会删除联系方式。和人们的方式一样，你有你的朋友圈，你的朋友有它的朋友圈，你们互相加微信，就互相认识了，过一段时间不联系，就删除朋友关系。 在社交网络中，还有个著名的六度理论，就是说社交网络中的任何两个人的直接距离不超过六度，也就是即使你想联系特朗普，最多通过六个人就能够联系上。 所以，Node New 想联系 Node C，就去万能的朋友圈去问，并且求转发，朋友再问朋友，直到找到 C。如果最后找不到 C，但是能找到离 C 很近的节点，也可以通过 C 的相邻节点下载文件 1。 在 Node C上，告诉 Node new，要下载文件 1，可以去 B、D、F，这里我们假设 Node new 选择了 Node B，那么新节点就和 B 进行 peer 连接，开始下载。它一旦开始下载，自己本地也有文件 1 了，于是，Node new 就告诉 C 以及 C 的相邻节点，我也有文件 1 了，可以将我加入文件 1 的拥有者列表了。 DHT Node ID 以及文件哈希值是什么？其实，我们可以将节点 ID 理解为一个 160bits（20字节）的字符串，文件的哈希也使用这样的字符串。 所谓 ID 相似，具体到什么程度算相似？在 Kademlia 网络中，两个节点的距离是通过异或（XOR）计算的。每个节点都有一个哈希 ID，这个 ID 由 20 个字符，160 bits 位组成。这里，我们就用一个 5 bits ID 来举例。我们假设，节点 A 的 ID 是 01010，节点 B 的 ID 是 01001，则： 距离 d = A XOR B = 01010 XOR 00011 = 01001 = 9 哈希值接近，可以理解为距离接近，也即，和这个节点距离近的 N 个节点要知道文件的保存位置。要注意的是，这个距离不是地理位置，因为在 Kademlia 网络中，位置近不算近，ID 近才算近。我们可以将这个距离理解为社交距离，也就是在朋友圈中的距离，或者社交网络中的距离。这个和你的空间位置没有多少关系，和人的经历关系比较大。 6.5 DHT 网络节点关系的维护就像人一样，虽然我们常联系的只有少数，但是朋友圈肯定是远近都有。DHT 网络的朋友圈也一样，远近都有，并且按距离分层。 假设某个节点的 ID 为 01010，如果一个节点的 ID，前面所有位数都与它相同，只有最后 1 位不停，这样的节点只有 1 个，为 01011。与基础节点的异或值为 00001，也就是距离为 1。那么对于 01010 而言，这样的节点归为第一层节点，也就是k-buket 1。 类似的，如果一个节点的 ID，前面所有位数和基础节点都相同，从倒数第 2 位开始不同，这样的节点只有 2 个，即 01000 和 01001，与基础节点的亦或值为 00010 和 00011，也就是距离为 2 和 3。这样的节点归为第二层节点，也就是k-bucket 2。 所以，我们可以总结出以下规律： 如果一个节点的 ID，前面所有位数相同，从倒数第 i 位开始不同，这样的节点只有 2^(i-1) 个，与基础节点的距离范围为 [2^(i-1), 2^i]，对于原始节点而言，这样的节点归为k-bucket i。 6.6 DHT 网络中查找好友 假设，Node A 的 ID 为 00110，要找 B（10000），异或距离为 10110，距离范围在 [2^4, 2^5)，这就说明 B 的 ID 和 A 的从第 5 位开始不同，所以 B 可能在 k-bucket 5 中。 然后，A 看看自己的 k-bucket 5 有没有 B，如果有，结束查找。如果没有，就在 k-bucket 5 里随便找一个 C。因为是二进制，C、B 都和 A 的第 5 位不停，那么 C 的 ID 第5 位肯定与 B 相同，即它与 B 的距离小于 2^4，相当于 A、B 之间的距离缩短了一半以上。 接着，再请求 C，在 C 的通讯里里，按同样的查找方式找 B，如果 C 找到了 B，就告诉 A。如果 C 也没有找到 B，就按同样的搜索方法，在自己的通讯里里找到一个离 B 更近一步的 D（D、B 之间距离小于 2^3），把 D 推荐给 A，A 请求 D 进行下一步查找。 Kademlia 这种查询机制，是通过折半查找的方式来收缩范围，对于总的节点数目为 N 的网络，最多只需要 log2(N) 次查询，就能够找到目标。 如下图，A 节点找 B 节点，最坏查找情况： 图中过程如下： A 和 B 的每一位都不一样，所以相差 31，A 找到的朋友 C，不巧正好在中间，和 A 的距离是 16，和 B 的距离是 15； C 去自己朋友圈找，碰巧找到了 D，距离 C 为 8，距离 B 为 7； D 去自己朋友圈找，碰巧找到了 E，距离 D 为 4，距离 B 为 3； E 在自己朋友圈找，找到了 F，距离 E 为 2，距离 B 为 1； F 在距离为 1 的地方找到了 B。 节点的沟通在 Kademlia 算法中，每个节点下面 4 个指令： PING：测试一个节点是否在线。相当于打个电话，看还能打通不； STORE：要钱一个节点存储一份数据； FIND_NODE：根据节点 ID 查找一个节点； ** FIND_VALUE：**根据 KEY 查找一个数据，实则上和 FIND_NODE 非常类似。KEY 就是文件对应的哈希值，找到保存文件的节点。 节点的更新整个 DHT 网络，会通过相互通信，维护自己朋友圈好友的状态。 每个 bucket 里的节点，都按最后一次接触时间倒序排列。相当于，朋友圈里最近联系的人往往是最熟的； 每次执行四个指令中的任意一个都会触发更新； 当一个节点与自己接触时，检查它是否已经在 k-bucket 中。就是说是否已经在朋友圈。如果在，那么就将它移到 k-bucket 列表的最底，也就是最新的位置（刚联系过，就置顶下，方便以后多联系）。如果不在，就要考虑新的联系人要不要加到通讯录里面。假设通讯录已满，就 PING 一下列表最上面的节点（最旧的），如果 PING 通了，将旧节点移动到列表最底，并丢弃新节点（老朋友还是要留点情面的）。如 PING 不同，就删除旧节点，并将新节点加入列表（联系不上的老朋友还是删掉吧）。 通过上面这个机制，保证了任意节点的加入和离开都不影响整体网络。 6.7 总结 下载一个文件可以通过 HTTP 或 FTP。这两种都是集中下载的方式，而 P2P 则换了一种思路，采用非中心化下载的方式； **P2P 有两种。一种是依赖于 Tracker 的，也就是元数据集中，文件数据分散。另一种是基于分布式的哈希算法，元数据和文件数据全部分散。 **","categories":[{"name":"随笔","slug":"随笔","permalink":"https://liangweijiang.github.io/categories/%E9%9A%8F%E7%AC%94/"}],"tags":[{"name":"网络协议","slug":"网络协议","permalink":"https://liangweijiang.github.io/tags/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/"}]},{"title":"数据结构学习笔记---栈","slug":"data-structure-03","date":"2020-03-26T10:12:11.725Z","updated":"2020-03-26T10:12:53.085Z","comments":true,"path":"2020/03/26/data-structure-03/","link":"","permalink":"https://liangweijiang.github.io/2020/03/26/data-structure-03/","excerpt":"","text":"学习自极客时间的《数据结构与算法之美》 作者：王争 栈一. 什么是栈?了解枪的弹夹的人应该很清楚,弹夹就是一种栈,我们将子弹压入弹夹中,最后压入的那颗子弹,总是最先发射出去。后进者先出，先进者后出，这就是典型的“栈”结构。 从栈的操作特性上来看，栈是一种“操作受限”的线性表，只允许在一端插入和删除数据。 事实上，从功能上来说，数组或链表确实可以替代栈，但你要知道，特定的数据结构是对特定场景的抽象，而且，数组或链表暴露了太多的操作接口，操作上的确灵活自由，但使用时就比较不可控，自然也就更容易出错。 二. 如何实现一个“栈”？栈既可以用数组来实现，也可以用链表来实现。用数组实现的栈，我们叫作顺序栈，用链表实现的栈，我们叫作链式栈。 1、顺序栈的实现这里因为python中没有数组这一数据结构,所以展示java的代码: 12345678910111213141516171819202122232425262728293031323334// 基于数组实现的顺序栈public class ArrayStack &#123; private String[] items; // 数组 private int count; // 栈中元素个数 private int n; //栈的大小 // 初始化数组，申请一个大小为n的数组空间 public ArrayStack(int n) &#123; this.items = new String[n]; this.n = n; this.count = 0; &#125; // 入栈操作 public boolean push(String item) &#123; // 数组空间不够了，直接返回false，入栈失败。 if (count == n) return false; // 将item放到下标为count的位置，并且count加一 items[count] = item; ++count; return true; &#125; // 出栈操作 public String pop() &#123; // 栈为空，则直接返回null if (count == 0) return null; // 返回下标为count-1的数组元素，并且栈中元素个数count减一 String tmp = items[count-1]; --count; return tmp; &#125;&#125; 2、链式栈的实现 首先需要创建一个节点的类 12345class Node: def __init__(self, value: int): self._value = value self._next = None 然后入栈,首先初始化栈为 None, 然后每一次入栈都更新栈顶元素。 12345678910class LinkedStack: \"\"\"A stack based upon singly-linked list. \"\"\" def __init__(self): self._top: Node = None def push(self, value: int): new_top = Node(value) new_top._next = self._top self._top = new_top 出栈,首先要判断链表是否为空,若不为空,则弹出栈顶元素,然后栈顶元素的下一元素变为栈顶元素 12345def pop(self) -&gt; Optional[int]: if self._top: value = self._top._data self._top = self._top._next return value 不管是顺序栈还是链式栈，我们存储数据只需要一个大小为 n 的数组就够了。在入栈和出栈过程中，只需要一两个临时变量存储空间，所以空间复杂度是 O(1)。 注意，这里存储数据需要一个大小为 n 的数组，并不是说空间复杂度就是 O(n)。因为，这 n 个空间是必须的，无法省掉。所以我们说空间复杂度的时候，是指除了原本的数据存储空间外，算法运行还需要额外的存储空间。 3、 支持动态扩容的顺序栈基于数组实现的栈，是一个固定大小的栈，也就是说，在初始化栈时需要事先指定栈的大小。当栈满之后，就无法再往栈里添加数据了。尽管链式栈的大小不受限，但要存储 next 指针，内存消耗相对较多。如果要实现一个支持动态扩容的栈，我们只需要底层依赖一个支持动态扩容的数组就可以了。当栈满了之后，我们就申请一个更大的数组，将原来的数据搬移到新数组中。 如果当前栈大小为 K，并且已满，当再有新的数据要入栈时，就需要重新申请 2 倍大小的内存，并且做 K 个数据的搬移操作，然后再入栈。但是，接下来的 K-1 次入栈操作，我们都不需要再重新申请内存和搬移数据，所以这 K-1 次入栈操作都只需要一个 simple-push 操作就可以完成。这 K 次入栈操作，总共涉及了 K 个数据的搬移，以及 K 次 simple-push 操作。将 K 个数据搬移均摊到 K 次入栈操作，那每个入栈操作只需要一个数据搬移和一个 simple-push 操作。以此类推，入栈操作的均摊时间复杂度就为 O(1)。 栈的运用1. 栈在函数调用中的应用操作系统给每个线程分配了一块独立的内存空间，这块内存被组织成“栈”这种结构, 用来存储函数调用时的临时变量。每进入一个函数，就会将临时变量作为一个栈帧入栈，当被调用函数执行完成，返回之后，将这个函数对应的栈帧出栈。 12345678910111213141516int main() &#123; int a = 1; int ret = 0; int res = 0; ret = add(3, 5); res = a + ret; printf(\"%d\", res); reuturn 0;&#125;int add(int x, int y) &#123; int sum = 0; sum = x + y; return sum;&#125; 首先main函数进栈,因为main方法调用了add方法,所以add函数也进栈。 当add方法执行完后add方法出栈，返回值，main继续执行，当main执行完成后main也出栈。所以他们的出栈顺序是 add –&gt; main。 函数的递归也离不开栈,递归的底层实现就是利用栈的数据结构。 2. 栈在表达式求值中的应用当机器对一个表达式求值的时候，编译器就是通过两个栈来实现的。其中一个保存操作数的栈，另一个是保存运算符的栈。我们从左向右遍历表达式，当遇到数字，我们就直接压入操作数栈；当遇到运算符，就与运算符栈的栈顶元素进行比较。如果比运算符栈顶元素的优先级高，就将当前运算符压入栈；如果比运算符栈顶元素的优先级低或者相同，从运算符栈中取栈顶运算符，从操作数栈的栈顶取 2 个操作数，然后进行计算，再把计算完的结果压入操作数栈，继续比较。 150. 逆波兰表达式求值 3. 栈在括号匹配中的应用 们用栈来保存未匹配的左括号，从左到右依次扫描字符串。当扫描到左括号时，则将其压入栈中；当扫描到右括号时，从栈顶取出一个左括号。如果能够匹配，比如“(”跟“)”匹配，“[”跟“]”匹配，“{”跟“}”匹配，则继续扫描剩下的字符串。如果扫描的过程中，遇到不能配对的右括号，或者栈中没有数据，则说明为非法格式。 20. 有效的括号 栈其他的相关题目155. 最小栈232. 用栈实现队列","categories":[{"name":"随笔","slug":"随笔","permalink":"https://liangweijiang.github.io/categories/%E9%9A%8F%E7%AC%94/"}],"tags":[{"name":"栈","slug":"栈","permalink":"https://liangweijiang.github.io/tags/%E6%A0%88/"}]},{"title":"每日一道算法之--回溯算法总结和全排列问题","slug":"algorithm-backtrack","date":"2020-03-26T06:23:48.767Z","updated":"2020-03-26T06:28:02.075Z","comments":true,"path":"2020/03/26/algorithm-backtrack/","link":"","permalink":"https://liangweijiang.github.io/2020/03/26/algorithm-backtrack/","excerpt":"","text":"回溯算法“回溯”算法也叫“回溯搜索”算法，主要用于在一个庞大的空间里搜索我们所需要的问题的解。我们每天使用的“搜索引擎”就是帮助我们在庞大的互联网上搜索我们需要的信息。“搜索”引擎的“搜索”和“回溯搜索”算法的“搜索”意思是一样的。 “回溯”指的是“状态重置”，可以理解为“回到过去”、“恢复现场”，是在编码的过程中，是为了节约空间而使用的一种技巧。而回溯其实是“深度优先遍历”特有的一种现象。之所以是“深度优先遍历”，是因为我们要解决的问题通常是在一棵树上完成的，在这棵树上搜索需要的答案，一般使用深度优先遍历。 回溯算法的模板: 1234567891011class Solution: def permute(self, nums: List[int]) -&gt; List[List[int]]: res = [] def backtrack(nums, tmp): if not nums: res.append(tmp) return for i in range(len(nums)): backtrack(nums[:i] + nums[i+1:], tmp + [nums[i]]) backtrack(nums, []) return res 这个模板适合许多利用回溯方法解决的问题，如全排列，组合总和，子集等问题。 全排列力扣第46题 ： https://leetcode-cn.com/problems/permutations/ 给定一个 没有重复 数字的序列，返回其所有可能的全排列。 示例: 输入: [1,2,3]输出:[ [1,2,3], [1,3,2], [2,1,3], [2,3,1], [3,1,2], [3,2,1]] 利用回溯解决可以看出,输入的数组中,所有元素排列组合,每一个元素都有可能在第一位,第二位……第N位,所以可以通过回溯算法,将所有的可能都列举出来。 创建一个列表res保留结果,构建回溯函数,参数分别为保留当前组合的列表 combination, 以及剩下的元素； 当剩下的元素为0时,证明所有元素都已经考虑进去了,就可以将该组合加进res中； 当剩下的元素不为空时,证明组合还没有结束,继续往下一层考虑。 12345678910class Solution: def permute(self, nums: List[int]) -&gt; List[List[int]]: res = [] def backtrack(combination, nums): if not nums: res.append(combination) for i in range(len(nums)): backtrack(combination + [nums[i]], nums[:i] + nums[i+1:]) backtrack([], nums) return res 复杂度分析 全排列II力扣第47题 : https://leetcode-cn.com/problems/permutations-ii/ 给定一个可包含重复数字的序列，返回所有不重复的全排列。 示例: 输入: [1,1,2]输出:[ [1,1,2], [1,2,1], [2,1,1]] 123456789101112131415class Solution: def permuteUnique(self, nums: List[int]) -&gt; List[List[int]]: res = [] # 排序去重 nums.sort() def backtrack(tmp, nums): if not nums: res.append(tmp) for i in range(len(nums)): # 去重 if i &gt; 0 and nums[i] == nums[i -1]: continue backtrack(tmp + [nums[i]], nums[:i] + nums[i+1:]) backtrack([], nums) return res 相似题目39.组合总和 40. 组合总和 II 78. 子集 90. 子集 II 17. 电话号码的字母组合","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://liangweijiang.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"回溯","slug":"回溯","permalink":"https://liangweijiang.github.io/tags/%E5%9B%9E%E6%BA%AF/"}]},{"title":"《趣谈网络协议》学习笔记之--最常用的运用层(上)","slug":"NetworkProtocol-06","date":"2020-03-25T12:15:04.903Z","updated":"2020-03-27T08:46:35.573Z","comments":true,"path":"2020/03/25/NetworkProtocol-06/","link":"","permalink":"https://liangweijiang.github.io/2020/03/25/NetworkProtocol-06/","excerpt":"","text":"学习自极客时间《趣谈网络协议》 作者：刘超 1. HTTP协议1.1 HTTP 请求的准备我们在浏览器上输入的网址,比如https://www.baidu.com/, 就是一个URL,也叫做统一资源定位符。之所以叫统一，是因为它是有格式的。HTTP 称为协议，www.baidu.com 是一个域名，表示互联网上的一个位置。正是因为这个东西是统一的，所以当你把这样一个字符串输入到浏览器的框里的时候，浏览器才知道如何进行统一处理。 浏览器将域名发送给DNS服务器,然后DNS将域名解析为IP地址,HTTP 是基于 TCP 协议的，先建立 TCP 连接,目前使用的 HTTP 协议大部分都是 1.1。在 1.1 的协议里面，默认是开启了 Keep-Alive 的，这样建立的 TCP 连接，就可以在多次请求中复用。 1.2 HTTP 请求的构建建立了连接后,就可以发送http请求。请求的格式如下： （1）请求行 方法 GET: GET 就是去服务器获取一些资源。对于访问网页来讲，要获取的资源往往是一个页面。其实也有很多其他的格式，比如说返回一个 JSON 字符串，到底要返回什么，是由服务器端的实现决定的。 POST:它需要主动告诉服务端一些信息，而非获取。要告诉服务端什么呢？一般会放在正文里面。正文可以有各种各样的格式。常见的格式也是 JSON。 PUT: 向指定资源位置上传最新内容。但是，HTTP 的服务器往往是不允许上传文件的，所以 PUT 和 POST 就都变成了要传给服务器东西的方法。在实际使用过程中，这两者还会有稍许的区别。POST 往往是用来创建一个资源的，而 PUT 往往是用来修改一个资源的。 DELETE: 这个顾名思义就是用来删除资源的。 URL就是类似 https://www.baidu.com/ 的统一资源定位符 版本为http版本如1.1 （2）首部字段首部是 key value，通过冒号分隔。这里面，往往保存了一些非常重要的字段。 Accept-Charset： 表示客户端可以接受的字符集。防止传过来的是另外的字符集，从而导致出现乱码。 Content-Type： 指正文的格式。例如，我们进行 POST 的请求，如果正文是 JSON，那么我们就应该将这个值设置为 JSON。 Cache-control： 用来控制缓存的。当客户端发送的请求中包含 max-age 指令时，如果判定缓存层中，资源的缓存时间数值比指定时间的数值小，那么客户端可以接受缓存的资源；当指定 max-age 值为 0，那么缓存层通常需要将请求转发给应用集群。 If-Modified-Since： 也是一个关于缓存的。也就是说，如果服务器的资源在某个时间之后更新了，那么客户端就应该下载最新的资源；如果没有更新，服务端会返回“304 Not Modified”的响应，那客户端就不用下载了，也会节省带宽。 为什么要使用缓存 减少相应延迟：因为请求从缓存服务器（离客户端更近）而不是源服务器被相应，这个过程耗时更少，让web服务器看上去相应更快； 减少网络带宽消耗：当副本被重用时会减低客户端的带宽消耗；客户可以节省带宽费用，控制带宽的需求的增长并更易于管理。 对于这种高并发场景下的系统，在真正的业务逻辑之前，都需要有个接入层，将这些静态资源的请求拦在最外面。 参考文献: https://zhuanlan.zhihu.com/p/90507417 1.3 HTTP 请求的发送HTTP 协议是基于 TCP 协议的，所以它使用面向连接的方式发送请求，通过 stream 二进制流的方式传给对方。当然，到了 TCP 层，它会把二进制流变成一个个报文段发送给服务器。 TCP层：在发送给每个报文段的时候，都需要对方有一个回应 ACK，来保证报文可靠地到达了对方。如果没有回应，那么 TCP 这一层会进行重新传输，直到可以到达。在tcp层报可能被传送多次，但对于http层是透明的。TCP 层发送每一个报文的时候，都需要加上自己的地址（即源地址）和它想要去的地方（即目标地址），将这两个信息放到 IP 头里面，交给 IP 层进行传输。 IP层： IP 层需要查看目标地址和自己是否是在同一个局域网。如果是，就发送 ARP 协议来请求这个目标地址对应的 MAC 地址，然后将源 MAC 和目标 MAC 放入 MAC 头，发送出去即可；如果不在同一个局域网，就需要发送到网关，还要需要发送 ARP 协议，来获取网关的 MAC 地址，然后将源 MAC 和网关 MAC 放入 MAC 头，发送出去。 链路层： 网关收到包发现 MAC 符合，取出目标 IP 地址，根据路由协议找到下一跳的路由器，获取下一跳路由器的 MAC 地址，将包发给下一跳路由器。这样路由器一跳一跳终于到达目标的局域网。这个时候，最后一跳的路由器能够发现，目标地址就在自己的某一个出口的局域网上。于是，在这个局域网上发送 ARP，获得这个目标地址的 MAC 地址，将包发出去。目标的机器发现 MAC 地址符合，就将包收起来；发现 IP 地址符合，根据 IP 头中协议项，知道自己上一层是 TCP 协议，于是解析 TCP 的头，里面有序列号，需要看一看这个序列包是不是我要的，如果是就放入缓存中然后返回一个 ACK，如果不是就丢弃。 TCP 头里面还有端口号，HTTP 的服务器正在监听这个端口号。于是，目标机器自然知道是 HTTP 服务器这个进程想要这个包，于是将包发给 HTTP 服务器。HTTP 服务器的进程看到，原来这个请求是要访问一个网页，于是就把这个网页发给客户端。 1.4 HTTP 返回的构建HTTP 的返回报文也是有一定格式的。这也是基于 HTTP 1.1 的。 (1) 状态行 版本: HTTP协议的版本号 状态码: （2）首部 Retry-After： 告诉客户端应该在多长时间以后再次尝试一下。“503 错误”是说“服务暂时不再和这个值配合使用”。 Content-Type: 指正文的格式。 HTTP 响应的发送和请求发送类似。 2. HTTP 2.0 HTTP 1.1 在应用层以纯文本的形式进行通信。每次通信都要带完整的 HTTP 的头，而且不考虑 pipeline 模式的话，每次的过程总是像上面描述的那样一去一回。这样在实时性、并发性上都存在问题。HTTP 2.0 会对 HTTP 的头进行一定的压缩，将原来每次都要携带的大量 key value 在两端建立一个索引表，对相同的头只发送索引表中的索引。 pipeLining 管道持久连接解决了连接复用问题，但还是存在着一个问题：在一个 TCP 连接中，同一时间只能够发送一个请求，并且需要等响应完成才能够发送第二个请求。因此 HTTP/1.1 制订了 PipeLining 管道，通过这个管道，浏览器的多个请求可以同时发到服务器，但是服务器的响应只能够一个接着一个的返回 ( 但各大浏览器有些不支持/默认关闭,因此这功能可以说是鸡肋)。因为每一条连接同时只能够返回一个响应，因此浏览器为了改善这种情况，会同时开启4~8个 TCP 连接进行发送请求。参考文章 : https://cloud.tencent.com/developer/article/1464264 另外，HTTP 2.0 协议将一个 TCP 的连接中，切分成多个流，每个流都有自己的 ID，而且流可以是客户端发往服务端，也可以是服务端发往客户端。它其实只是一个虚拟的通道。流是有优先级的。 HTTP 2.0 还将所有的传输信息分割为更小的消息和帧，并对它们采用二进制格式编码。常见的帧有 Header 帧，用于传输 Header 内容，并且会开启一个新的流。再就是 Data 帧，用来传输正文实体。多个 Data 帧属于同一个流。 通过这两种机制，HTTP 2.0 的客户端可以将多个请求分到不同的流中，然后将请求内容拆成帧，进行二进制传输。这些帧可以打散乱序发送， 然后根据每个帧首部的流标识符重新组装，并且可以根据优先级，决定优先处理哪个流的数据。发送多个请求的时候，使用 HTTP 1.1 就是串行的，但是如果使用 HTTP 2.0，就可以在一个连接里，客户端和服务端都可以同时发送多个请求或回应，而且不用按照顺序一对一对应。 HTTP 2.0 成功解决了 HTTP 1.1 的队首阻塞问题，同时，也不需要通过 HTTP 1.x 的 pipeline 机制用多条 TCP 连接来实现并行请求与响应；减少了 TCP 连接数对服务器性能的影响。 3. QUIC 协议QUIC（全称 Quick UDP Internet Connections，快速 UDP 互联网连接）是 Google 提出的一种基于 UDP 改进的通信协议，其目的是降低网络通信的延迟，提供更好的用户互动体验。QUIC 在应用层上，会自己实现快速连接建立、减少重传时延，自适应拥塞控制， 因为 HTTP 2.0 也是基于 TCP 协议的，TCP 协议在处理包时是有严格顺序的。当其中一个数据包遇到问题，TCP 连接需要等待这个包完成重传之后才能继续进行。虽然 HTTP 2.0 通过多个 stream，使得逻辑上一个 TCP 连接上的并行内容，进行多路数据的传输，然而这中间并没有关联的数据。一前一后，前面 stream 2 的帧没有收到，后面 stream 1 的帧也会因此阻塞。 机制一. 自定义连接机制 一条 TCP 连接是由四元组标识的，分别是源 IP、源端口、目的 IP、目的端口。一旦一个元素发生变化时，就需要断开重连，重新连接。 但是基于 UDP，就可以在 QUIC 自己的逻辑里面维护连接的机制，不再以四元组标识，而是以一个 64 位的随机数作为 ID 来标识，而且 UDP 是无连接的，所以当 IP 或者端口变化的时候，只要 ID 不变，就不需要重新建立连接。 机制二. 自定义重传机制 TCP 为了保证可靠性，通过使用序号和应答机制，来解决顺序问题和丢包问题。任何一个序号的包发过去，都要在一定的时间内得到应答，否则一旦超时，就会重发这个序号的包。利用自适应重传算法,通过采样往返时间 RTT 不断调整超时时间。 QUIC 也有个序列号，是递增的。任何一个序列号的包只发送一次，下次就要加一了。例如，发送一个包，序号是 100，发现没有返回；再次发送的时候，序号就是 101 了；如果返回的 ACK 100，就是对第一个包的响应。如果返回 ACK 101 就是对第二个包的响应，RTT 计算相对准确。 QUIC 既然是面向连接的，也就像 TCP 一样，是一个数据流，发送的数据在这个数据流里面有个偏移量 offset，可以通过 offset 查看数据发送到了哪里，这样只要这个 offset 的包没有来，就要重发；如果来了，按照 offset 拼接，还是能够拼成一个流。 机制三. 无阻塞的多路复用有了自定义的连接和重传机制，我们就可以解决上面 HTTP 2.0 的多路复用问题。同 HTTP 2.0 一样，同一条 QUIC 连接上可以创建多个 stream，来发送多个 HTTP 请求。但是，QUIC 是基于 UDP 的，一个连接上的多个 stream 之间没有依赖。这样，假如 stream2 丢了一个 UDP 包，后面跟着 stream3 的一个 UDP 包，虽然 stream2 的那个包需要重传，但是 stream3 的包无需等待，就可以发给用户。 机制四. 自定义流量控制TCP 的流量控制是通过滑动窗口协议。QUIC 的流量控制也是通过 window_update，来告诉对端它可以接受的字节数。但是 QUIC 的窗口是适应自己的多路复用机制的，不但在一个连接上控制窗口，还在一个连接中的每个 stream 控制窗口。 在 TCP 协议中，接收端的窗口的起始点是下一个要接收并且 ACK 的包，即便后来的包都到了，放在缓存里面，窗口也不能右移，因为 TCP 的 ACK 机制是基于序列号的累计应答，一旦 ACK 了一个系列号，就说明前面的都到了，所以只要前面的没到，后面的到了也不能 ACK，就会导致后面的到了，也有可能超时重传，浪费带宽。 QUIC 的 ACK 是基于 offset 的，每个 offset 的包来了，进了缓存，就可以应答，应答后就不会重发，中间的空挡会等待到来或者重发即可，而窗口的起始位置为当前收到的最大 offset，从这个 offset 到当前的 stream 所能容纳的最大缓存，是真正的窗口大小。显然，这样更加准确。 详细的可以上网或者看看文档啥的。。。 4. HTTPS 协议4.1 什么是 HTTPS?HTTPS协议 = HTTP协议 + SSL/TLS协议，在HTTPS数据传输的过程中，需要用SSL/TLS对数据进行加密和解密，需要用HTTP对加密后的数据进行传输，由此可以看出HTTPS是由HTTP和SSL/TLS一起合作完成的。 SSL的全称是Secure Sockets Layer，即安全套接层协议，是为网络通信提供安全及数据完整性的一种安全协议。SSL协议在1994年被Netscape发明，后来各个浏览器均支持SSL，其最新的版本是3.0 TLS的全称是Transport Layer Security，即安全传输层协议，最新版本的TLS（Transport Layer Security，传输层安全协议）是IETF（Internet Engineering Task Force，Internet工程任务组）制定的一种新的协议，它建立在SSL 3.0协议规范之上，是SSL 3.0的后续版本。在TLS与SSL3.0之间存在着显著的差别，主要是它们所支持的加密算法不同，所以TLS与SSL3.0不能互操作。 4.2 加密算法** 1、 对称加密** 对称加密又叫做私钥加密，即信息的发送方和接收方使用同一个密钥去加密和解密数据。对称加密的特点是算法公开、加密和解密速度快，适合于对大数据量进行加密，常见的对称加密算法有DES、3DES、TDEA、Blowfish、RC5和IDEA。 对称加密中用到的密钥叫做私钥，私钥表示个人私有的密钥，即该密钥不能被泄露。所以该密钥不能直接在网络上传输。 ** 2、 非对称加密** 对称加密的通信双方使用相同的密钥，如果一方的密钥遭泄露，那么整个通信就会被破解。而非对称加密使用一对密钥，即公钥和私钥，且二者成对出现。私钥被自己保存，不能对外泄露。公钥指的是公共的密钥，任何人都可以获得该密钥。用公钥或私钥中的任何一个进行加密，用另一个进行 由于加密和解密使用了两个不同的密钥，这就是非对称加密“非对称”的原因。非对称加密的缺点是加密和解密花费时间长、速度慢，只适合对少量数据进行加密。 4.3 数字证书不对称加密也会有同样的问题，如何将不对称加密的公钥给对方呢？一种是放在一个公网的地址上，让对方下载；另一种就是在建立连接的时候，传给对方。 怎么鉴别别人给你的公钥是对的。会不会有人冒充网站，发给你一个它的公钥。接下来，你和它所有的互通，看起来都是没有任何问题的。毕竟每个人都可以创建自己的公钥和私钥。这时候就需要权威的机构来保证公钥的真实性了，也就是权威机构颁发证书。这个权威机构称为CA（ Certificate Authority）。 数字签名算法我们现实生活中比如派出所给你开一个证明的时候，也会给你盖上一个章或者签名，从而保证证明的真实性。数字证书也一样，要保证证书的真实性，就要有权威机构的签名，称为数字签名。数字签名的算法就叫做签名算法。 数字签名是使用数字证书与信息加密技术、用于鉴别电子数据信息的技术，可通俗理解为加盖在电子文件上的“数字指纹”。数字证书是由权威公证的第三方认证机构（即CA，Certificate Authority）负责签发和管理的、个人或企业的网络数字身份证明。 要怎么签名才能保证是真的权威机构签名的呢？当然只有用只掌握在权威机构手里的东西签名了才行，这就是 CA 的私钥。 签名算法大概是这样工作的：一般是对信息做一个 Hash 计算，得到一个 Hash 值，这个过程是不可逆的，也就是说无法通过 Hash 值得出原来的信息内容。在把信息发送出去时，把这个 Hash 值加密后，作为一个签名和信息一起发出去。 要想验证证书，需要 CA 的公钥，问题是，你怎么确定 CA 的公钥就是对的呢？所以，CA 的公钥也需要更牛的 CA 给它签名，然后形成 CA 的证书。要想知道某个 CA 的证书是否可靠，要看 CA 的上级证书的公钥，能不能解开这个 CA 的签名。就像你不相信区公安局，可以打电话问市公安局，让市公安局确认区公安局的合法性。这样层层上去，直到全球皆知的几个著名大 CA，称为 root CA，做最后的背书。通过这种层层授信背书的方式，从而保证了非对称加密模式的正常运转。除此之外，还有一种证书，称为 Self-Signed Certificate，就是自己给自己签名。这个给人一种“我就是我，你爱信不信”的感觉。 4.4 HTTPS 的工作模式HTTPS单向认证（客户端认证服务端）流程(一共产生了三个随机数)： c-&gt;s,客户端发起加密通信请求，这个请求通常叫做 ClientHello请求，告知自己支持的协议版本号，加密算法，压缩算法，以及一个用于生成后续通信密钥的随机数； s-&gt;c,服务端响应，也叫作 ServerHello，确认加密通信协议，加密算法，以及一个用于生成后续通信密钥的随机数，还有网站证书； c-&gt;s,客户端在收到上一步服务端的响应之后，首先会检查证书的颁发者是否可信任，是否过期，域名是否一致，并且从操作系统的证书链中找出该证书的上一级证书（寻找授信的CA的过程是不断往上追溯的），并拿出服务端证书的公钥，然后验证签名和hash，如果验证失败，就会显示警告，我们经常在Chrome里面看到，“此网站有风险，是否继续什么的”。如果验证通过，客户端会向服务端发送一个称作 “pre-master-key” 的随机数，该随机数使用证书的公钥加密，以及编码改变通知（以后就用协商的密钥堆成加密通信了），客户端完成握手。 服务端在收到上一步客户端请求之后，也会确认我以后发给你的信息是加密的，并且完成握手。 此时，客户端有第一步自己生成的随机数，第二步收到服务端的随机数，第三步的 pre-master-key，服务端也是如此，他们就可以用这三个随机数使用约定的算法生成同一个会话密钥(session key )来加密以后的通信数据了。 上面的过程只包含了 HTTPS 的单向认证，也即客户端验证服务端的证书，是大部分的场景，也可以在更加严格安全要求的情况下，启用双向认证，双方互相验证证书。 SSL协议在握手阶段使用的是非对称加密，在传输阶段使用的是对称加密，也就是说在SSL上传送的数据是使用对称密钥加密的。因为非对称加密的速度缓慢，耗费资源。其实当客户端和主机使用非对称加密方式建立连接后，客户端和主机已经决定好了在传输过程使用的对称加密算法和关键的对称加密密钥，由于这个过程本身是安全可靠的，也即对称加密密钥是不可能被窃取盗用的，因此，保证了在传输过程中对数据进行对称加密也是安全可靠的，因为除了客户端和主机之外，不可能有第三方窃取并解密出对称加密密钥！如果有人窃听通信，他可以知道双方选择的加密方法，以及三个随机数中的两个。整个通话的安全，只取决于第三个随机数（pre-master secret）能不能被破解。 HTTPS的单向认证和双向认证单向认证：(1). 客户端保存着服务器端的证书并信任该证书即可(2). HTTPS一般是单向认证，这样可以让绝大部分人都访问你的站点双向认证：(1). 先决条件是有两个或两个以上的证书，一个是服务器端证书，另一个或多个是客户端证书(2). 服务器端保存着客户端的证书并信任该证书，客户端保存着服务器端的证书并信任该证书。这样，成功的情况下即可完成请求响应(3). 双向认证一般企业应用对接 4.5 重放与篡改有了加密和解密，黑客截获了包也打不开了，但是它可以发送 N 次。这个往往通过 Timestamp 和 Nonce 随机数联合起来，然后做一个不可逆的签名来保证。 Nonce 随机数保证唯一，或者 Timestamp 和 Nonce 合起来保证唯一，同样的，请求只接受一次，于是服务器多次受到相同的 Timestamp 和 Nonce，则视为无效即可。如果有人想篡改 Timestamp 和 Nonce，还有签名保证不可篡改性，如果改了用签名算法解出来，就对不上了，可以丢弃了。 4.6 问题思考HTTPS 协议比较复杂，沟通过程太繁复，这样会导致效率问题，有哪些优化方法？ CDN接入HTTPS增加的延时主要是传输延时RTT，RTT的特点是节点越近延时越少，CDN天然离用户最近，因此选择CDN作为HTTPS的接入口，能极大减少延时。 会话缓存利用会话缓存从而复用连接，基于会话缓存建立的HTTPS连接不需要服务器使用私钥解密获取Pre-master信息，可以省掉CPU的消耗。 硬件加速为接入服务器安装专用的SSL硬件加速卡。 远程解密本地接入消耗过多的CPU资源，考虑将最消耗CPU资源的RSA解密计算任务转移到其他服务器。 SPDY/HTTP2这个是通过修改协议本身，利用TLS/SSL带来的优势，来提升HTTPS的性能。 参考文章https协议 : https://www.jianshu.com/p/f9b8a3e62af1 网络&amp;浏览器 : https://blog.csdn.net/qq_33262202/article/details/90318419","categories":[{"name":"随笔","slug":"随笔","permalink":"https://liangweijiang.github.io/categories/%E9%9A%8F%E7%AC%94/"}],"tags":[{"name":"网络协议","slug":"网络协议","permalink":"https://liangweijiang.github.io/tags/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/"}]},{"title":"数据结构学习笔记---链表","slug":"data-structure-02","date":"2020-03-25T07:40:44.444Z","updated":"2020-03-28T10:18:19.451Z","comments":true,"path":"2020/03/25/data-structure-02/","link":"","permalink":"https://liangweijiang.github.io/2020/03/25/data-structure-02/","excerpt":"","text":"学习自极客时间的《数据结构与算法之美》 作者：王争 链表一. 链表和数组的区别 数组需要一块连续的内存空间来存储，对内存的要求比较高。如果我们申请一个 100MB 大小的数组，当内存中没有连续的、足够大的存储空间时，即便内存的剩余总可用空间大于 100MB，仍然会申请失败。 而链表恰恰相反，它并不需要一块连续的内存空间，它通过“指针”将一组零散的内存块串联起来使用，所以如果我们申请的是 100MB 大小的链表，根本不会有问题。其中，我们把内存块称为链表的“结点”。 二. 链表的插入和删除 在进行数组的插入、删除操作时，为了保持内存数据的连续性，需要做大量的数据搬移，所以时间复杂度是 O(n)。 而在链表中插入或者删除一个数据，我们并不需要为了保持内存的连续性而搬移结点，因为链表的存储空间本身就不是连续的。所以，在链表中插入和删除一个数据是非常快速的。 三. 链表的分类链表有很多种,如常见的单链表,双向链表和循环链表等。 ** 1. 单链表**为了将所有的结点串起来，每个链表的结点除了存储数据之外，还需要记录链上的下一个结点的地址。我们把这个记录下个结点地址的指针叫作后继指针 next。 我们习惯性地把第一个结点叫作头结点，把最后一个结点叫作尾结点。其中，头结点用来记录链表的基地址。有了它，我们就可以遍历得到整条链表。而尾结点特殊的地方是：指针不是指向下一个结点，而是指向一个空地址 NULL，表示这是链表上最后一个结点。 ** 2. 循环链表**单链表的尾指针是指向空地址的,而循环链表的尾指针是指向头指针的。循环链表的优点是从链尾到链头比较方便。当要处理的数据具有环型结构特点时，就特别适合采用循环链表。比如著名的约瑟夫问题。尽管用单链表也可以实现，但是用循环链表实现的话，代码就会简洁很多。 ** 3. 双向链表** 单向链表只有一个方向，结点只有一个后继指针 next 指向后面的结点。而双向链表，顾名思义，它支持两个方向，每个结点不止有一个后继指针 next 指向后面的结点，还有一个前驱指针 prev 指向前面的结点。 双向链表的高效体现在哪里删除操作在实际的软件开发中，从链表中删除一个数据无外乎这两种情况： 删除结点中“值等于某个给定值”的结点； 删除给定指针指向的结点。 对于单链表来说: 第一种情况,虽然删除的时间复杂度为O(1), 但是我们需要遍历节点并一一和给定的值对比,直到找到值等于给定值的结点,所以删除值等于给定值的结点对应的链表操作的总时间复杂度为 O(n)。 对于第二种情况，我们已经找到了要删除的结点，但是删除某个结点 q 需要知道其前驱结点，而单链表并不支持直接获取前驱结点，所以，为了找到前驱结点，我们还是要从头结点开始遍历链表，直到 p-&gt;next=q，说明 p 是 q 的前驱结点。 插入操作也是类似的。除了插入、删除操作有优势之外，对于一个有序链表，双向链表的按值查询的效率也要比单链表高一些。因为，我们可以记录上次查找的位置 p，每次查询时，根据要查找的值与 p 的大小关系，决定是往前还是往后查找，所以平均只需要查找一半的数据。 问题思考如果字符串是通过单链表来存储的，那该如何来判断是一个回文串呢？ 力扣第234题 : https://leetcode-cn.com/problems/palindrome-linked-list/submissions/ 使用快慢两个指针找到链表中点，慢指针每次前进一步，快指针每次前进两步。在慢指针前进的过程中，同时修改其 next 指针，使得链表前半部分反序。最后比较中点两侧的链表是否相等。 https://github.com/wangzheng0822/algo/blob/master/python/06_linkedlist/palindrome.py 关于链表的题目 单链表反转力扣第206题:https://leetcode-cn.com/problems/reverse-linked-list/ 链表中环的检测力扣第141题:https://leetcode-cn.com/problems/linked-list-cycle/ 两个有序的链表合并力扣第21题:https://leetcode-cn.com/problems/merge-two-sorted-lists/ k个有序的链表合并力扣第23题:https://leetcode-cn.com/problems/merge-k-sorted-lists/ 删除链表倒数第 n 个结点力扣第19题:https://leetcode-cn.com/problems/remove-nth-node-from-end-of-list/ 求链表的中间结点力扣第876题:https://leetcode-cn.com/problems/middle-of-the-linked-list/ 两两交换链表中的节点力扣第24题:https://leetcode-cn.com/problems/swap-nodes-in-pairs/ 我做完这几道题后,总结了一个道理,链表经常使用快慢指针来解决问题。","categories":[{"name":"随笔","slug":"随笔","permalink":"https://liangweijiang.github.io/categories/%E9%9A%8F%E7%AC%94/"}],"tags":[{"name":"链表","slug":"链表","permalink":"https://liangweijiang.github.io/tags/%E9%93%BE%E8%A1%A8/"}]},{"title":"每日一道算法之--电话号码的字母组合","slug":"algorithm-17","date":"2020-03-25T06:32:46.864Z","updated":"2020-03-25T06:33:35.649Z","comments":true,"path":"2020/03/25/algorithm-17/","link":"","permalink":"https://liangweijiang.github.io/2020/03/25/algorithm-17/","excerpt":"","text":"电话号码的字母组合力扣第17题:https://leetcode-cn.com/problems/letter-combinations-of-a-phone-number/ 给定一个仅包含数字 2-9 的字符串，返回所有它能表示的字母组合。给出数字到字母的映射如下（与电话按键相同）。注意 1 不对应任何字母。 示例: 输入：”23”输出：[“ad”, “ae”, “af”, “bd”, “be”, “bf”, “cd”, “ce”, “cf”].说明:尽管上面的答案是按字典序排列的，但是你可以任意选择答案输出的顺序。 1. 回溯 首先看一看数字对应的可能是3个字母或是4个字母。所以可以构建列表或者哈希表，因为列表的话索引值需要转化，所以适合构建哈希表。 给出如下回溯函数 backtrack(combination, next_digits) ，它将一个目前已经产生的组合 combination 和接下来准备要输入的数字 next_digits 作为参数。 一个数字下面有三或4中可能，所以每一种可能都要穷举出来，每递归一层输入的字符串就减小一个长度 12345678910111213141516171819202122232425class Solution: def letterCombinations(self, digits: str) -&gt; List[str]: if not digits: return [] phone = &#123; '2': 'abc', '3': 'def', '4': 'ghi', '5': 'jkl', '6': 'mno', '7': 'pqrs', '8': 'tuv', '9': 'wxyz', &#125; def backtrack(combination, next_digits): if len(next_digits) == 0: output.append(combination) else: for letter in phone[next_digits[0]]: backtrack(combination + letter, next_digits[1:]) output = [] if digits: backtrack(\"\", digits) return output 复杂度分析时间复杂度: O((3^N)*(4^M)),其中N为对应3个字母的数字,M为对应4个字母的数字。 空间复杂度：O((3^N)*(4^M))，每一个结果都需要保存。 队列 作者：z1m链接：https://leetcode-cn.com/problems/letter-combinations-of-a-phone-number/solution/hui-su-dui-lie-tu-jie-by-ml-zimingmeng/ 使用队列，先将输入的 digits 中第一个数字对应的每一个字母入队，然后将出队的元素与第二个数字对应的每一个字母组合后入队…直到遍历到 digits 的结尾。最后队列中的元素就是所求结果。 1234567891011121314151617181920class Solution: def letterCombinations(self, digits: str) -&gt; List[str]: if not digits: return [] phone = &#123; '2': 'abc', '3': 'def', '4': 'ghi', '5': 'jkl', '6': 'mno', '7': 'pqrs', '8': 'tuv', '9': 'wxyz', &#125; queue = [''] for digit in digits: for _ in range(len(queue)): tmp = queue.pop(0) for char in phone[digit]: queue.append(tmp + char) return queue 复杂度分析时间复杂度: O((3^N)*(4^M)),其中N为对应3个字母的数字,M为对应4个字母的数字。 空间复杂度：O((3^N)*(4^M))，每一个结果都需要保存。","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://liangweijiang.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"回溯","slug":"回溯","permalink":"https://liangweijiang.github.io/tags/%E5%9B%9E%E6%BA%AF/"},{"name":"队列","slug":"队列","permalink":"https://liangweijiang.github.io/tags/%E9%98%9F%E5%88%97/"}]},{"title":"头条一面凉经","slug":"Interview-experience","date":"2020-03-23T09:27:04.502Z","updated":"2020-03-23T09:29:08.870Z","comments":true,"path":"2020/03/23/Interview-experience/","link":"","permalink":"https://liangweijiang.github.io/2020/03/23/Interview-experience/","excerpt":"","text":"头条一面总结昨天头条的一面结束了,总体感觉就是有一点出乎一点的感觉,哈哈哈,和平时网上看到的面经完全不一样呀,主要还是自己的积累不够,所以做一个总结来警示自己还要多加努力啊,大厂从来都不会这么容易就能进去的。 一. 实现队列的方法首先第一道题问了队列的实现，我首先有想到了是三种实现方法。基于栈的实现方法；基于链表的实现方法；基于数组的实现方法 首先基于站的实现方法我回答出来了，就是是想用辅助栈实现 基于链表的实现也挺简单，插入直接插到最后，取的时候去头元素，然后在更新头元素 基于数组的方法，妈的一开始没想到双指针我真是服了自己了，做了这么多的双指针的题，竟然没想到，这个真的是自己疏忽了，该打。。 二. 输入一个网址到页面显示出来,发生了生么事我回答首先域名解析将域名解析成IP地址,然后经过负载均衡分配服务器,服务器从数据库拿取数据,渲染到页面最终返回。 三.负载均衡求最小平均响应时间(有点没理解面试官的问题,现在都没怎么理解)的实现这道题可能大致的意思就是已知客户端的IP,当前时间和响应时间,现在要将它分配给过去一分钟之内平均响应时间最短的服务器。 这里我的思路就是维护一个哈希表，记录服务器IP地址和响应次数，每当时间超过 四. 优先级队列的实现,以及为什么堆的时间复杂度?这里听错了，回答成了堆化的时间复杂度O（n） 五. 单链表实现快排我一开始的思路就是和数组的快拍差不多,也是找基准点,然后通过分治的方法,递归实现,不同的地方就是基准点 要遍历,因为他是链表,不支持连续访问,但是我记得我处理基准点那里有一个参数写错了,操蛋,我太紧张吧high传成了mid……. 这里看一下别人的实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980// 链表结点类class Node(): def __init__(self, item=None): self.item = item // 数据域 self.next = None // 指针域// 链表类，生成链表以及定义相关方法class LinkList(): def __init__(self): self.head = None // 生成链表，这里使用list来生成 def create(self, item): self.head = Node(item[0]) p = self.head for i in item[1:]: p.next = Node(i) p = p.next // 遍历显示 def print(self): p = self.head while p != None: print(p.item, end=' ') p = p.next print() // 根据索引取值 def getItem(self, index): p = self.head count = 0 while count != index: p = p.next count += 1 return p.item // 根据索引设值 def setItem(self, index, item): p = self.head count = -1 while count &lt; index-1: p = p.next count += 1 p.item = item // 互换 def swapItem(self, i, j): t = self.getItem(j) self.setItem(j, self.getItem(i)) self.setItem(i, t) def quicksortofloop(self, left, right): if left &lt; right: // 初始化 i = left j = i+1 start = self.getItem(i) // 大循环条件，j不能超过链表长度 while (j &lt;= right): // 如果 j 指向的值大于等于基准数字，直接跳过 while (j &lt;= right and self.getItem(j) &gt;= start): j += 1 // 否则，j 指向的值小于基准，则交换 if (j &lt;= right): i += 1 self.swapItem(i, j) self.print() j += 1 self.swapItem(left, i) self.quicksortofloop(left, i-1) self.quicksortofloop(i+1, right)if __name__ == \"__main__\": L = LinkList() L.create([4, 2, 5, 3, 7, 9, 0, 1]) L.quicksortofloop(0, 7) L.print() 原文链接：https://blog.csdn.net/a66273039/article/details/84565065后来面试官看了一下,八成凉了,直接就说面试就到这里了,问我有啥想问的不,我当时挺灰心的,就啥也没问 总结想字节跳动这种大厂,一定要平时多积累,就练习,多思考,踏踏实实地,想单纯的靠抱佛脚是真的不怎么可能能进的,在今后的日子里要多思考和练习了,下次再战,其实真的挺爽的哈哈,第一次面试大厂,挺刺激,也算是没啥遗憾了,还是自己的实力不够,加油吧。","categories":[{"name":"面经","slug":"面经","permalink":"https://liangweijiang.github.io/categories/%E9%9D%A2%E7%BB%8F/"}],"tags":[]},{"title":"数据结构学习笔记---数组","slug":"data-structure-01","date":"2020-03-23T09:13:41.432Z","updated":"2020-03-23T09:14:55.629Z","comments":true,"path":"2020/03/23/data-structure-01/","link":"","permalink":"https://liangweijiang.github.io/2020/03/23/data-structure-01/","excerpt":"","text":"学习自极客时间的《数据结构与算法之美》 作者：王争 数组一. 什么是数组数组（Array）是一种线性表数据结构。它用一组连续的内存空间，来存储一组具有相同类型的数据。 什么是线性表（Linear List）。顾名思义，线性表就是数据排成像一条线一样的结构。每个线性表上的数据最多只有前和后两个方向。其实除了数组，链表、队列、栈等也是线性表结构。而非线性表的代表则有树,图,堆等。之所以叫非线性，是因为，在非线性表中，数据之间并不是简单的前后关系。 连续的内存空间和相同类型的数据，使得数组支持随机访问，但是带来的麻烦就是删除和插入操作时变得不高效。 数组是如何实现根据下标随机访问数组元素的？ 当计算机需要随机访问数组中的某个元素时，它会首先通过下面的寻址公式，计算出该元素存储的内存地址 a[i]_address = base_address + i * data_type_size其中 data_type_size 表示数组中每个元素的大小。 数组和链表的区别:链表适合插入、删除，时间复杂度 O(1)；数组支持随机访问，根据下标随机访问的时间复杂度为 O(1)。 二. 低效的“插入”和“删除”** (1) 插入** 如果在数组的末尾插入元素，那就不需要移动数据了，这时的时间复杂度为 O(1)。但如果在数组的开头插入元素，那所有的数据都需要依次往后移动一位，所以最坏时间复杂度是 O(n)。 因为我们在每个位置插入元素的概率是一样的，所以平均情况时间复杂度为 (1+2+…n)/n=O(n)。 如果数组中存储的数据并没有任何规律，数组只是被当作一个存储数据的集合。在这种情况下，如果要将某个数据插入到第 k 个位置，为了避免大规模的数据搬移，我们还有一个简单的办法就是，直接将第 k 位的数据搬移到数组元素的最后，把新的元素直接放入第 k 个位置。快排中就是利用了这个思想。 （2）删除 和插入类似，如果删除数组末尾的数据，则最好情况时间复杂度为 O(1)；如果删除开头的数据，则最坏情况时间复杂度为 O(n)；平均情况时间复杂度也为 O(n)。 实际上，在某些特殊场景下，我们并不一定非得追求数组中数据的连续性。如果我们将多次删除操作集中在一起执行，删除的效率就可以提高很多。这就是JVM 标记清除垃圾回收算法的核心思想。 JVM标记清除算法： 大多数主流虚拟机采用可达性分析算法来判断对象是否存活，在标记阶段，会遍历所有 GC ROOTS，将所有 GC ROOTS 可达的对象标记为存活。只有当标记工作完成后，清理工作才会开始。 不足：1.效率问题。标记和清理效率都不高，但是当知道只有少量垃圾产生时会很高效。2.空间问题。会产生不连续的内存空间碎片。 三. 数组的访问越界问题12345678int main(int argc, char* argv[])&#123; int i = 0; int arr[3] = &#123;0&#125;; for(; i&lt;=3; i++)&#123; arr[i] = 0; printf(\"hello world\\n\"); &#125; return 0;&#125; 这段代码的运行结果并非是打印三行“hello word”，而是会无限打印“hello world。因为，数组大小为 3，a[0]，a[1]，a[2]，而我们的代码因为书写错误，导致 for 循环的结束条件错写为了 i&lt;=3 而非 i&lt;3，所以当 i=3 时，数组 a[3]访问越界。我们知道，在 C 语言中，只要不是访问受限的内存，所有的内存空间都是可以自由访问的。根据我们前面讲的数组寻址公式，a[3]也会被定位到某块不属于数组的内存地址上，而这个地址正好是存储变量 i 的内存地址，那么 a[3]=0 就相当于 i=0，所以就会导致代码无限循环。 对于数组访问越界造成无限循环，是编译器的问题，对于不同的编译器，在内存分配时，会按照内存地址递增或递减的方式进行分配。老师的程序，如果是内存地址递减的方式，就会造成无限循环。 四. 数组和容器容器最大的优势就是可以将很多数组操作的细节封装起来。比如前面提到的数组插入、删除数据时需要搬移其他数据等。另外，它还有一个优势，就是支持动态扩容。我们就完全不需要关心底层的扩容逻辑。所以，在平时的业务开发中，我们可以直接使用编程语言提供的容器类，但是，如果是特别底层的开发，直接使用数组可能会更合适。 五. 问题思考二维数组的内存寻址公式是怎样的呢？ 对于 m * n 的数组，a [ i ][ j ] (i &lt; m,j &lt; n)的地址为： address = base_address + ( i * n + j) * type_size 关于数组和链表的几个必知必会的代码实现 实现一个支持动态扩容的数组 实现一个大小固定的有序数组，支持动态增删改操作 实现两个有序数组合并为一个有序数组 力扣关于数组的题目11. 盛最多水的容器15. 三数之和153. 寻找旋转排序数组中的最小值 我虽然刷的题不多,哈哈,但是一般数组这个数据结构的解题算法和双指针,排序,二分查找等结合的比较多,个人总结哈!","categories":[{"name":"随笔","slug":"随笔","permalink":"https://liangweijiang.github.io/categories/%E9%9A%8F%E7%AC%94/"}],"tags":[{"name":"数组","slug":"数组","permalink":"https://liangweijiang.github.io/tags/%E6%95%B0%E7%BB%84/"}]},{"title":"《趣谈网络协议》学习笔记之--最重要的传输层(下)","slug":"NetworkProtocol-05","date":"2020-03-23T07:36:59.725Z","updated":"2020-03-27T08:44:01.260Z","comments":true,"path":"2020/03/23/NetworkProtocol-05/","link":"","permalink":"https://liangweijiang.github.io/2020/03/23/NetworkProtocol-05/","excerpt":"","text":"学习自极客时间《趣谈网络协议》 作者：刘超 4.socket套接字4.1 什么是socket?在计算机通信领域，socket 被翻译为“套接字”，它是计算机之间进行通信的一种约定或一种方式。通过 socket 这种约定，一台计算机可以接收其他计算机的数据，也可以向其他计算机发送数据。套接字（socket）是一个抽象层，应用程序可以通过它发送或接收数据，可对其进行像对文件一样的打开、读写和关闭等操作。套接字允许应用程序将I/O插入到网络中，并与网络中的其他应用程序进行通信。网络套接字是IP地址与端口的组合。 4.2 socket的通讯方式在网络层，Socket 函数需要指定到底是 IPv4 还是 IPv6，分别对应设置为 AF_INET 和 AF_INET6。另外，还要指定到底是 TCP 还是 UDP。TCP 协议是基于数据流的，所以设置为 SOCK_STREAM，而 UDP 是基于数据报的，因而设置为 SOCK_DGRAM。 4.3 基于 TCP 协议的 Socket 程序函数调用过程 TCP 的服务端要先监听一个端口，一般是先调用 bind 函数，给这个 Socket 赋予一个 IP 地址和端口。内核要通过 TCP 头里面的这个端口，来找到你这个应用程序，把包给你。有时候，一台机器会有多个网卡，也就会有多个 IP 地址，你可以选择监听所有的网卡，也可以选择监听一个网卡，这样，只有发给这个网卡的包，才会给你。 当服务端有了 IP 和端口号，就可以调用 listen 函数进行监听。在 TCP 的状态图里面，有一个 listen 状态，当调用这个函数之后，服务端就进入了这个状态，这个时候客户端就可以发起连接了。 在内核中，为每个 Socket 维护两个队列。一个是已经建立了连接的队列，这时候连接三次握手已经完毕，处于 established 状态；一个是还没有完全建立连接的队列，这个时候三次握手还没完成，处于 syn_rcvd 的状态。 接下来，服务端调用 accept 函数，拿出一个已经完成的连接进行处理。如果还没有完成，就要等着。 在服务端等待的时候，客户端可以通过 connect 函数发起连接。先在参数中指明要连接的 IP 地址和端口号，然后开始发起三次握手。内核会给客户端分配一个临时的端口。一旦握手成功，服务端的 accept 就会返回另一个 Socket（监听的 Socket 和真正用来传数据的 Socket 是两个，一个叫作监听 Socket，一个叫作已连接 Socket。） 连接建立成功之后，双方开始通过 read 和 write 函数来读写数据，就像往一个文件流里面写东西一样。 Socket 在 Linux 中就是以文件的形式存在的。除此之外，还存在文件描述符。写入和读出，也是通过文件描述符。 在内核中，Socket 是一个文件，那对应就有文件描述符。每一个进程都有一个数据结构 task_struct，里面指向一个文件描述符数组，来列出这个进程打开的所有文件的文件描述符。文件描述符是一个整数，是这个数组的下标。 这个数组中的内容是一个指针，指向内核中所有打开的文件的列表。既然是一个文件，就会有一个 inode，只不过 Socket 对应的 inode 不像真正的文件系统一样，保存在硬盘上的，而是在内存中的。在这个 inode 中，指向了 Socket 在内核中的 Socket 结构。 在这个结构里面，主要的是两个队列，一个是发送队列，一个是接收队列。在这两个队列里面保存的是一个缓存 sk_buff。这个缓存里面能够看到完整的包的结构。 4.4 基于 UDP 协议的 Socket 程序函数调用过程UDP 是没有连接的，所以不需要三次握手，也就不需要调用 listen 和 connect，但是，UDP 的的交互仍然需要 IP 和端口号，因而也需要 bind。UDP 是没有维护连接状态的，因而不需要每对连接建立一组 Socket，而是只要有一个 Socket，就能够和多个客户端通信。也正是因为没有连接状态，每次通信的时候，都调用 sendto 和 recvfrom，都可以传入 IP 地址和端口。 4.5 服务器如何连接更多的请求方式一：多进程方式 因为复制了文件描述符列表，而文件描述符都是指向整个内核统一的打开文件列表的，因而父进程刚才因为 accept 创建的已连接 Socket 也是一个文件描述符，同样也会被子进程获得。接下来，子进程就可以通过这个已连接 Socket 和客户端进行互通了，当通信完毕之后，就可以退出进程，那父进程如何知道子进程干完了项目，要退出呢？这个整数就是子进程的 ID，父进程可以通过这个 ID 查看子进程是否完成项目，是否需要退出。 方式二:多线程方式 方式三:IO多路复用(重点) select监听由于 Socket 是文件描述符，因而某个线程盯的所有的 Socket，都放在一个文件描述符集合 fd_set 中，这就是项目进度墙，然后调用 select 函数来监听文件描述符集合是否有变化。一旦有变化，就会依次查看每个文件描述符。那些发生变化的文件描述符在 fd_set 对应的位都设为 1，表示 Socket 可读或者可写，从而可以进行读写操作，然后再调用 select，接着盯着下一轮的变化。 epoll函数上面 select 函数还是有问题的，因为每次 Socket 所在的文件描述符集合中有 Socket 发生变化的时候，都需要通过轮询的方式。而epoll函数，它在内核中的实现不是通过轮询的方式，而是通过注册 callback 函数的方式，当某个文件描述符发送变化的时候，就会主动通知。 4.6 问题思考 epoll 是 Linux 上的函数，那你知道 Windows 上对应的机制是什么吗？如果想实现一个跨平台的程序，你知道应该怎么办吗？Windows 上对应的机制是IOCP。通常的办法是，线程池中的工作线程的数量与CPU内核数量相同，以此来最小化线程切换代价。一个IOCP对象，在操作系统中可关联着多个Socket和（或）文件控制端。 IOCP对象内部有一个先进先出（FIFO）队列，用于存放IOCP所关联的输入输出端的服务请求完成消息。请求输入输出服务的进程不接收IO服务完成通知，而是检查IOCP的消息队列以确定IO请求的状态。 （线程池中的）多个线程负责从IOCP消息队列中取走完成通知并执行数据处理；如果队列中没有消息，那么线程阻塞挂起在该队列。这些线程从而实现了负载均衡。 select, poll 和 epoll 的区别 select的第一个参数nfds为fdset集合中最大描述符值加1，fdset是一个位数组，其大小限制为__FD_SETSIZE（1024），位数组的每一位代表其对应的描述符是否需要被检查。第二三四参数表示需要关注读、写、错误事件的文件描述符位数组，这些参数既是输入参数也是输出参数，可能会被内核修改用于标示哪些描述符上发生了关注的事件，所以每次调用select前都需要重新初始化fdset。timeout参数为超时时间，该结构会被内核修改，其值为超时剩余的时间。 poll与select不同，通过一个pollfd数组向内核传递需要关注的事件，故没有描述符个数的限制，pollfd中的events字段和revents分别用于标示关注的事件和发生的事件，故pollfd数组只需要被初始化一次。poll的实现机制与select类似，其对应内核中的sys_poll，只不过poll向内核传递pollfd数组，然后对pollfd中的每个描述符进行poll，相比处理fdset来说，poll效率更高。poll返回后，需要对pollfd中的每个元素检查其revents值，来得指事件是否发生。 select，poll实现需要自己不断轮询所有fd集合，直到设备就绪，期间可能要睡眠和唤醒多次交替。而epoll其实也需要调用 epoll_wait不断轮询就绪链表，期间也可能多次睡眠和唤醒交替，但是它是设备就绪时，调用回调函数，把就绪fd放入就绪链表中，并唤醒在 epoll_wait中进入睡眠的进程。虽然都要睡眠和交替，但是select和poll在“醒着”的时候要遍历整个fd集合，而epoll在“醒着”的 时候只要判断一下就绪链表是否为空就行了，这节省了大量的CPU时间，这就是回调机制带来的性能提升。 select，poll每次调用都要把fd集合从用户态往内核态拷贝一次，并且要把current往设备等待队列中挂一次，而epoll只要 一次拷贝，而且把current往等待队列上挂也只挂一次（在epoll_wait的开始，注意这里的等待队列并不是设备等待队列，只是一个epoll内 部定义的等待队列），这也能节省不少的开销。 参考来自:https://www.cnblogs.com/xiaoyuanqujing/protected/articles/11715744.html","categories":[{"name":"随笔","slug":"随笔","permalink":"https://liangweijiang.github.io/categories/%E9%9A%8F%E7%AC%94/"}],"tags":[{"name":"网络协议","slug":"网络协议","permalink":"https://liangweijiang.github.io/tags/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/"}]},{"title":"《趣谈网络协议》学习笔记之--最重要的传输层(上)","slug":"NetworkProtocol-04","date":"2020-03-22T15:43:31.110Z","updated":"2020-03-27T08:42:48.260Z","comments":true,"path":"2020/03/22/NetworkProtocol-04/","link":"","permalink":"https://liangweijiang.github.io/2020/03/22/NetworkProtocol-04/","excerpt":"","text":"学习自极客时间《趣谈网络协议》 作者：刘超 1. UDP协议1.1 TCP 和 UDP的区别 TCP 是面向连接的，UDP 是面向无连接的。 所谓的建立连接，是为了在客户端和服务端维护连接，而建立一定的数据结构来维护双方交互的状态，用这样的数据结构来保证所谓的面向连接的特性。 TCP提供可靠交付,通过 TCP 连接传输的数据，无差错、不丢失、不重复、并且按序到达。UDP 继承了 IP 包的特性，不保证不丢失，不保证按顺序到达。 TCP是面向字节流的,而 UDP 继承了 IP 的特性，基于数据报的，一个一个地发，一个一个地收。 TCP 是可以有拥塞控制的。它意识到包丢弃了或者网络的环境不好了，就会根据情况调整自己的行为，看看是不是发快了，要不要发慢点。UDP 就不会，应用让我发，我就发，管它洪水滔天。 TCP 其实是一个有状态服务,精确地记着哪个包发送了没有，接收到没有，发送到哪个了，应该接收哪个了，错一点儿都不行。而 UDP 则是无状态服务。通俗地说是没脑子的，天真无邪的，发出去就发出去了。 1.2 UDP 包头是什么样的？ 无论应用程序写的使用 TCP 传数据，还是 UDP 传数据，都要监听一个端口,根据端口号，将数据交给相应的应用程序。 1.3 UDP 的三大使用场景 需要资源少，在网络情况比较好的内网，或者对于丢包不敏感的应用。DHCP 就是基于 UDP 协议的。一般的获取 IP 地址都是内网请求，而且一次获取不到 IP 又没事，过一会儿还有机会。PXE 可以在启动的时候自动安装操作系统，操作系统镜像的下载使用的 TFTP，这个也是基于 UDP 协议的。在还没有操作系统的时候，客户端拥有的资源很少，不适合维护一个复杂的状态机，而且因为是内网，一般也没啥问题。 不需要一对一沟通，建立连接，而是可以广播的应用。UDP 的不面向连接的功能，可以使得可以承载广播或者多播的协议。DHCP 就是一种广播的形式，就是基于 UDP 协议的。 需要处理速度快，时延低，可以容忍少数丢包，但是要求即便网络拥塞，也毫不退缩，一往无前的时候。 1.4 基于 UDP 的例子 网页或者 APP 的访问原来访问网页和手机 APP 都是基于 HTTP 协议的。HTTP 协议是基于 TCP 的，建立连接都需要多次交互，对于时延比较大的目前主流的移动互联网来讲，建立一次连接需要的时间会比较长，然而既然是移动中，TCP 可能还会断了重连，也是很耗时的。而且目前的 HTTP 协议，往往采取多个数据通道共享一个连接的情况，这样本来为了加快传输速度，但是 TCP 的严格顺序策略使得哪怕共享通道，前一个不来，后一个和前一个即便没关系，也要等着，时延也会加大。 流媒体的协议直播协议多使用 RTMP,而这个 RTMP 协议也是基于 TCP 的。TCP 的严格顺序传输要保证前一个收到了，下一个才能确认，如果前一个收不到，下一个就算包已经收到了，在缓存里面，也需要等着。对于直播来讲，这显然是不合适的，因为老的视频帧丢了其实也就丢了，就算再传过来用户也不在意了，他们要看新的了，如果老是没来就等着，卡顿了，新的也看不了，那就会丢失客户，所以直播，实时性比较比较重要，宁可丢包，也不要卡顿的。还有就是当网络不好的时候，TCP 协议会主动降低发送速度，这对本来当时就卡的看视频来讲是要命的，应该应用层马上重传，而不是主动让步。因而，很多直播应用，都基于 UDP 实现了自己的视频传输协议。 实时游戏游戏对实时要求较为严格的情况下，采用自定义的可靠 UDP 协议，自定义重传策略，能够把丢包产生的延迟降到最低，尽量减少网络问题对游戏性造成的影响。 IoT 物联网一方面，物联网领域终端资源少，很可能只是个内存非常小的嵌入式系统，而维护 TCP 协议代价太大；另一方面，物联网对实时性要求也很高，而 TCP 还是因为上面的那些原因导致时延大。Google 旗下的 Nest 建立 Thread Group，推出了物联网通信协议 Thread，就是基于 UDP 协议的。 移动通信领域在 4G 网络里，移动流量上网的数据面对的协议 GTP-U 是基于 UDP 的。因为移动网络协议比较复杂，而 GTP 协议本身就包含复杂的手机上线下线的通信协议。如果基于 TCP，TCP 的机制就显得非常多余。 1.5 问题思考都说 TCP 是面向连接的，在计算机看来，怎么样才算一个连接呢？ TCP/UDP建立连接的本质就是在客户端和服务端各自维护一定的数据结构（一种状态机），来记录和维护这个“连接”的状态 。并不是真的会在这两个端之间有一条类似“网络专线”这么一个东西。 在IP层，网络情况该不稳定还是不稳定，数据传输走的是什么路径上层是控制不了的，TCP能做的只能是做更多判断，更多重试，更多拥塞控制之类的东西。 2. TCP协议（上）2.1 TCP 包头格式 源端口和目的端口: 两个应用程序通讯少不了端口 *序号和确认序号: *编号是为了解决乱序问题。发出去的包应该有确认，如果没有收到就应该重新发送，直到送达。确认序号可以解决不丢包的问题。 *状态位: *SYN 是发起一个连接，ACK 是回复，RST 是重新连接，FIN 是结束连接等。TCP 是面向连接的，因而双方要维护连接的状态，这些带状态位的包的发送，会引起双方的状态变更。 *窗口大小: *TCP 要做流量控制，通信双方各声明一个窗口，标识自己当前能够的处理能力，别发送的太快,也别发的太慢。除了做流量控制以外，TCP 还会做拥塞控制。 2.2 TCP三次握手 一开始，客户端和服务端都处于 CLOSED 状态。先是服务端主动监听某个端口，处于 LISTEN 状态。然后客户端主动发起连接 SYN，之后处于 SYN-SENT 状态。 服务端收到发起的连接，返回 SYN，并且 ACK 客户端的 SYN，之后处于 SYN-RCVD 状态。 客户端收到服务端发送的 SYN 和 ACK 之后，发送 ACK 的 ACK，之后处于 ESTABLISHED 状态，因为它一发一收成功了。服务端收到 ACK 的 ACK 之后，处于 ESTABLISHED 状态，因为它也一发一收了。 为了维护这个连接，双方都要维护一个状态机，在连接建立的过程中，双方的状态变化时序图就像这样。 三次握手除了双方建立连接外，主要还是为了沟通一件事情，就是 TCP 包的序号的问题。每个连接都要有不同的序号。这个序号的起始序号是随着时间变化的，可以看成一个 32 位的计数器，每 4ms 加一，如果计算一下，如果到重复，需要 4 个多小时，那个绕路的包早就死翘翘了，因为我们都知道 IP 包头里面有个 TTL，也即生存时间。 为什么要三次，而不是两次？按说两个人打招呼，一来一回就可以了啊？为了可靠，为什么不是四次？ 假设A要与B通讯,当A发起第一个请求是,可能请求到达不了B,或者B没有响应,这时候A就会选择继续重发,终于B收到了A的消息,知道A想要连接,但是A还不知道,可能还会继续重发。 B 收到了请求包，就知道了 A 的存在，并且知道 A 要和它建立连接。如果 B 不乐意建立连接，则 A 会重试一阵后放弃，连接建立失败，没有问题；如果 B 是乐意建立连接的，则会发送应答包给 A。当然对于 B 来说，这个应答包也是一入网络深似海，不知道能不能到达 A。这个时候 B 自然不能认为连接是建立好了，因为应答包仍然会丢，会绕弯路，或者 A 已经挂了都有可能。 而且这个时候 B 还能碰到一个诡异的现象就是，A 和 B 原来建立了连接，做了简单通信后，结束了连接。还记得吗？A 建立连接的时候，请求包重复发了几次，有的请求包绕了一大圈又回来了，B 会认为这也是一个正常的的请求的话，因此建立了连接，可以想象，这个连接不会进行下去，也没有个终结的时候，纯属单相思了。因而两次握手肯定不行。 当然 A 发给 B 的应答之应答也会丢，也会绕路，甚至 B 挂了。按理来说，还应该有个应答之应答之应答，这样下去就没底了。所以四次握手是可以的，四十次都可以，关键四百次也不能保证就真的可靠了。只要双方的消息都有去有回，就基本可以了。 2.3 TCP四次挥手 断开的时候,客户端首先发起FIN请求,接着进入FIN_WAIT_1 的状态。服务器接收到客户端的FIN的请求后，就ACK客户端的请求，然后进入CLOSE_WAIT 的状态。 客户端收到服务器的ACK后，便进入FIN_WAIT_2 的状态，如果这个时候 服务器 直接跑路，则客户端将永远在这个状态。TCP 协议里面并没有对这个状态的处理，但是 Linux 有，可以调整 tcp_fin_timeout 这个参数，设置一个超时时间。 服务器也想断开连接,便向客户端发送FIN,表示要结束连接。 客户端收到服务器发来的FIN,也向服务器发送一个ACK，确认收到了服务器的请求，便从 FIN_WAIT_2 状态结束。为了确保服务器能够收到这个ACK，TCP 协议要求客户端最后等待一段时间 TIME_WAIT，这个时间要足够长，长到如果服务器没收到 ACK 的话，服务器会重发FIN，客户端收到后也会重新发一个 ACK 并且足够时间到达服务器。而且如果客户端没有等待足够的时间，端口就直接空出来了，但是服务器不知道，服务器原来发过的很多包很可能还在路上，如果客户端的端口被一个新的应用占用了，这个新的应用会收到上个连接中服务器发过来的包。等待的时间设为 2MSL，MSL 是 Maximum Segment Lifetime，报文最大生存时间，它是任何报文在网络上存在的最长时间，超过这个时间报文将被丢弃。 2.4 TCP 状态机 2.5 问题思考 TCP 的连接有这么多的状态，如何在系统中查看某个连接的状态? netstat可以在系统中查看某个连接的状态。 2.如何避免TCP的TIME_WAIT状态（高并发）?首先服务器可以设置SO_REUSEADDR套接字选项来通知内核，如果端口忙，但TCP连接位于TIME_WAIT状态时可以重用端口。在一个非常有用的场景就是，如果你的服务器程序停止后想立即重启，而新的套接字依旧希望使用同一端口，此时SO_REUSEADDR选项就可以避免TIME_WAIT状态。如何避免TCP的TIME_WAIT状态（高并发) 服务器遭到SYN攻击怎么办？如何防御SYN攻击？ TCP SYN flood洪水攻击原理和防御破解 针对SYN洪水攻击的防御措施 3. TCP 协议（下）3.1 如何实现一个靠谱的协议？为了保证顺序性，每一个包都有一个 ID。在建立连接的时候，会商定起始的 ID 是什么，然后按照 ID 一个个发送。为了保证不丢包，对于发送的包都要进行应答，但是这个应答也不是一个一个来的，而是会应答某个之前的 ID，表示都收到了，这种模式称为累计确认或者累计应答（cumulative acknowledgment）。 为了记录所有发送的包和接收的包，TCP 也需要发送端和接收端分别都有缓存来保存这些记录。发送端的缓存里是按照包的 ID 一个个排列，根据处理的情况分成四个部分。 第一部分, 发送并确认的。 第二部分, 发送了尚未确认的。 第三部分, 没有发送，但是已经等待发送的。 第四部分， 没有发送，并且暂时还不会发送的。 区分第三和第四部分是因为要流量控制，把握分寸，根据窗口的工作量先评估一下，多了就加，少了就减。 在 TCP 里，接收端会给发送端报一个窗口的大小，叫 Advertised window。这个窗口的大小应该等于上面的第二部分加上第三部分。超过这个窗口的，接收端做不过来，就不能发送了。 发送端需要保持下面的数据结构： 接收方的数据结构如下： AdvertisedWindow=MaxRcvBuffer-((NextByteExpected-1)-LastByteRead)。 其中第二部分里面，由于受到的包可能不是顺序的，会出现空挡，只有和第一部分连续的，可以马上进行回复，中间空着的部分需要等待，哪怕后面的已经来了。 3.2 顺序问题与丢包问题发送方和接收方可能会出现以下情况: 接收方接受到了包并发送了相应的ACK,但是可能在路上丢失了. 接收方接受了包但是这个包的前一个包没有接收到,出现了乱序,所以只能缓存着不能ACK 顺序问题和丢包问题都有可能发生，所以我们先来看确认与重发的机制。 一种方法就是超时重试，也即对每一个发送了，但是没有 ACK 的包，都有设一个定时器，超过了一定的时间，就重新尝试。估计往返时间，需要 TCP 通过采样 RTT 的时间，然后进行加权平均，算出一个值，而且这个值还是要不断变化的，因为网络状况不断的变化。除了采样 RTT，还要采样 RTT 的波动范围，计算出一个估计的超时时间。由于重传时间是不断变化的，我们称为自适应重传算法。 每当遇到一次超时重传的时候，都会将下一次超时时间间隔设为先前值的两倍。两次超时，就说明网络环境差，不宜频繁反复发送。有一个可以快速重传的机制，当接收方收到一个序号大于下一个所期望的报文段时，就会检测到数据流中的一个间隔，于是它就会发送冗余的 ACK，仍然 ACK 的是期望接收的报文段。而当客户端收到三个冗余的 ACK 后，就会在定时器过期之前，重传丢失的报文段 还有一种方式称为 Selective Acknowledgment （SACK）。这种方式需要在 TCP 头里加一个 SACK 的东西，可以将缓存的地图发送给发送方。例如可以发送 ACK6、SACK8、SACK9，有了地图，发送方一下子就能看出来是 7 丢了。 3.3 流量控制问题 当发送方发送包的数量过大,例如将第三部分没有发送但等待发送的包一次性发完,这时候只有当发送方收到接收方发来的ACK,才能继续发送包。 如果接收方实在处理的太慢，导致缓存中没有空间了，可以通过确认信息修改窗口的大小，甚至可以设置为 0，则发送方将暂时停止发送。如果接收端还是一直不处理数据，则随着确认的包越来越多，窗口越来越小，直到为 0。 发送方会定时发送窗口探测数据包，看是否有机会调整窗口的大小。当接收方比较慢的时候，要防止低能窗口综合征，别空出一个字节来就赶快告诉发送方，然后马上又填满了，可以当窗口太小的时候，不更新窗口，直到达到一定大小，或者缓冲区一半为空，才更新窗口。 3.4 拥塞控制问题拥塞控制的问题，也是通过窗口的大小来控制的，前面的滑动窗口 rwnd 是怕发送方把接收方缓存塞满，而拥塞窗口 cwnd，是怕把网络塞满。 原来发送一个包，从一端到达另一端，假设一共经过四个设备，每个设备处理一个包时间耗费 1s，所以到达另一端需要耗费 4s，如果发送的更加快速，则单位时间内，会有更多的包到达这些中间设备，这些设备还是只能每秒处理一个包的话，多出来的包就会被丢弃，这是我们不想看到的。 这个时候，我们可以想其他的办法，例如这个四个设备本来每秒处理一个包，但是我们在这些设备上加缓存，处理不过来的在队列里面排着，这样包就不会丢失，但是缺点是会增加时延，这个缓存的包，4s 肯定到达不了接收端了，如果时延达到一定程度，就会超时重传，也是我们不想看到的。 TCP 的拥塞控制主要来避免两种现象，包丢失和超时重传。一旦出现了这些现象就说明，发送速度太快了，要慢一点。 窗口调整的办法: 慢启动 一条 TCP 连接开始，cwnd 设置为一个报文段，一次只能发送一个；当收到这一个确认的时候，cwnd 加一，于是一次能够发送两个；当这两个的确认到来的时候，每个确认 cwnd 加一，两个确认 cwnd 加二，于是一次能够发送四个；当这四个的确认到来的时候，每个确认 cwnd 加一，四个确认 cwnd 加四，于是一次能够发送八个。可以看出这是指数性的增长。 有一个值 ssthresh 为 65535 个字节，当超过这个值的时候，就要小心一点了，不能倒这么快了，要慢下来。每收到一个确认后，cwnd 增加 1/cwnd，我们接着上面的过程来，一次发送八个，当八个确认到来的时候，每个确认增加 1/8，八个确认一共 cwnd 增加 1，于是一次能够发送九个，变成了线性增长。但是始终都会溢出。 快速重传算法。当接收端发现丢了一个中间包的时候，发送三次前一个包的 ACK，于是发送端就会快速的重传，不必等待超时再重传。TCP 认为这种情况不严重，因为大部分没丢，只丢了一小部分，这时候就将cwnd 减半为 cwnd/2，然后 sshthresh = cwnd，当三个包返回的时候，cwnd = sshthresh + 3，也就是没有一夜回到解放前，而是还在比较高的值，呈线性增长。 但是还是会出现问题; 第一个问题是丢包并不代表着通道满了，也可能是管子本来就漏水。例如公网上带宽不满也会丢包，这个时候就认为拥塞了，退缩了，其实是不对的。 第二个问题是 TCP 的拥塞控制要等到将中间设备都填充满了，才发生丢包，从而降低速度，这时候已经晚了。其实 TCP 只要填满管道就可以了，不应该接着填，直到连缓存也填满。 为了优化这两个问题，后来有了 TCP BBR 拥塞算法。它企图找到一个平衡点，就是通过不断的加快发送速度，将管道填满，但是不要填满中间设备的缓存，因为这样时延会增加，在这个平衡点可以很好的达到高带宽和低时延的平衡。 3.5 问题思考TCP 的 BBR 听起来很牛，你知道他是如何达到这个最优点的嘛？ 1 设备缓存会导致延时？假如经过设备的包都不需要进入缓存，那么得到的速度是最快的。进入缓存且等待，等待的时间就是额外的延时。BBR就是为了避免这些问题：充分利用带宽；降低buffer占用率。 2 降低发送packet的速度，为何反而提速了？标准TCP拥塞算法是遇到丢包的数据时快速下降发送速度，因为算法假设丢包都是因为过程设备缓存满了。快速下降后重新慢启动，整个过程对于带宽来说是浪费的。通过packet速度-时间的图来看，从积分上看，BBR充分利用带宽时发送效率才是最高的。可以说BBR比标准TCP拥塞算法更正确地处理了数据丢包。对于网络上有一定丢包率的公网，BBR会更加智慧一点。回顾网络发展过程，带宽的是极大地改进的，而最小延迟会受限与介质传播速度，不会明显减少。BBR可以说是应运而生。 3 BBR如何解决延时？S1：慢启动开始时，以前期的延迟时间为延迟最小值Tmin。然后监控延迟值是否达到Tmin的n倍，达到这个阀值后，判断带宽已经消耗尽且使用了一定的缓存，进入排空阶段。S2：指数降低发送速率，直至延迟不再降低。这个过程的原理同S1S3：协议进入稳定运行状态。交替探测带宽和延迟，且大多数时间下都处于带宽探测阶段。","categories":[{"name":"随笔","slug":"随笔","permalink":"https://liangweijiang.github.io/categories/%E9%9A%8F%E7%AC%94/"}],"tags":[{"name":"网络协议","slug":"网络协议","permalink":"https://liangweijiang.github.io/tags/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/"}]},{"title":"每日一道算法之--合并K个排序链表","slug":"algorithm-23","date":"2020-03-20T07:59:05.513Z","updated":"2020-03-20T08:00:24.956Z","comments":true,"path":"2020/03/20/algorithm-23/","link":"","permalink":"https://liangweijiang.github.io/2020/03/20/algorithm-23/","excerpt":"","text":"合并K个排序链表力扣第23题:https://leetcode-cn.com/problems/merge-k-sorted-lists/ 合并 k 个排序链表，返回合并后的排序链表。请分析和描述算法的复杂度。 示例: 输入:[ 1-&gt;4-&gt;5, 1-&gt;3-&gt;4, 2-&gt;6]输出: 1-&gt;1-&gt;2-&gt;3-&gt;4-&gt;4-&gt;5-&gt;6 暴力算法把N个数放到一个数组里，再一起排序，O(NlogN)。 12345678910111213141516class Solution(object): def mergeKLists(self, lists): \"\"\" :type lists: List[ListNode] :rtype: ListNode \"\"\" self.nodes = [] head = point = ListNode(0) for l in lists: while l: self.nodes.append(l.val) l = l.next for x in sorted(self.nodes): point.next = ListNode(x) point = point.next return head.next 分治算法这道题其实和归并排序基本是完全一样的,利用分治思想,将k个链表分成多个两组链表再合并,和归并排序基本一致. 123456789101112131415161718192021222324252627282930# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def mergeKLists(self, lists: List[ListNode]) -&gt; ListNode: if not lists: return n = len(lists) if n &lt; 2: return lists[n - 1] mid = n // 2 left = lists[0:mid] right= lists[mid:] return self.merge(self.mergeKLists(left), self.mergeKLists(right)) def merge(self, l, r): new_node = ListNode(0) n = new_node while l and r: if l.val &lt;= r.val: n.next, l = l, l.next else: n.next, r = r, r.next n = n.next if l: n.next = l if r: n.next = r return new_node.next 复杂度分析时间复杂度:我们可以在 O(n)的时间内合并两个有序链表，其中 n 是两个链表中的总节点数。将所有的合并进程加起来，我们可以得到时间复杂度为 O(Nlogk),其中 k 是链表的数目。 空间复杂度:没有用到额外的数据结构,可以用O(1)的空间复杂度合并两个链表,所以空间复杂度为O(1) 利用堆(优先队列)实现由于k个链表是有序的，我们实际上只需要维护k个指针从k个链表的头向尾滑动，每次选取k个链表的表头里的最小加入新的有序链表里。这就可以维护一个最小堆(优先队列)。 123456789101112131415161718class Solution: def mergeKLists(self, lists: List[ListNode]) -&gt; ListNode: import heapq que = [] # curs存K个链表滑动的头指针 for index, node in enumerate(lists): if node!=None: heapq.heappush(que ,(node.val, index)) dummy_node = ListNode(-1) cur = dummy_node while que: val, index = heapq.heappop(que) cur.next = lists[index] cur = cur.next lists[index] = lists[index].next if lists[index] is not None: heapq.heappush(que, (lists[index].val, index)) return dummy_node.next 复杂度分析时间复杂度: 因为是维护的堆,当我们弹出一个元素是,重新堆化的时间复杂度为 O(logk),其中k为链表的数目,同时找到最小元素的时间复杂度为O(1), 链表总共有N个节点,所以时间复杂度为 O(Nlogk)。 空间复杂度： O(n) 。创造一个新的链表需要 O(n) 的开销。 O(k) 。以上代码采用了重复利用原有节点，所以只要 O(1) 的空间。同时优先队列（通常用堆实现）需要 O(k) 的空间（远比大多数情况的 N 要小）。","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://liangweijiang.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"分治","slug":"分治","permalink":"https://liangweijiang.github.io/tags/%E5%88%86%E6%B2%BB/"},{"name":"优先队列","slug":"优先队列","permalink":"https://liangweijiang.github.io/tags/%E4%BC%98%E5%85%88%E9%98%9F%E5%88%97/"},{"name":"堆","slug":"堆","permalink":"https://liangweijiang.github.io/tags/%E5%A0%86/"}]},{"title":"每日一道算法之--最长上升子序列","slug":"algorithm-300","date":"2020-03-17T16:54:50.537Z","updated":"2020-04-08T10:08:37.092Z","comments":true,"path":"2020/03/18/algorithm-300/","link":"","permalink":"https://liangweijiang.github.io/2020/03/18/algorithm-300/","excerpt":"","text":"最长上升子序列力扣第300题:https://leetcode-cn.com/problems/longest-increasing-subsequence/ 给定一个无序的整数数组，找到其中最长上升子序列的长度。 示例: 输入: [10,9,2,5,3,7,101,18]输出: 4解释: 最长的上升子序列是 [2,3,7,101]，它的长度是 4。说明: 可能会有多种最长上升子序列的组合，你只需要输出对应的长度即可。你算法的时间复杂度应该为 O(n2) 。进阶: 你能将算法的时间复杂度降低到 O(n log n) 吗? 1. 动态规划一看到最字,首先想到的是动态规划和贪心算法。这道题动态规划还是挺好想的，循环两次遍历，这样子就能找到所有的情况，状态转移方程为： if num[j] &gt; num[i]: dp[i] = max(dp[i], dp[j] + 1)代码如下: 123456789class Solution: def lengthOfLIS(self, nums: List[int]) -&gt; int: if not nums: return 0 dp = [1 for _ in range(len(nums))] for i in range(len(nums)): for j in range(i): if nums[i] &gt; nums[j]: dp[i] = max(dp[i], dp[j] + 1) return max(dp) 1.1 复杂度分析时间复杂度:O(n^2)，其中 n 为数组nums 的长度。动态规划的状态数为 n，计算状态 dp[i] 时，需要 O(n) 的时间遍历 dp[0…i−1] 的所有状态，所以总时间复杂度为 O(n^2)。 空间复杂度:需要额外使用长度为 n 的 dp 数组,所以空间复杂度为O(n)。 2. 贪心 + 二分查找我一开始想到了用分治去处理这道题,但是因为分治太难去统计长度了,所以觉得不太行,就去看了一下官方题解。 代码如下： 12345678910111213141516171819class Solution: def lengthOfLIS(self, nums: List[int]) -&gt; int: if not nums: return 0 res = [] for num in nums: if not res or num &gt; res[-1]: res.append(num) else: l, r = 0, len(res) - 1 index = r while l &lt;= r: mid = l + int((r -l) &gt;&gt; 1) if res[mid] &gt;= num: index = mid r = mid - 1 else: l = mid + 1 res[index] = num return len(res) 2.1 复杂度分析时间复杂度：数组nums 的长度为 n，我们依次用数组中的元素去更新 d 数组，而更新 d 数组时需要进行 O(logn) 的二分搜索，所以总时间复杂度为 O(nlogn)。 空间复杂度:需要额外使用长度为 n 的 dp 数组,所以空间复杂度为O(n)。 参考文章官方题解:https://leetcode-cn.com/problems/longest-increasing-subsequence/solution/zui-chang-shang-sheng-zi-xu-lie-by-leetcode-soluti/ 精选题解:https://leetcode-cn.com/problems/longest-increasing-subsequence/solution/zui-chang-shang-sheng-zi-xu-lie-dong-tai-gui-hua-2/ 相似题目673. 最长递增子序列的个数 646. 最长数对链 334. 递增的三元子序列","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://liangweijiang.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"动态规划","slug":"动态规划","permalink":"https://liangweijiang.github.io/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"},{"name":"二分查找","slug":"二分查找","permalink":"https://liangweijiang.github.io/tags/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/"},{"name":"贪心","slug":"贪心","permalink":"https://liangweijiang.github.io/tags/%E8%B4%AA%E5%BF%83/"}]},{"title":"写给自己的一些话","slug":"dream","date":"2020-03-15T18:36:58.976Z","updated":"2020-03-15T18:41:53.689Z","comments":true,"path":"2020/03/16/dream/","link":"","permalink":"https://liangweijiang.github.io/2020/03/16/dream/","excerpt":"","text":"写给自己的一些话昨晚收到了人生的第一份面试邀请，头条的邀请，心情真的很激动，但是更多的也是紧张。倒也不是说自己的技术不行吧，而是，在几个月前，对于腾讯头条阿里这种像是珠穆朗玛峰那种我想都不敢想的大厂，我竟然能收到面试。 高中， 那个自傲的自己高中生活三年，当时在学校成绩一直还算可以，起初是蹦着中科大和复旦去的，最差也得有个华工吧哈哈，当时成绩确实还可以，但是有点飘了哈。当我高考成绩出来后，我懵了，真的当时就整个人飘忽忽的，看着这个成绩单，我知道自己再也不是那个当初自傲的自己了。大伯劝我复读，说什么算过我的命今天考试不行，明年清华都能上（当时高考前大伯就叮嘱我要我复读了），但是我不信命，我这辈子都没想过我要复读。我不后悔，换句话说，后悔也没用。 大一，那个懵懂的自己于是来到了广工，大一这一年，真的是对专业一窍不通，开始的时候连C的课设都难办，下学期的时候，有几位同学进了工作室，而我真的一点基础都没有，笔试一面混一混，到了二面，人就傻了，操作系统是啥？正则匹配是什么？完全不懂啊。哎，大一真的是跟个无头苍蝇一样，根本不知道自己的目标是什么，甚至连自己的专业都不知道是什么，更不用说去看什么书去充实自己。 大二，那个颓废的自己大二，逐渐变成了老司机，开始疯狂逃课了，一周五天睡四天，那个时候的心情我现在都还记得，哎，工作室进不去了，还有啥用，看着在工作室的同学天天有着自己的事做，心里其实很难受，但是又不知道从何下手，只能通过打游戏来麻痹自己，成绩一落千丈，从全级第六掉到了全班倒数，那时候就处在每天的颓废和罪恶感中，很是难受。知道下学期，因为性格很开朗，认识了很多朋友，也包括工作室的伙伴，我开始向他们询问学习方法和学习书籍，同时接触到了Python这一门语言，渐渐地爱上了这一门语言，虽然对比于c++和java来说，它有许多不足，但是它有着自己的魅力去吸引我。大二下学期快放假的时候，我逐渐开始学习起来，虽然效率不高，因为看的是视频，学的时间不多，但是那时候我开始，真的想学习了。 大三，开始去改变自己大三我知道自己不能再这样下去了，于是我决定彻彻底底的改变自己。起步的时候很辛苦，这种辛苦是要入门计算机，看视频效率真的太慢了，但是看书看不下去，我就讲视频1.5倍速，看了15天，当时记得看到了socket编程，刚好碰到了计网的课设，我就自己写了一人ftp文件传输系统，参加了答辩，那是我人生中第一次答辩，我第一个感受到了作为一个技术人员给我带来的成就感，逐渐地，我爱上了编程。后来碰上了老师叫上一届的师兄回来分享经验，谈到了春招，当时距离春招就三个多月了，我又开始了恐慌，真的每天都在愁。然后我得到了我一个朋友的帮助，这里我真的非常感谢我这个朋友，是他给了我很大的信心和建议，让我去复习底层的知识。当时刚好碰上了考试，我又浪费了半个月的时间。考完试后，我开始了痛苦但是又快乐的基础知识学习，真的太爽了学习基础。计算机网络，操所系统，数据结构与算法，我开始尝试着去看书，去看极客专栏，一步一步脚踏实地地去学习，这段时间的收获真的太多了，不仅仅是知识，更多的是心态上的成熟。 梦想是我学习的动力从小到大我的梦想都没有改变过，虽然我颓废过，堕落过，但是他真的是我生活上一个很大的支撑点。最近我在一个视频上看到这样一段话： 我当时真的眼泪直接绷不住了，这真的是我一直以来藏在心里的话。对于一个的大学生来说，除了梦想，他还能有什么呢，但这就是我一直没有放弃的原因，因为我知道，这是驱动我努力前行的动力。 感谢我身边的所有人在我大学期间，真的受到了无数人的帮助，真的感谢每一位爱我的人和我爱的人，人生真的就是需要这样一群人，才能构建你一生的轨迹。 感谢我这个编程很牛逼的好朋友，在自己也要复习，完全没有必要帮助我的情况下，给我推荐技术线，学习资料，一直鼓励我叫我不要放弃，一直帮我修改简历，真的这段时间一直陪着我。 感谢我大学的好哥们，一直安慰我，鼓励我，给我信心，帮我找岗位，一直默默地支持我 感谢我的叔叔，给我分享自己的人生经验，无时无刻都在支持我 感谢这位阿里的师兄，本来互不相识，却给我很多宝贵的学习经验和方法，同时也一直鼓励我 感谢我的女朋友，在我这段时间里，学习没怎么陪她，但是她很体谅我，还天天过来安慰鼓励我 我要感谢的人真的太多太多了，父母亲人就更不用多说了，他们总是默默地为我付出着，我一定会带着他们的祝福，坚定地走下去 学习目标最后附上自己的学习目标：坚持学习算法，知道完全掌握；对网络协议达到精通的程度；熟悉自己领域的技术栈，做到踏踏实实的学习。希望4年之后，自己能达到自己理想的水准。最后附上自己的日学习计划，一定要长期坚持下来 面试来了，尽全力对待，不留遗憾，给爷冲！！！！","categories":[{"name":"随笔","slug":"随笔","permalink":"https://liangweijiang.github.io/categories/%E9%9A%8F%E7%AC%94/"}],"tags":[]},{"title":"《趣谈网络协议》学习笔记之--从二层到三层（下）","slug":"NetworkProtocol-03","date":"2020-03-15T16:50:10.771Z","updated":"2020-03-27T08:41:16.388Z","comments":true,"path":"2020/03/16/NetworkProtocol-03/","link":"","permalink":"https://liangweijiang.github.io/2020/03/16/NetworkProtocol-03/","excerpt":"","text":"学习自极客时间《趣谈网络协议》 作者：刘超 4. 网关的实现4.1 怎么样在宿舍上网有两种方式: 有一台电脑需要两张网卡,一张网卡的线插到交换机上，另一张网卡的线插到校园网的网口。而且，这张新的网卡的 IP 地址要按照学校网管部门分配的配置，不然上不了网。这种情况下，如果你们宿舍的人要上网，就需要一直开着那台电脑。 买个家庭路由器,家庭路由器会有内网网口和外网网口。把外网网口的线插到校园网的网口上，将这个外网网口配置成和网管部的一样。内网网口连上所有的电脑。这种情况下，如果你有人要上网，就需要一直开着路由器。 其实家庭路由器和电脑的功能一样,只不过是嵌入式系统而已。 其他电脑要上网还需要配置网卡。当然 DHCP 是可以默认配置的。在进行网卡配置的时候，除了 IP 地址，还需要配置一个Gateway 的东西，这个就是网关。 4.2 MAC 头和 IP 头的细节 在 MAC 头里面，先是目标 MAC 地址，然后是源 MAC 地址，然后有一个协议类型，用来说明里面是 IP 协议。 IP 头里面的版本号，目前主流的还是 IPv4，数据包是按照服务类型（Type of Service，TOS）被分配到三个波段（band）里面的。TOS 是 IP 头里面的一个字段，代表了当前的包是高优先级的，还是低优先级的。TTL 指生存时间,与ICMP有关。 另外，还有 8 位标识协议。这里到了下一层的协议，也就是，是 TCP 还是 UDP。最重要的就是源 IP 和目标 IP。先是源 IP 地址，然后是目标 IP 地址。在任何一台机器上，当要访问另一个 IP 地址的时候，都会先判断，这个目标 IP 地址，和当前机器的 IP 地址，是否在同一个网段。怎么判断同一个网段呢？需要 CIDR 和子网掩码。 如果是同一个网段，则直接通过ARP获取Mac地址，如果不同网段，就需要经过网关了。获取网关的Mac地址也是在局域网中吼，流程是一样的。 网关往往是一个路由器，是一个三层转发的设备。啥叫三层设备？就是把 MAC 头和 IP 头都取下来，然后根据里面的内容，看看接下来把包往哪里转发的设备。路由器是一台设备，它有多个网口或者网卡，分别连着多个局域网。每个网卡的 IP 地址都和局域网的 IP 地址相同的网段，就是那个局域网的网关。 4.3 静态路由路由器可以分为静态路由和动态路由。静态路由，其实就是在路由器上，配置一条一条规则。这些规则包括：想访问 BBS 站（它肯定有个网段），从 2 号口出去，下一跳是 IP2；想访问教学视频站（它也有个自己的网段），从 3 号口出去，下一跳是 IP3，然后保存在路由器里。每当要选择从哪只手抛出去的时候，就一条一条的匹配规则，找到符合的规则，就按规则中设置的那样，从某个口抛出去，找下一跳 IPX。 4.4 IP 头和 MAC 头哪些变、哪些不变？MAC 地址是一个局域网内才有效的地址。因而，MAC 地址只要过网关，就必定会改变，因为已经换了局域网。两者主要的区别在于 IP 地址是否改变。不改变 IP 地址的网关，我们称为转发网关；改变 IP 地址的网关，我们称为NAT 网关。 当各个局域网间网段有商量，即IP段没有冲突的时候，不同网段间用户的通讯，经过网关的时候，IP地址不会改变，改变的是Mac地址。 当局域网之间没有商量过，各定各的网段，因而 IP 段冲突了。不同网段间用户的通讯，经过网关的时候，需要将当前局域网内的IP地址转换成国际通用的IP地址，当到了最后一跳时，NAT网关再将国际IP地址转换成私网的IP地址，最终达到接收方。这里Mac地址也在不断变化的。从这个过程可以看出，IP 地址也会变。这个过程用英文说就是 Network Address Translation，简称 NAT。 4.5 总结 如果离开本局域网，就需要经过网关，网关是路由器的一个网口； 路由器是一个三层设备，里面有如何寻找下一跳的规则； 当在你家里要访问 163 网站的时候，你的包需要 NAT 成为公网 IP，返回的包又要 NAT 成你的私有 IP，返回包怎么知道这是你的请求呢？它怎么就这么智能的 NAT 成了你的 IP 而非别人的 IP 呢？ NAT在进行地址替换时不仅仅包含IP地址，还有端口号。具体说来就是，我们在进行连接外网服务器请求的数据包中，除了源、目的IP地址外，还有源、目的端口号。其中目的端口号是固定的，比如21或80等等。但源端口号是随机生成的。当数据包到达进行NAT的设备时，除了私有IP地址会被替换成公网IP地址外，端口号也会被替换成NAT随机生成的端口号。NAT的端口号和局域网中的主机一一对应，同时NAT设备维护一张端口号和主机对应的表。当外网服务器返回数据到NAT设备时，NAT设备通过返回数据包中的端口号找到局域网中的主机并将数据转发。 5. 路由协议一张路由表中会有多条路由规则。每一条规则至少包含这三项信息。 目的网络：这个包想去哪儿？ 出口设备：将包从哪个口扔出去？ 下一跳网关：下一个路由器的地址。 5.1 动态路由算法复杂的网络拓扑结构中,路由的路径就转化成为如何在途中找到最短路径的问题。求最短路径常用的有两种方法，一种是 Bellman-Ford 算法，一种是 Dijkstra 算法。在计算机网络中基本也是用这两种方法计算的。 距离矢量路由算法 第一大类的算法称为距离矢量路由（distance vector routing）。它是基于 Bellman-Ford 算法的。 这种算法的基本思路是，每个路由器都保存一个路由表，包含多行，每行对应网络中的一个路由器，每一行 包含两部分信息，一个是要到目标路由器，从那条线出去，另一个是到目标路由器的距离。由此可以看出，每个路由器都是知道全局信息的。那这个信息如何更新呢？每个路由器都知道自己和邻居之间的距离，每过几秒，每个路由器都将自己所知的到达所有的路由器的距离告知邻居，每个路由器也能从邻居那里得到相似的信息。每个路由器根据新收集的信息，计算和其他路由器的距离，比如自己的一个邻居距离目标路由器的距离是 M，而自己距离邻居是 x，则自己距离目标路由器是 x+M。 距离矢量路由算法存在的问题: 第一个问题就是好消息传得快，坏消息传得慢。 如果有个路由器加入了这个网络，它的邻居就能很快发现它，然后将消息广播出去。要不了多久，整个网络就都知道了。但是一旦一个路由器挂了，挂的消息是没有广播的。当每个路由器发现原来的道路到不了这个路由器的时候，感觉不到它已经挂了，而是试图通过其他的路径访问，直到试过了所有的路径，才发现这个路由器是真的挂了。 这种算法的第二个问题是，每次发送的时候，要发送整个全局路由表。 链路状态路由算法 第二大类算法是链路状态路由（link state routing），基于 Dijkstra 算法。 这种算法的基本思路是：当一个路由器启动的时候，首先是发现邻居，向邻居 say hello，邻居都回复。然后计算和邻居的距离，发送一个 echo，要求马上返回，除以二就是距离。然后将自己和邻居之间的链路状态包广播出去，发送到整个网络的每个路由器。这样每个路由器都能够收到它和邻居之间的关系的信息。因而，每个路由器都能在自己本地构建一个完整的图，然后针对这个图使用 Dijkstra 算法，找到两点之间的最短路径。 不像距离距离矢量路由协议那样，更新时发送整个路由表。链路状态路由协议只广播更新的或改变的网络拓扑，这使得更新信息更小，节省了带宽和 CPU 利用率。而且一旦一个路由器挂了，它的邻居都会广播这个消息，可以使得坏消息迅速收敛。 5.2 动态路由协议 基于链路状态路由算法的 OSPF OSPF（Open Shortest Path First，开放式最短路径优先）就是这样一个基于链路状态路由协议，广泛应用在数据中心中的协议。由于主要用在数据中心内部，用于路由决策，因而称为内部网关协议（Interior Gateway Protocol，简称 IGP）。 内部网关协议的重点就是找到最短的路径。在一个组织内部，路径最短往往最优。当然有时候 OSPF 可以发现多个最短的路径，可以在这多个路径中进行负载均衡，这常常被称为等价路由。这一点非常重要。有了等价路由，到一个地方去可以有相同的两个路线，可以分摊流量，还可以当一条路不通的时候，走另外一条路。 基于距离矢量路由算法的 BGP 但是外网的路由协议，也即国家之间的，又有所不同。我们称为外网路由协议（Border Gateway Protocol，简称 BGP）。 对于网络包同样，每个数据中心都设置自己的 Policy。例如，哪些外部的 IP 可以让内部知晓，哪些内部的 IP 可以让外部知晓，哪些可以通过，哪些不能通过。 在网络世界，这一个个国家成为自治系统 AS（Autonomous System）。自治系统分几种类型。 Stub AS：对外只有一个连接。这类 AS 不会传输其他 AS 的包。例如，个人或者小公司的网络。 Multihomed AS：可能有多个连接连到其他的 AS，但是大多拒绝帮其他的 AS 传输包。例如一些大公司的网络。 Transit AS：有多个连接连到其他的 AS，并且可以帮助其他的 AS 传输包。例如主干网。 BGP 又分为两类，eBGP 和 iBGP。自治系统间，边界路由器之间使用 eBGP 广播路由。内部网络也需要访问其他的自治系统。边界路由器如何将 BGP 学习到的路由导入到内部网络呢？就是通过运行 iBGP，使得内部的路由器能够找到到达外网目的地的最好的边界路由器。 BGP 协议使用的算法是路径矢量路由协议（path-vector protocol）。它是距离矢量路由协议的升级版。前面说了距离矢量路由协议的缺点。其中一个是收敛慢。在 BGP 里面，除了下一跳 hop 之外，还包括了自治系统 AS 的路径，从而可以避免坏消息传的慢的问题，也即上面所描述的，B 知道 C 原来能够到达 A，是因为通过自己，一旦自己都到达不了 A 了，就不用假设 C 还能到达 A 了。 路由信息协议(RIP) 路由信息协议是在一个AS系统中使用地内部路由选择协议，是个非常简单的基于距离向量路由选择的协议。 它路由器生产商之间使用的第一个开放标准，是最广泛的路由协议，在所有IP路由平台上都可以得到。当使用RIP时，一台Cisco路由器可以与其他厂商的路由器连接。 路由信息协议是内部网关协议IGP中最先得到使用的协议。RIP是一种分布式的基于距离矢量的路由选择协议，是因特网的标准协议，其最大优点就是实现简单，开销较小。 5.3 问题拓展路由器之间信息的交换使用什么协议呢？ OSPF基于IP协议，端口号为89原因：ospf自身提供主从协商机制，可以保证可靠的传输，另外全网路由器保持着同样的一个lsdb，当拓扑发生变化时，需要携带的变更信息较少，通过IP协议即可完成 RIP协议采用UDP是因为，rip每周期需全网组播路由信息，路由信息数目较大，故使用UDP协议可提高效率 BGP为边界网关协议，因携带的路由信息较多，且可能跨不同网络传送路由信息，为保证可靠性，需使用TCP协议，可兼顾容量和可靠性","categories":[{"name":"随笔","slug":"随笔","permalink":"https://liangweijiang.github.io/categories/%E9%9A%8F%E7%AC%94/"}],"tags":[{"name":"网络协议","slug":"网络协议","permalink":"https://liangweijiang.github.io/tags/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/"}]},{"title":"每日一道算法之--最长公共子序列","slug":"algorithm-1143","date":"2020-03-15T12:19:20.111Z","updated":"2020-03-15T12:20:02.798Z","comments":true,"path":"2020/03/15/algorithm-1143/","link":"","permalink":"https://liangweijiang.github.io/2020/03/15/algorithm-1143/","excerpt":"","text":"最长公共子序列力扣第1143题:https://leetcode-cn.com/problems/longest-common-subsequence/ 给定两个字符串 text1 和 text2，返回这两个字符串的最长公共子序列。 一个字符串的 子序列 是指这样一个新的字符串：它是由原字符串在不改变字符的相对顺序的情况下删除某些字符（也可以不删除任何字符）后组成的新字符串。例如，”ace” 是 “abcde” 的子序列，但 “aec” 不是 “abcde” 的子序列。两个字符串的「公共子序列」是这两个字符串所共同拥有的子序列。 若这两个字符串没有公共子序列，则返回 0。 示例 1: 输入：text1 = “abcde”, text2 = “ace”输出：3解释：最长公共子序列是 “ace”，它的长度为 3。 示例 2: 输入：text1 = “abc”, text2 = “abc”输出：3解释：最长公共子序列是 “abc”，它的长度为 3。 示例 3: 输入：text1 = “abc”, text2 = “def”输出：0解释：两个字符串没有公共子序列，返回 0。 动态规划哈哈,又看到了最字,这次要多分析题目了,看看动态规划是不是合适.看看是否符合动态规划的三大特征最优子结构、无后效性和重复子问题。 首先看这道题,每当遍历到一个位置时,当前的最长公共字符串和子字符串的最长公共子序列有关,所以符合最优子结构。 我们只关心前面子串最长公共子序列的长度，不关心这个状态是怎么一步一步推导出来的。所以符合无后性所以动态规划来做这道题应该还是可以的 下面来分析一下这道题目： 这道题有两个字符串，很明显我们可以构造一个二维数组，来存储每种情况对应的状态值 我们循环遍历这两个字符串，当两个字符串的字符是一样的，就证明找到了一个公共子序列的元素，在子问题的基础上加一；如果不相等，则看两个字符串当前谁的公共子序列最长。 123456789101112class Solution: def longestCommonSubsequence(self, text1: str, text2: str) -&gt; int: if not text1 or not text2: return 0 len1, len2 = len(text1), len(text2) dp = [[0] * (len2 + 1) for _ in range(len1 + 1)] for i in range(1, len1 + 1): for j in range(1, len2 + 1): if text1[i-1] == text2[j-1]: dp[i][j] = dp[i - 1][j - 1] + 1 else: dp[i][j] = max(dp[i - 1][j], dp[i][j - 1]) return dp[-1][-1] 优化动态规划可以看到上面的这一段代码,运用了二维数组这种比较复杂的数据结构,把所有的状态都记录了下来,但是可以发现,其实只有最后一行的数据才是有效的,因为其他数据会失效,所以我们可以用一维数组,只存储最后一行的数据。 1234567891011121314151617class Solution: def longestCommonSubsequence(self, text1: str, text2: str) -&gt; int: if not text1 or not text2: return 0 len1, len2 = len(text1), len(text2) dp = [0 for _ in range(len2 + 1)] for i in range(1, len1 + 1): tmp = 0 for j in range(1, len2 + 1): # 拿到上一次的最大子序列 prev = tmp # 获取上一个循环的在这个字符位置的最大子序列 tmp = dp[j] if text1[i-1] == text2[j-1]: dp[j] = prev + 1 else: dp[j] = max(dp[j -1], dp[j]) return dp[-1]","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://liangweijiang.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"动态规划","slug":"动态规划","permalink":"https://liangweijiang.github.io/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"}]},{"title":"每日一道算法之--岛屿的最大面积","slug":"algorithm-695","date":"2020-03-15T10:29:09.174Z","updated":"2020-03-15T10:31:36.670Z","comments":true,"path":"2020/03/15/algorithm-695/","link":"","permalink":"https://liangweijiang.github.io/2020/03/15/algorithm-695/","excerpt":"","text":"岛屿的最大面积力扣第695题:https://leetcode-cn.com/problems/max-area-of-island/ 给定一个包含了一些 0 和 1的非空二维数组 grid , 一个 岛屿 是由四个方向 (水平或垂直) 的 1 (代表土地) 构成的组合。你可以假设二维矩阵的四个边缘都被水包围着。 找到给定的二维数组中最大的岛屿面积。(如果没有岛屿，则返回面积为0。) 示例 1: [[0,0,1,0,0,0,0,1,0,0,0,0,0], [0,0,0,0,0,0,0,1,1,1,0,0,0], [0,1,1,0,1,0,0,0,0,0,0,0,0], [0,1,0,0,1,1,0,0,1,0,1,0,0], [0,1,0,0,1,1,0,0,1,1,1,0,0], [0,0,0,0,0,0,0,0,0,0,1,0,0], [0,0,0,0,0,0,0,1,1,1,0,0,0], [0,0,0,0,0,0,0,1,1,0,0,0,0]]对于上面这个给定矩阵应返回 6。注意答案不应该是11，因为岛屿只能包含水平或垂直的四个方向的‘1’。 示例 2: [[0,0,0,0,0,0,0,0]]对于上面这个给定的矩阵, 返回 0。 注意: 给定的矩阵grid 的长度和宽度都不超过 50。 1. 深度优先搜索(DFS)DFS重在追求”专一”吧!一条道走到黑,我们想知道网格中每个连通形状的面积，然后取最大值。 二维数组中每一个位置都有可能是刀鱼最大面积的起点,所以每一个位置都要去作为起点去搜索 以 4 个方向探索与之相连的每一个土地（以及与这些土地相连的土地），那么探索过的土地总数将是该连通形状的面积。递归到最深一层,然后判断是否符合要求,然后再层层往上回退。为了确保每个土地访问不超过一次，我们每次经过一块土地时，将这块土地的值置为 0。这样我们就不会多次访问同一土地。 12345678910111213141516171819202122class Solution: def maxAreaOfIsland(self, grid: List[List[int]]) -&gt; int: if not grid: return 0 rows = len(grid) cols = len(grid[0]) max_area = 0 for row in range(rows): for col in range(cols): max_area = max(self.dfs(row, col, rows, cols, grid), max_area) return max_area def dfs(self, row, col, rows, cols, grid): res = 0 # 只有为1的时候才算岛屿 if 0 &lt;= row &lt; rows and 0 &lt;= col &lt; cols and grid[row][col] != 0 : # 搜索过的就置为0，防止重复搜索 grid[row][col] = 0 res = 1 for di, dj in [[0, 1], [0, -1], [1, 0], [-1, 0]]: new_row, new_col = row + di, col + dj res += self.dfs(new_row, new_col, rows, cols, grid) return res DFS也可以用栈来实现 1234567891011121314151617181920class Solution: def maxAreaOfIsland(self, grid: List[List[int]]) -&gt; int: if not grid: return 0 rows = len(grid) cols = len(grid[0]) max_area = 0 for row in range(rows): for col in range(cols): stack = [(row, col)] res = 0 while stack: row, col = stack.pop() if 0 &lt;= row &lt; rows and 0 &lt;= col &lt; cols and grid[row][col] != 0 : grid[row][col] = 0 res += 1 for di, dj in [[0, 1], [0, -1], [1, 0], [-1, 0]]: new_row, new_col = row + di, col + dj stack.append((new_row, new_col)) max_area = max(max_area, res) return max_area 1.1 复杂度分析时间复杂度： 我们访问每个网格最多一次。所以时间复杂度为$$O(R * C)$$。其中 R 是给定网格中的行数，C 是列数。 空间复杂度：递归的深度最大可能是整个网格的大小，因此最大可能使用 $$O(R * C) $$的栈空间,所以空间复杂度为$$O(R * C)$$ 2. 广度优先搜索(BFS)顾名思义，广度优先搜索追求的是”覆盖面积”。其实只要将深度优先搜索中的栈换成队列，就可以实现广度优先搜索。 1234567891011121314151617181920class Solution: def maxAreaOfIsland(self, grid: List[List[int]]) -&gt; int: if not grid: return 0 rows = len(grid) cols = len(grid[0]) max_area = 0 for i in range(rows): for j in range(cols): queue = collections.deque([(i, j)]) res = 0 while queue: row, col = queue.popleft() if 0 &lt;= row &lt; rows and 0 &lt;= col &lt; cols and grid[row][col] != 0 : grid[row][col] = 0 res += 1 for di, dj in [[0, 1], [0, -1], [1, 0], [-1, 0]]: new_row, new_col = row + di, col + dj queue.append((new_row, new_col)) max_area = max(max_area, res) return max_area 2.1 复杂度分析时间复杂度： 我们访问每个网格最多一次。所以时间复杂度为$$O(R * C)$$。其中 R 是给定网格中的行数，C 是列数。 空间复杂度：递归的深度最大可能是整个网格的大小，因此最大可能使用 $$O(R * C) $$的栈空间,所以空间复杂度为$$O(R * C)$$ ##总结深度优先搜索和栈有关,广度优先搜索和队列有关. ##python特别注意的一点如果仔细看代码,你会发现我在用栈实现的时候和用队列实现的时候,有一句代码不一样: 123456789用栈实现的;for row in range(rows): for col in range(cols): stack = [(row, col)]用队列实现的:for i in range(rows): for j in range(cols): queue = collections.deque([(i, j)]) 这里我调试了很久,用队列实现的时候,row, col = queue.popleft()这两个变量如果和上面的一样,提交的结果就不正确,但是用栈就可以,我猜想应该是队列底层实现的缘故,具体原因还不知道. 相似题目200. 岛屿数量 面试题13. 机器人的运动范围 面试题12. 矩阵中的路径","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://liangweijiang.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"BSF","slug":"BSF","permalink":"https://liangweijiang.github.io/tags/BSF/"},{"name":"DFS","slug":"DFS","permalink":"https://liangweijiang.github.io/tags/DFS/"},{"name":"回溯","slug":"回溯","permalink":"https://liangweijiang.github.io/tags/%E5%9B%9E%E6%BA%AF/"}]},{"title":"《趣谈网络协议》学习笔记之--从二层到三层（上）","slug":"NetworkProtocol-02","date":"2020-03-15T03:47:29.800Z","updated":"2020-03-27T08:40:15.102Z","comments":true,"path":"2020/03/15/NetworkProtocol-02/","link":"","permalink":"https://liangweijiang.github.io/2020/03/15/NetworkProtocol-02/","excerpt":"","text":"学习自极客时间《趣谈网络协议》 作者：刘超 1. 从物理层到MAC层1.1 物理层主要定义物理设备标准，如网线的接口类型、光纤的接口类型、各种传输介质的传输速率等。它的主要作用是传输比特流（就是由1、0转化为电流强弱来进行传输,到达目的地后在转化为1、0，也就是我们常说的数模转换与模数转换），这一层的数据叫做比特，单位是bit比特。集线器(hub):和交换机不同，集线器没有大脑，它完全在物理层工作。它会将自己收到的每一个字节，都复制到其他端口上去。这是第一层物理层联通的方案。 1.2 数据链路层1.2.1 解决三个问题Hub 采取的是广播的模式，如果每一台电脑发出的包，局域网的每个电脑都能收到，这就需要解决几个问题： 这个包是发给谁的？谁应该接收？ 大家都在发，会不会产生混乱？有没有谁先发、谁后发的规则？ 如果发送的时候出现了错误，怎么办？ 这几个问题，都是第二层，数据链路层，也即 MAC 层要解决的问题。MAC 的全称是 Medium Access Control，即媒体访问控制。控制什么呢？其实就是控制在往媒体上发数据的时候，谁先发、谁后发的问题。防止发生混乱。这解决的是第二个问题。这个问题中的规则，学名叫多路访问。解决的方式有信道划分; 轮流协议；随机接入协议(著名的以太网，用的就是这个方式)。 接下来要解决第一个问题：发给谁，谁接收？这里用到一个物理地址，叫作链路层地址。但是因为第二层主要解决媒体接入控制的问题，所以它常被称为MAC 地址。解决第一个问题就牵扯到第二层的网络包格式。对于以太网，第二层的最开始，就是目标的 MAC 地址和源的 MAC 地址。 类型中多部分是IP数据包,ip又包含了运输层和应用层的信息,层层封装。 有了目标mac地址，数据包就可以在链路上广播，当有网卡发现mac地址正好是它的，就把这个数据包收起来，然后在打开IP,发现IP地址也是自己的，再打开TCP，端口也是自己的，就交给应用层去处理。应用层处理好后，又层层封装返回，这时候之前的源mac地址就变成了目标mac地址。 对于以太网，第二层的最后面是 CRC，也就是循环冗余检测。通过 XOR 异或的算法，来计算整个包是否在发送的过程中出现了错误，主要解决第三个问题。 1.2.2 如何获取目标mac地址?一个广播的网络里面接入了 N 台机器，我怎么知道每个 MAC 地址是谁呢？这就是 ARP 协议，也就是已知 IP 地址，求 MAC 地址的协议。 1.2.3 ARP协议的报文格式如下: 1.2.4 ARP协议的流程大致如下:当 A 想要与B通信时,A是知道B的IP地址的,但是不知道mac地址 A主机先是在自己的本地ARP缓存中检查主机B的匹配MAC地址。 如果主机A在ARP缓存中没有找到映射，它将询问自己所属的局域网的硬件地址，从而将ARP请求帧广播到本地网络上的所有主机。源主机A的IP地址和MAC地址都包括在ARP请求中。本地网络上的每台主机都接收到ARP请求并且检查是否与自己的IP地址匹配。如果主机发现请求的IP地址与自己的IP地址不匹配，它将丢弃ARP请求。 这里ARP协议报文头部的第一个的目标mac 为 ff ff ff ff ff ff ,这是一个广播地址,相当于在局域网里吼;ARP报文内容里的目标mac留空,就是为了找到想要的mac地址。 主机B确定ARP请求中的IP地址与自己的IP地址匹配，则将主机A的IP地址和MAC地址映射添加到本地ARP缓存 主机B将包含其MAC地址的ARP回复消息直接发送回主机A。 当主机A收到从主机B发来的ARP回复消息时，会用主机B的IP和MAC地址映射更新ARP缓存。本机缓存是有生存期的，生存期结束后，将再次重复上面的过程。主机B的MAC地址一旦确定，主机A就能向主机B发送IP通信了。 1.3 总结第一，MAC 层是用来解决多路访问的堵车问题的；第二，ARP 是通过吼的方式来寻找目标 MAC 地址的，吼完之后记住一段时间，这个叫作缓存；第三，交换机是有 MAC 地址学习能力的，学完了它就知道谁在哪儿了，不用广播了。 2. 交换机与VLAN2.1 网络拓补结构的形成 两台交换机将网络划分成了三个局域网,一开始大家都不知道对方的任何信息,然后靠广播将信息群发到所以的机器中,然后找到对应的MAC地址,交换机有学习功能,他就知道了该机器所在的局域网,下次广播就不会发送到其他的局域网,就这样慢慢形成了拓补结构。 2.2 拓补结构中的环路问题 机器 1 的广播包到达交换机 A 和交换机 B 的时候，本来两个交换机都学会了机器 1 是在局域网一的，但是当交换机 A 将包广播到局域网二之后，交换机 B 右边的网口收到了来自交换机 A 的广播包。根据学习机制，这彻底损坏了交换机 B 的三观，刚才机器 1 还在左边的网口呢，怎么又出现在右边的网口呢？哦，那肯定是机器 1 换位置了，于是就误会了，交换机 B 就学会了，机器 1 是从右边这个网口来的，把刚才学习的那一条清理掉。同理，交换机 A 右边的网口，也能收到交换机 B 转发过来的广播包，同样也误会了，于是也学会了，机器 1 从右边的网口来，不是从左边的网口来。然而当广播包从左边的局域网一广播的时候，两个交换机再次刷新三观，原来机器 1 是在左边的，过一会儿，又发现不对，是在右边的，过一会，又发现不对，是在左边的。 STP（最小生成树）协议 将图的环破坏变成树，解决环路问题。 2.3 如何解决广播问题和安全问题？如果机器多了，交换机也多了，就算交换机比 Hub 智能一些，但是还是难免有广播的问题，一大波机器，相关的部门、不相关的部门，广播一大堆，性能就下来了。而且每一个部门所接触的信息不一样，有的部门信息可能比较敏感，不希望别的部门接收到这个包。 两个隔离的方法： 一个是物理隔离。每个部门设一个单独的会议室，对应到网络方面，就是每个部门有单独的交换机，配置单独的子网，这样部门之间的沟通就需要路由器了。这样的问题在于，有的部门人多，有的部门人少。人少的部门慢慢人会变多，人多的部门也可能人越变越少。如果每个部门有单独的交换机，口多了浪费，少了又不够用。 另外一种方式是虚拟隔离，就是用我们常说的 VLAN，或者叫虚拟局域网。使用 VLAN，一个交换机上会连属于多个局域网的机器，那交换机怎么区分哪个机器属于哪个局域网呢？ 我们只需要在原来的二层的头上加一个 TAG，里面有一个 VLAN ID，一共 12 位。12 位可以划分 4096 个 VLAN。 如果我们买的交换机是支持 VLAN 的，当这个交换机把二层的头取下来的时候，就能够识别这个 VLAN ID。这样只有相同 VLAN 的包，才会互相转发，不同 VLAN 的包，是看不到的。这样广播问题和安全问题就都能够解决了。 将两个交换机连接起来的口应该设置成什么 VLAN 呢？对于支持 VLAN 的交换机，有一种口叫作 Trunk 口。它可以转发属于任何 VLAN 的口。交换机之间可以通过这种口相互连接。 2.4 总结 当交换机的数目越来越多的时候，会遭遇环路问题，让网络包迷路，这就需要使用 STP 协议，将有环路的图变成没有环路的树，从而解决环路问题。但是STP 对于跨地域甚至跨国组织的网络支持，就很难做了，计算量太大。 交换机数目多会面临隔离问题，可以通过 VLAN 形成虚拟局域网，从而解决广播问题和安全问题。 3. ICMP与ping3.1 ICMP 协议的格式ping 是基于 ICMP 协议工作的。ICMP 全称 Internet Control Message Protocol，就是互联网控制报文协议。这里面的关键词是“控制”，那具体是怎么控制的呢？ 网络包在异常复杂的网络环境中传输时，常常会遇到各种各样的问题。当遇到问题的时候，总不能“不明不白”，要传出消息来，报告情况，这样才可以调整传输策略。ICMP就是相当于这个侦察兵。 ICMP 报文是封装在 IP 包里面的。因为传输指令的时候，肯定需要源地址和目标地址。它本身非常简单。因为作为侦查兵，要轻装上阵，不能携带大量的包袱。 ICMP 报文有很多的类型，不同的类型有不同的代码。最常用的类型是主动请求为 8，主动请求的应答为 0。 1. 查询报文类型 这种主动发起的，主动查看敌情，对应 ICMP 的查询报文类型。例如，常用的 ping 就是查询报文，是一种主动请求，并且获得主动应答的 ICMP 协议。所以，ping 发的包也是符合 ICMP 协议格式的，只不过它在后面增加了自己的格式。对 ping 的主动请求，进行网络抓包，称为 ICMP ECHO REQUEST。同理主动请求的回复，称为ICMP ECHO REPLY。比起原生的 ICMP，这里面多了两个字段，一个是标识符。这要用标识侦查的对象是什么，另一个是序号，用来确认回收包的情况，如果派出去 10 个，回来 10 个，就说明前方战况不错；如果派出去 10 个，回来 2 个，说明情况可能不妙。在选项数据中，ping 还会存放发送请求的时间值，来计算往返时间，说明路程的长短。 2. 差错报文类型这种是异常情况发起的，来报告发生了不好的事情，对应 ICMP 的差错报文类型。几个 ICMP 差错报文的例子：终点不可达为 3，源抑制为 4，超时为 11，重定向为 5。 终点不可达: 具体的原因在代码中表示就是，网络不可达代码为 0，主机不可达代码为 1，协议不可达代码为 2，端口不可达代码为 3，需要进行分片但设置了不分片位代码为 4。 第二种是源站抑制，也就是让源站放慢发送速度。 第三种是时间超时，也就是超过网络包的生存时间还是没到。 第四种是路由重定向，也就是让下次发给另一个路由器。 差错报文的结构相对复杂一些。除了前面还是 IP，ICMP 的前 8 字节不变，后面则跟上出错的那个 IP 包的 IP 头和 IP 正文的前 8 个字节。 3.2 ping：查询报文类型的使用 ping 命令执行的时候，源主机首先会构建一个 ICMP 请求数据包，ICMP 数据包内包含多个字段。最重要的是两个，第一个是类型字段，对于请求数据包而言该字段为 8；另外一个是顺序号，主要用于区分连续 ping 的时候发出的多个数据包。每发出一个请求数据包，顺序号会自动加 1。为了能够计算往返时间 RTT，它会在报文的数据部分插入发送时间。 由 ICMP 协议将这个数据包连同对方的IP地址 一起交给 IP 层。IP 层将以对方的IP地址作为目的地址，本机 IP 地址作为源地址，加上一些其他控制信息，构建一个 IP 数据包。 接下来，需要加入 MAC 头。如果在本节 ARP 映射表中查找出 对方的IP 地址 所对应的 MAC 地址，则可以直接使用；如果没有，则需要发送 ARP 协议查询 MAC 地址，获得 MAC 地址后，由数据链路层构建一个数据帧，目的地址是 IP 层传过来的 MAC 地址，源地址则是本机的 MAC 地址；还要附加上一些控制信息，依据以太网的介质访问规则，将它们传送出去。 对方主机收到这个数据帧后，先检查它的目的 MAC 地址，并和本机的 MAC 地址对比，如符合，则接收，否则就丢弃。接收后检查该数据帧，将 IP 数据包从帧中提取出来，交给本机的 IP 层。同样，IP 层检查后，将有用的信息提取后交给 ICMP 协议。 对方主机会构建一个 ICMP 应答包，应答数据包的类型字段为 0，顺序号为接收到的请求数据包中的顺序号，然后再发送出去给源主机。 在规定的时候间内，源主机如果没有接到 ICMP 的应答包，则说明目标主机不可达；如果接收到了 ICMP 应答包，则说明目标主机可达。此时，源主机会检查，用当前时刻减去该数据包最初从源主机上发出的时刻，就是 ICMP 数据包的时间延迟。 如果跨网段的话，还会涉及网关的转发、路由器的转发等等。但是对于 ICMP 的头来讲，是没什么影响的。会影响的是根据目标 IP 地址，选择路由的下一跳，还有每经过一个路由器到达一个新的局域网，需要换 MAC 头里面的 MAC 地址。 经常会遇到一个问题，如果不在我们的控制范围内，很多中间设备都是禁止 ping 的，但是 ping 不通不代表网络不通。这个时候就要使用 telnet，通过其他协议来测试网络是否通。 3.3 Traceroute：差错报文类型的使用Traceroute，会使用 ICMP 的规则，故意制造一些能够产生错误的场景。 1. Traceroute 的第一个作用就是故意设置特殊的 TTL，来追踪去往目的地时沿途经过的路由器。Traceroute 的参数指向某个目的 IP 地址，它会发送一个 UDP 的数据包。将 TTL 设置成 1，也就是说一旦遇到一个路由器或者一个关卡。如果他“牺牲”了，就返回一个 ICMP 包，也就是网络差错包，类型是时间超时。这时候就知道达到目标IP的路径不值隔了一个路由，然后把TTL 设置成 2，如此反复，直到到达目的主机。这样，Traceroute 就拿到了所有的路由器 IP。当然，有的路由器压根不会回这个 ICMP。这也是 Traceroute 一个公网的地址，看不到中间路由的原因。 怎么知道 UDP 有没有到达目的主机呢？Traceroute 程序会发送一份 UDP 数据报给目的主机，但它会选择一个不可能的值作为 UDP 端口号（大于 30000）。当该数据报到达时，将使目的主机的 UDP 模块产生一份“端口不可达”错误 ICMP 报文。如果数据报没有到达，则可能是超时。 2. Traceroute 还有一个作用是故意设置不分片，从而确定路径的 MTU。要做的工作首先是发送分组，并设置“不分片”标志。发送的第一个分组的长度正好与出口 MTU 相等。如果中间遇到窄的关口会被卡住，会发送 ICMP 网络差错包，类型为“需要进行分片但设置了不分片位”。其实，这是人家故意的好吧，每次收到 ICMP“不能分片”差错时就减小分组的长度，直到到达目标主机。 3.4 总结 ICMP 相当于网络世界的侦察兵。 两种类型的 ICMP 报文，一种是主动探查的查询报文，一种异常报告的差错报文； ping 使用查询报文，Traceroute 使用差错报文。 不应发送ICMP差错报文的几种情况： 对ICMP差错报文，不再发送ICMP差错报告报文。 对第一个分片的数据报片的所有后续数据报片，都不发送ICMP差错报告报文 对具有多播地址的数据报，都不发送ICMP差错报告报文 对具有特殊地址（如127.0.0.0或0.0.0.0）的数据报，不发送ICMP差错报告报文 tracerouter发udp，为啥出错回icmp？ICMP一般认为属于网络层的，和IP同一层，是管理和控制IP的一种协议，而UDP和TCP是传输层，所以UDP出错可以返回ICMP差错报文。正常情况下，协议栈能正常走到udp，当然正常返回udp。但是，你主机不可达，是ip层的（还没到udp）。ip层，当然只知道回icmp。报文分片错误也是同理。","categories":[{"name":"随笔","slug":"随笔","permalink":"https://liangweijiang.github.io/categories/%E9%9A%8F%E7%AC%94/"}],"tags":[{"name":"网络协议","slug":"网络协议","permalink":"https://liangweijiang.github.io/tags/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/"}]},{"title":"学习笔记之---restful架构","slug":"restful","date":"2020-03-14T13:19:05.357Z","updated":"2020-03-14T13:20:50.713Z","comments":true,"path":"2020/03/14/restful/","link":"","permalink":"https://liangweijiang.github.io/2020/03/14/restful/","excerpt":"","text":"详解restful架构1. web服务我们在浏览器中能看到的每个网站，都是一个web服务。这种”互联网软件”采用客户端/服务器模式，建立在分布式体系上，通过互联网通信，具有高延时（high latency）、高并发等特点。网站开发，完全可以采用软件开发的模式。但是传统上，软件和网络是两个不同的领域，很少有交集；软件开发主要针对单机环境，网络则主要研究系统之间的通信。互联网的兴起，使得这两个领域开始融合，现在我们必须考虑，如何开发在互联网环境中使用的软件。 目前主流的三种web服务交互方案： – REST （ Representational State Transfer）表述性状态转移 – SOAP （Simple Object Access Protocol） 简单的对象访问协议 – XML-RPC （XML Remote Procedure Call）基于XML的远程过程调用 XML-RPC是通过XML将调用函数封装，并使用HTTP协议作为传送机制。后来在新的功能不断被引入下，这个标准慢慢演变成为今日的SOAP协定。SOAP服务则是以本身所定义的操作集，来访问网络上的资源。SOAP也是基于XML的，但是它不只限于HTTP协议的传输，包括TCP协议，UDP协议都可以传输。REST相比SOAP更加简洁，性能和开发效率也有突出的优势。 2. restful架构的历史REST这个词，全称是Representational State Transfer，中文意思是表述（编者注：通常译为表征）性状态转移，是Roy Thomas Fielding在他2000年的博士论文中提出的。Fielding是一个非常重要的人，他是HTTP协议（1.0版和1.1版）的主要设计者、Apache服务器软件的作者之一、Apache基金会的第一任主席。所以，他的这篇论文一经发表，就引起了关注，并且立即对互联网开发产生了深远的影响。他在论文中提到：”我这篇文章的写作目的，就是想在符合架构原理的前提下，理解和评估以网络为基础的应用软件的架构设计，得到一个功能强、性能好、适宜通信的架构。REST指的是一组架构约束条件和原则。” 如果一个架构符合REST的约束条件和原则，我们就称它为RESTful架构。 REST本身并没有创造新的技术、组件或服务，而隐藏在RESTful背后的理念就是使用Web的现有特征和能力， 更好地使用现有Web标准中的一些准则和约束。虽然REST本身受Web技术的影响很深， 但是理论上REST架构风格并不是绑定在HTTP上，只不过目前HTTP是唯一与REST相关的实例。 所以我们这里描述的REST也是通过HTTP实现的REST。 3. 理解restful3.1 什么是资源REST全称是表述性状态转移，那究竟指的是什么的表述? 其实指的就是资源。所谓”资源”，就是网络上的一个实体，或者说是网络上的一个具体信息。它可以是一段文本、一张图片、一首歌曲、一种服务，总之就是一个具体的实在。任何事物，只要有被引用到的必要，它就是一个资源。那么在我们的网络中，我们要引用资源，资源一定要有一个标识，在web中的唯一标识就是URI（统一资源定位符），URI我们不常听说，我们经常用URL，那么两者区别是什么 什么是URI，URL？ URI 统一资源标志符。 URL 统一资源定位符。 URI是给我们的资源进行标识的，URL是描述我们资源地址的。比如说我们每个人都有名字和身份证，名字可能重名，但是身份证是唯一的，那么身份证号就可以是我们的URI，标识我们每个人，也可以说标识我们每个资源。我们可以通过身份证号找到这个人，也可以通过地址找到他，这个就是我们的URL，我们通过这两种方式都可以找到我们的资源， 其实我们的URL可以说是URI的子集，通过定位的方式实现的URI。 3.2 统一资源接口现在我们可以通过URL去访问到资源，那么我们对资源会有很多不同的操作，增删改查，以前我们可能会为了这个增加新设计一个URL，然后这个URL就是对数据进行增加的，还会为了更新和删除分别设计一个URL，现在我们不用了，我们只有一个URL，然后根据HTTP请求方式的不同，对资源进行不同的操作，这个就是是统一资源接口。我们一定要遵循HTTP请求方法的语义，也就是说POST请求就在新增数据等…. RESTful架构应该遵循统一接口原则，统一接口包含了一组受限的预定义的操作，不论什么样的资源，都是通过使用相同的接口进行资源的访问。接口应该使用标准的HTTP方法如GET，PUT和POST，并遵循这些方法的语义。如果按照HTTP方法的语义来暴露资源，那么接口将会拥有安全性和幂等性的特性，例如GET和HEAD请求都是安全的， 无论请求多少次，都不会改变服务器状态。而GET、HEAD、PUT和DELETE请求都是幂等的，无论对资源操作多少次， 结果总是一样的，后面的请求并不会产生比第一次更多的影响。 下面列出了GET，DELETE，PUT和POST的典型用法: GET 安全且幂等 获取表示 变更时获取表示（缓存） 200（OK） - 表示已在响应中发出 204（无内容） - 资源有空表示 301（Moved Permanently） - 资源的URI已被更新 303（See Other） - 其他（如，负载均衡） 304（not modified）- 资源未更改（缓存） 400 （bad request）- 指代坏请求（如，参数错误） 404 （not found）- 资源不存在 406 （not acceptable）- 服务端不支持所需表示 500 （internal server error）- 通用错误响应 503 （Service Unavailable）- 服务端当前无法处理请求 POST 不安全且不幂等 使用服务端管理的（自动产生）的实例号创建资源 创建子资源 部分更新资源 如果没有被修改，则不过更新资源（乐观锁） 200（OK）- 如果现有资源已被更改 201（created）- 如果新资源被创建 202（accepted）- 已接受处理请求但尚未完成（异步处理） 301（Moved Permanently）- 资源的URI被更新 303（See Other）- 其他（如，负载均衡） 400（bad request）- 指代坏请求 404 （not found）- 资源不存在 406 （not acceptable）- 服务端不支持所需表示 409 （conflict）- 通用冲突 412 （Precondition Failed）- 前置条件失败（如执行条件更新时的冲突） 415 （unsupported media type）- 接受到的表示不受支持 500 （internal server error）- 通用错误响应 503 （Service Unavailable）- 服务当前无法处理请求 PUT 不安全但幂等 用客户端管理的实例号创建一个资源 通过替换的方式更新资源 如果未被修改，则更新资源（乐观锁） 200 （OK）- 如果已存在资源被更改 201 （created）- 如果新资源被创建 301（Moved Permanently）- 资源的URI已更改 303 （See Other）- 其他（如，负载均衡） 400 （bad request）- 指代坏请求 404 （not found）- 资源不存在 406 （not acceptable）- 服务端不支持所需表示 409 （conflict）- 通用冲突 412 （Precondition Failed）- 前置条件失败（如执行条件更新时的冲突） 415 （unsupported media type）- 接受到的表示不受支持 500 （internal server error）- 通用错误响应 503 （Service Unavailable）- 服务当前无法处理请求 DELETE 不安全但幂等 删除资源 200 （OK）- 资源已被删除 301 （Moved Permanently）- 资源的URI已更改 303 （See Other）- 其他，如负载均衡 400 （bad request）- 指代坏请求 404 （not found）- 资源不存在 409 （conflict）- 通用冲突 500 （internal server error）- 通用错误响应 503 （Service Unavailable）- 服务端当前无法处理请求 3.3 资源的表述资源的表述其实就是资源的展现形式，我们客户端和服务端传输的都是资源的表述，而不是资源本身。例如文本资源可以采用html、xml、json等格式，图片可以使用PNG或JPG展现出来。资源的表述包括数据和描述数据的元数据，例如，HTTP头”Content-Type” 就是这样一个元数据属性。那么客户端如何知道服务端提供哪种表述形式呢?可以通过HTTP内容协商，客户端可以通过Accept头请求一种特定格式的表述，服务端则通过Content-Type告诉客户端资源的表述形式。这些资源的表述呈现在页面上，就是我们说的资源状态。 3.4 状态转移我们在看页面的时候，从当前资源的表述(也可以说状态或者表现层)会跳转到其他的资源状态。服务端通过超媒体告诉客户端当前状态有哪些后续状态可以进入。这些类似”下一页”之类的链接起的就是这种推进状态的作用——指引你如何从当前状态进入下一个可能的状态。互联网通信协议HTTP协议，是一个无状态协议。这意味着，所有的状态都保存在服务器端。因此，如果客户端想要操作服务器，必须通过某种手段，让服务器端发生”状态转化”（State Transfer）。而这种转化是建立在表现层之上的，所以就是”表现层状态转化”。客户端用到的手段，只能是HTTP协议。具体来说，就是HTTP协议里面，四个表示操作方式的动词：GET、POST、PUT、DELETE。它们分别对应四种基本操作：GET用来获取资源，POST用来新建资源（也可以用于更新资源），PUT用来更新资源，DELETE用来删除资源。 3.5 总结可以得知REST风格的特点如下： 在web中，只要有被引用的必要都叫资源。 每个URI代表一个资源，独一无二的。 客户端通过HTTP的方法，对服务器端资源进行操作； 客户端和服务器之间，传递这种资源的某种表现层； 通过超链接的指引，实现”表现层状态转移”。 4. restful规范一 、面向资源编程 每个URL代表一种资源，URL中尽量不要用动词，要用名词。 二 、根据method不同，进行不同的操作 GET : 用来获取资源 POST : 用来新建资源（也可以用于更新资源） PUT : 用来更新资源 DELETE : 用来删除资源 PATCH : 用来局部更新资源 三 、在URL中体现版本 https://www.bootcss.com/v1/mycss https://v1.bootcss.com/mycss 四 、在URL中体现是否是API https://www.bootcss.com/api/mycss https://api.bootcss.com/mycss 五 、在URL中的过滤条件https://www.bootcss.com/v1/mycss？page=3 六 、尽量使用HTTPShttps://www.bootcss.com/v1/mycss 七 、响应时设置状态码 1** 信息，服务器收到请求，需要请求者继续执行操作 2** 成功，操作被成功接收并处理 3** 重定向，需要进一步的操作以完成请求 4** 客户端错误，请求包含语法错误或无法完成请求 5** 服务器错误，服务器在处理请求的过程中发生了错误 八 、返回值 GET请求 返回查到所有或单条数据 POST请求 返回新增的数据 PUT请求 返回更新数据 PATCH请求 局部更新 返回更新整条数据 DELETE请求 返回值为空 九 、返回错误信息返回值携带错误信息 十 、Hypermedia API如果遇到需要跳转的情况 携带调转接口的URL ret = { code: 100, data: { id: 1, name: &apos;小明&apos;, depart_id: &apos;http://www.baidu.com/api/v1/depart/4/&apos; } }参考文章理解RESTful架构 RESTful RESTful 架构详解","categories":[{"name":"随笔","slug":"随笔","permalink":"https://liangweijiang.github.io/categories/%E9%9A%8F%E7%AC%94/"}],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://liangweijiang.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}]},{"title":"学习笔记之---七层网络协议和四层网络协议的区别","slug":"osi--tcp","date":"2020-03-14T09:11:21.531Z","updated":"2020-03-14T13:31:43.080Z","comments":true,"path":"2020/03/14/osi--tcp/","link":"","permalink":"https://liangweijiang.github.io/2020/03/14/osi--tcp/","excerpt":"","text":"浅谈七层网络协议和四层网络协议的区别1.OSI七层网络模型介绍 OSI（Open System Interconnection，开放系统互连）七层网络模型称为开放式系统互联参考模型 ，是一个逻辑上的定义，一个规范，它把网络从逻辑上分为了7层。每一层都有相关、相对应的物理设备，比如路由器，交换机。 OSI七层模型是一种框架性的设计方法，建立七层模型的主要目的是为解决异种网络互连时所遇到的兼容性问题，其最主要的功能就是帮助不同类型的主机实现数据传输。它的最大优点是将服务、接口和协议这三个概念明确地区分开来，通过七个层次化的结构模型使不同的系统不同的网络之间实现可靠的通讯。 七层模型的结构如下: 各层简介： 物理层（Physical Layer）：主要定义物理设备标准，如网线的接口类型、光纤的接口类型、各种传输介质的传输速率等。它的主要作用是传输比特流（就是由1、0转化为电流强弱来进行传输,到达目的地后在转化为1、0，也就是我们常说的数模转换与模数转换），这一层的数据叫做比特，单位是bit比特。 数据链路层（Datalink Layer）：定义了如何让格式化数据以进行传输，以及如何让控制对物理介质的访问，这一层通常还提供错误检测和纠正，以确保数据的可靠传输。交换机(二层)、网桥设备在这一层。数据链路层协议的代表包括：PPP、STP、帧中继等。 网络层（Network Layer）：在位于不同地理位置的网络中的两个主机系统之间提供连接和路径选择，Internet的发展使得从世界各站点访问信息的用户数大大增加，而网络层正是管理这种连接的层。网络层负责在源机器和目标机器之间建立它们所使用的路由。路由器在该层。协议有：IP、ICMP（互联网控制报文协议）、ARP（地址转换协议）、RARP（反向地址转换协议） 传输层（Transport Layer）：O S I 模型中最重要的一层。定义了一些传输数据的协议和端口号（WWW端口80等），如：TCP（传输控制协议，传输效率低，可靠性强，用于传输可靠性要求高，数据量大的数据），UDP（用户数据报协议，与TCP特性恰恰相反，用于传输可靠性要求不高，数据量小的数据，如QQ聊天数据就是通过这种方式传输的）， 主要是将从下层接收的数据进行分段和传输，到达目的地址后再进行重组，常常把这一层数据叫做段。传输协议同时进行流量控制或是基于接收方可接收数据的快慢程度规定适当的发送速率。除此之外，传输层按照网络能处理的最大尺寸将较长的数据包进行强制分割。例如，以太网无法接收大于1 5 0 0 字节的数据包。发送方节点的传输层将数据分割成较小的数据片，同时对每一数据片安排一序列号，以便数据到达接收方节点的传输层时，能以正确的顺序重组。该过程即被称为排序。 会话层（Session Layer）：负责在网络中的两节点之间建立、维持和终止通信。 会话层的功能包括：建立通信链接，保持会话过程通信链接的畅通，同步两个节点之间的对话，决定通信是否被中断以及通信中断时决定从何处重新发送。通过传输层（端口号：传输端口与接收端口）建立数据传输的通路，主要在你的系统之间发起会话或者接受会话请求（设备之间需要互相认识可以是IP也可以是MAC或者是主机名）。 表示层（Presentation Layer）：应用程序和网络之间的翻译官，在表示层，数据将按照网络能理解的方案进行格式化；这种格式化也因所使用网络的类型不同而不同。 表示层管理数据的解密与加密，如系统口令的处理。可确保一个系统的应用层所发送的信息可以被另一个系统的应用层读取。例如，PC程序与另一台计算机进行通信，其中一台计算机使用扩展二一十进制交换码（EBCDIC），而另一台则使用美国信息交换标准码（ASCII）来表示相同的字符。如有必要，表示层会通过使用一种通格式来实现多种数据格式之间的转换。 应用层（Application Layer）： 是最靠近用户的OSI层，这一层为用户的应用程序（例如电子邮件、文件传输和终端仿真）提供网络服务。 2. TCP/IP四层网络模型介绍TCP/IP协议栈是美国国防部高级研究计划局计算机网（Advanced Research Projects Agency Network，ARPANET）和其后继因特网使用的参考模型。ARPANET是由美国国防部（U.S．Department of Defense，DoD）赞助的研究网络。最初，它只连接了美国境内的四所大学。随后的几年中，它通过租用的电话线连接了数百所大学和政府部门。最终ARPANET发展成为全球规模最大的互连网络-因特网。最初的ARPANET于1990年永久性地关闭。 TCP/IP是一组协议的代名词，它还包括许多协议，组成了TCP/IP协议簇。TCP/IP协议簇分为四层，IP位于协议簇的第二层(对应OSI的第三层)，TCP位于协议簇的第三层(对应OSI的第四层)。TCP/IP通讯协议采用了4层的层级结构，每一层都呼叫它的下一层所提供的网络来完成自己的需求。这4层分别为： 各层简介： 应用层：应用程序间沟通的层，如简单电子邮件传输（SMTP）、文件传输协议（FTP）、网络远程访问协议（Telnet）等。 传输层：在此层中，它提供了节点间的数据传送服务，如传输控制协议（TCP）、用户数据报协议（UDP）等，TCP和UDP给数据包加入传输数据并把它传输到下一层中，这一层负责传送数据，并且确定数据已被送达并接收。 网络层：负责提供基本的数据封包传送功能，让每一块数据包都能够到达目的主机（但不检查是否被正确接收），如网际协议（IP）。 网络接口层：对实际的网络媒体的管理，定义如何使用实际网络（如Ethernet、Serial Line等）来传送数据。 3. OSI七层和TCP/IP四层的关系 其实在应用、表示和会话这三层之间的协议可共用（由于实际的网络协议将它们归了一类所致） 4. OSI七层和TCP/IP四层的区别 OSI引入了服务、接口、协议、分层的概念，TCP/IP借鉴了OSI的这些概念建立TCP/IP模型。 OSI先有模型，后有协议，先有标准，后进行实践；而TCP/IP则相反，先有协议和应用再提出了模型，且是参照的OSI模型。 OSI是一种理论下的模型，而TCP/IP已被广泛使用，成为网络互联事实上的标准。 除了层的数量之外，开放式系统互联（OSI）模型与TCP/IP协议有什么区别？ 开放式系统互联模型是一个参考标准，解释协议相互之间应该如何相互作用。TCP/IP协议是美国国防部发明的，是让互联网成为了目前这个样子的标准之一。开放式系统互联模型中没有清楚地描绘TCP/IP协议，但是在解释TCP/IP协议时很容易想到开放式系统互联模型。两者的主要区别如下： TCP/IP协议中的应用层处理开放式系统互联模型中的第五层、第六层和第七层的功能。 TCP/IP协议中的传输层并不能总是保证在传输层可靠地传输数据包，而开放式系统互联模型可以做到。这是因为TCP/IP协议还提供一项名为UDP（用户数据报协议）的选择。UDP不能保证可靠的数据包传输。 参考文章网络OSI七层架构与TCP四层架构的应用与区别 详谈OSI七层网络协议和TCP/IP协议 七层协议和四层协议","categories":[{"name":"随笔","slug":"随笔","permalink":"https://liangweijiang.github.io/categories/%E9%9A%8F%E7%AC%94/"}],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://liangweijiang.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}]},{"title":"每日一道算法之--最小栈","slug":"algorithm-155","date":"2020-03-14T04:36:34.607Z","updated":"2020-03-14T04:37:37.764Z","comments":true,"path":"2020/03/14/algorithm-155/","link":"","permalink":"https://liangweijiang.github.io/2020/03/14/algorithm-155/","excerpt":"","text":"最小栈力扣第155题 : https://leetcode-cn.com/problems/min-stack/ 设计一个支持 push，pop，top 操作，并能在常数时间内检索到最小元素的栈。 push(x) – 将元素 x 推入栈中。pop() – 删除栈顶的元素。top() – 获取栈顶元素。getMin() – 检索栈中的最小元素。示例: MinStack minStack = new MinStack();minStack.push(-2);minStack.push(0);minStack.push(-3);minStack.getMin(); –&gt; 返回 -3.minStack.pop();minStack.top(); –&gt; 返回 0.minStack.getMin(); –&gt; 返回 -2. 最开始的思路当我看到这道题的时候,因为是求最小的元素,所以我只用了一个变量,想要动态的更新最小值。但是有一个问题就出现了，当我把最小值pop出来后，当前的最小值就又会发生变化了，所以用变量保存最小值不是一个理想的解法。我又想到了再用一个数组来保存最小的的变化信息，当pop的时候在更新最小值。 12345678910111213141516171819202122232425class MinStack: def __init__(self): self.stack = [] self.min_ele = float('inf') self.min_list = [] def push(self, x: int) -&gt; None: self.stack.append(x) self.min_ele = min(self.min_ele, x) if self.min_ele not in self.min_list: self.min_list.append(self.min_ele) def pop(self) -&gt; None: pop_ele = self.stack.pop() if self.min_ele == pop_ele and pop_ele not in self.stack: self.min_list.pop() if self.min_list: self.min_ele = self.min_list[-1] def top(self) -&gt; int: if self.stack: return self.stack[-1] def getMin(self) -&gt; int: return self.min_ele 虽然没通过,但是辅助栈的雏形已经出来了 1. 同步的辅助栈 上面的代码其实存储最小值的变量是没必要的,因为最小值已经存储在辅助栈里面了 当我们在push入栈的时候,同时将该数和辅助栈的栈顶元素比较大小,如果小于栈顶元素,则最小值为该元素,将该元素push入辅助栈中;如果大于栈顶元素,则证明最小值仍为栈顶元素 从而实现了主栈和辅助栈永远是同步的,即长度一样,级主栈中每一个元素对应的栈中最小值是正确的. 123456789101112131415161718192021222324252627282930class MinStack: def __init__(self): \"\"\" initialize your data structure here. \"\"\" self.stack = [] self.min_stack = [] def push(self, x: int) -&gt; None: self.stack.append(x) # 主要逻辑就是在这里 if not self.min_stack or self.min_stack[-1] &gt; x: self.min_stack.append(x) else: self.min_stack.append(self.min_stack[-1]) def pop(self) -&gt; None: if self.stack: self.stack.pop() self.min_stack.pop() def top(self) -&gt; int: if self.stack: return self.stack[-1] def getMin(self) -&gt; int: if self.min_stack: return self.min_stack[-1] 1.1 复杂度分析时间复杂度：，“出栈”、“入栈”、“查看栈顶元素”的操作不论数据规模多大，都只是有限个步骤，因此时间复杂度是：O(1)。 空间复杂度：O(N)，这里 N 是读出的数据的个数。 2. 不同步的辅助栈同步辅助栈虽然很好理解,但是有一个问题就是它为了保持长度跟主栈一样,从而造成了没必要的浪费,所以这里讲一下不同步的辅助栈。 不同步辅助栈就是只有在push时该数小于或等于辅助栈的栈顶元素是才push入辅助栈 在pop元素之，如果该元素等于辅助栈的栈顶元素，则将辅助栈的栈顶元素pop出来 12345678910111213141516171819202122232425262728class MinStack: def __init__(self): \"\"\" initialize your data structure here. \"\"\" self.stack = [] self.min_stack = [] def push(self, x: int) -&gt; None: self.stack.append(x) if not self.min_stack or self.min_stack[-1] &gt;= x: self.min_stack.append(x) def pop(self) -&gt; None: if self.stack: top = self.stack.pop() if self.min_stack and top == self.min_stack[-1]: self.min_stack.pop() def top(self) -&gt; int: if self.stack: return self.stack[-1] def getMin(self) -&gt; int: if self.min_stack: return self.min_stack[-1] 2.1 复杂度分析时间复杂度：，“出栈”、“入栈”、“查看栈顶元素”的操作不论数据规模多大，都只是有限个步骤，因此时间复杂度是：O(1)。 空间复杂度：O(N)，这里 N 是读出的数据的个数 相似题目最大栈 滑动窗口最大值","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://liangweijiang.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"栈","slug":"栈","permalink":"https://liangweijiang.github.io/tags/%E6%A0%88/"}]},{"title":"学习笔记之---堆和栈的区别","slug":"stack-heap","date":"2020-03-13T15:51:37.404Z","updated":"2020-03-15T03:48:02.075Z","comments":true,"path":"2020/03/13/stack-heap/","link":"","permalink":"https://liangweijiang.github.io/2020/03/13/stack-heap/","excerpt":"","text":"浅谈堆和栈的区别1. 数据结构中的堆栈相信每一个程序员对数据结构中的堆和栈都不陌生,他们在我们计算机的数据结构中承担着不可或缺的角色。其实堆和栈都是一种数据项按序排列的数据结构。 1.1 栈就像一摞叠在一起的盘子我们平时放盘子的时候，都是从下往上一个一个放；取的时候，我们也是从上往下一个一个地依次取，不能从中间任意抽出。后进者先出，先进者后出，这就是典型的“栈”结构。从栈的操作特性上来看，栈是一种“操作受限”的线性表，只允许在一端插入和删除数据。 1.2 堆就像一颗倒过来的数堆是一种经过排序的树形数据结构，每个结点都有一个值。通常我们所说的堆的数据结构，是指二叉堆。堆是一种特殊的树。只要满足这两点，它就是一个堆。 堆是一个完全二叉树； 堆中每一个节点的值都必须大于等于（或小于等于）其子树中每个节点的值。 由于堆的这个特性，常用来实现优先队列，堆的存取是随意，这就如同我们在图书馆的书架上取书，虽然书的摆放是有顺序的，但是我们想取任意一本时不必像栈一样，先取出前面所有的书，书架这种机制不同于箱子，我们可以直接取出我们想要的书。 2. 内存分配中的堆栈一个由c/C++编译的程序占用的内存分为以下几个区: 栈区：存放函数的参数值、局部变量等，由编译器自动分配和释放，通常在函数执行完后就释放了，其操作方式类似于数据结构中的栈。栈内存分配运算内置于CPU的指令集，效率很高，但是分配的内存量有限，比如iOS中栈区的大小是2M。 堆区：就是通过new、malloc、realloc分配的内存块，编译器不会负责它们的释放工作，需要用程序区释放。分配方式类似于数据结构中的链表。在iOS开发中所说的“内存泄漏”说的就是堆区的内存。 静态区：全局变量和静态变量（在iOS中就是用static修饰的局部变量或者是全局全局变量）的存储是放在一块的，初始化的全局变量和静态变量在一块区域，未初始化的全局变量和未初始化的静态变量在相邻的另一块区域。程序结束后，由系统释放。 常量区：常量存储在这里，不允许修改。 代码区：存放函数体的二进制代码。 2.1 内存分配中堆和栈的区别1. 申请方式和回收方式不同 栈(stack):栈空间的内存是由系统自动分配，一般存放局部变量，比如对象的地址等值，例如，声明在函数中一个局部变量 int b; 系统自动在栈中为b开辟空间。不需要程序员对这块内存进行管理，比如，函数中的局部变量的作用范围（生命周期）就是在调完这个函数之后就结束了。这些在系统层面都已经限定住了，程序员只需要在这种约束下进行程序编程就好，根本就没有把这块内存的管理权给到程序员，肯定也就不存在让程序员管理一说。 堆(heap):需要程序员自己申请，并指明大小，在c中malloc函数如 p1 = (char *)malloc(10);在C++中用new运算符如 p2 = (char *)malloc(10); 但是注意p1、p2本身是在栈中的。堆空间的内存是动态分配的，一般存放对象，并且需要手动释放内存。当然，iOS引入了ARC（自动引用计数管理技术）之后，程序员就不需要用代码管理对象的内存了，之前MRC（手动管理内存）的时候，程序员需要手动release对象。另外，ARC只是一种中间层的技术，虽然在ARC模式下，程序员不需要像之前那么麻烦管理内存，但是需要遵循ARC技术的规范操作，比如使用属性限定符weak、strong、assigen等。因此，如果程序员没有按ARC的规则并合理的使用这些属性限定符的话，同样是会造成内存泄漏的。 由于栈上的空间是自动分配自动回收的，所以栈上的数据的生存周期只是在函数的运行过程中，运行后就释放掉，不可以再访问。而堆上的数据只要程序员不释放空间，就一直可以访问到，不过缺点是一旦忘记释放会造成内存泄露。 2. 申请后系统的响应 栈：只要栈的剩余空间大于所申请空间，系统将为程序提供内存，否则将报异常提示栈溢出。 堆：首先应该知道操作系统有一个记录空闲内存地址的链表，当系统收到程序的申请时，会遍历该链表，寻找第一个空间大于所申请空间的堆结点，然后将该结点从空闲结点链表中删除，并将该结点的空间分配给程序，另外，对于大多数系统，会在这块内存空间中的首地址处记录本次分配的大小，这样，代码中的delete语句才能正确的释放本内存空间。另外，由于找到的堆结点的大小不一定正好等于申请的大小，系统会自动的将多余的那部分重新放入空闲链表中。 3. 申请大小的限制 栈：在Windows下,栈是向低地址扩展的数据结构，是一块连续的内存的区域。这句话的意思是栈顶的地址和栈的最大容量是系统预先规定好的，在WINDOWS下，栈的大小是2M（也有的说是1M，总之是一个编译时就确定的常数），如果申请的空间超过栈的剩余空间时，将提示overflow。因此，能从栈获得的空间较小。 堆：堆是向高地址扩展的数据结构，是不连续的内存区域。这是由于系统是用链表来存储的空闲内存地址的，自然是不连续的，而链表的遍历方向是由低地址向高地址。堆的大小受限于计算机系统中有效的虚拟内存。由此可见，堆获得的空间比较灵活，也比较大。 4. 申请效率的比较： 栈由系统自动分配，速度较快。但程序员是无法控制的。 堆是由new分配的内存，一般速度比较慢，而且容易产生内存碎片,不过用起来最方便. 另外，在WINDOWS下，最好的方式是用VirtualAlloc分配内存，他不是在堆，也不是在栈是直接在进程的地址空间中保留一快内存，虽然用起来最不方便。但是速度快，也最灵活。 2.5 堆和栈中的存储内容 栈： 栈空间中一般存储基本数据类型，对象的地址；在函数调用时，第一个进栈的是主函数中后的下一条指令（函数调用语句的下一条可执行语句）的地址，然后是函数的各个参数，在大多数的C编译器中，参数是由右往左入栈的，然后是函数中的局部变量。注意静态变量是不入栈的。 当本次函数调用结束后，局部变量先出栈，然后是参数，最后栈顶指针指向最开始存的地址，也就是主函数中的下一条指令，程序由该点继续运行。 堆：堆空间一般存放对象本身，block的copy等;一般是在堆的头部用一个字节存放堆的大小。堆中的具体内容有程序员安排。 2.6 存取效率的比较char s1[] = “aaaaaaaaaaaaaaa”;char *s2 = “bbbbbbbbbbbbbbbbb”;aaaaaaaaaaa是在运行时刻赋值的；而bbbbbbbbbbb是在编译时就确定的；但是，在以后的存取中，在栈上的数组比指针所指向的字符串(例如堆)快。比如： 123456789＃includevoid main() &#123; char a = 1; char c[] = \"1234567890\"; char *p =\"1234567890\"; a = c[1]; a = p[1]; return;&#125; 对应的汇编代码 123456710: a = c[1];00401067 8A 4D F1 mov cl,byte ptr [ebp-0Fh]0040106A 88 4D FC mov byte ptr [ebp-4],cl11: a = p[1];0040106D 8B 55 EC mov edx,dword ptr [ebp-14h]00401070 8A 42 01 mov al,byte ptr [edx+1]00401073 88 45 FC mov byte ptr [ebp-4],al 第一种在读取时直接就把字符串中的元素读到寄存器cl中，而第二种则要先把指针值读到edx中，在根据edx读取字符，显然慢了。 总结堆和栈的区别可以用如下的比喻来看出： 使用栈就象我们去饭馆里吃饭，只管点菜（发出申请）、付钱、和吃（使用），吃饱了就走，不必理会切菜、洗菜等准备工作和洗碗、刷锅等扫尾工作，他的好处是快捷，但是自由度小。 使用堆就象是自己动手做喜欢吃的菜肴，比较麻烦，但是比较符合自己的口味，而且自由 度大。 (经典！) 参考文章 堆栈的工作原理 C语言堆栈入门——堆和栈的区别 关于堆栈的讲解 堆和栈的区别是什么？","categories":[{"name":"随笔","slug":"随笔","permalink":"https://liangweijiang.github.io/categories/%E9%9A%8F%E7%AC%94/"}],"tags":[{"name":"堆栈","slug":"堆栈","permalink":"https://liangweijiang.github.io/tags/%E5%A0%86%E6%A0%88/"},{"name":"内存分配","slug":"内存分配","permalink":"https://liangweijiang.github.io/tags/%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D/"}]},{"title":"每日一道算法之--剪绳子","slug":"algorithm-interview-14","date":"2020-03-12T15:58:33.660Z","updated":"2020-03-12T16:00:21.071Z","comments":true,"path":"2020/03/12/algorithm-interview-14/","link":"","permalink":"https://liangweijiang.github.io/2020/03/12/algorithm-interview-14/","excerpt":"","text":"剪绳子力扣 面试题14- I :https://leetcode-cn.com/problems/jian-sheng-zi-lcof/相同题目:343. 整数拆分 给你一根长度为 n 的绳子，请把绳子剪成整数长度的 m 段（m、n都是整数，n&gt;1并且m&gt;1），每段绳子的长度记为 k[0],k[1]…k[m] 。请问 k[0]k[1]…*k[m] 可能的最大乘积是多少？例如，当绳子的长度是8时，我们把它剪成长度分别为2、3、3的三段，此时得到的最大乘积是18。 示例 1： 输入: 2输出: 1解释: 2 = 1 + 1, 1 × 1 = 1 示例 2: 输入: 10输出: 36解释: 10 = 3 + 3 + 4, 3 × 3 × 4 = 36提示： 2 &lt;= n &lt;= 58 1. 动态规划一看到这道题的时候,看到最这个字,首先我第一肯定是想到动态规划。当我们首先要判断边界，当n小于4的时候，只会有一种可能或者没有，如 n == 2时,只可能分成两段,分别为1, 1。当n大于3时，我们剪断绳子的第一刀时，可以剪1,2，.., n-1的长度,即有n-1中可能所以状态转移方程为 f(n) = max(f(i) * f(n - i)) , 其中 0&lt;i&lt;n 1234567891011121314151617class Solution: def cuttingRope(self, n: int) -&gt; int: if n &lt; 2: return 0 if n == 2: return 1 if n == 3: return 2 dp = ['' for _ in range(n + 1)] dp[0] = 0 dp[1] = 1 dp[2] = 2 dp[3] = 3 max_count = 0 for i in range(4, n + 1): # 只需要遍历到一半,因为前半部分和后半部分是一样的 for j in range(1, i //2 + 1): max_count = max(max_count, dp[j] * dp[i - j]) dp[i] = max_count return dp[n] 1.1 复杂度分析时间复杂度:当n &gt; 4时,每一个子问题(假设长度为m),他的第一刀都有m - 1中可能,所以需要两层循环,时间复杂度为$$O(n^2)$$ 空间复杂度:运用了列表的数据结构,且其大小和n有关,即空间复杂度为O(1) 2. 贪婪算法通过复杂度分析,动态规划的时间复杂度和空间复杂度都不太理想,所以我有翻了《剑指offer》里看了一种非常巧妙的方法—-贪婪算法 可以发现,只有当n == 4时,以2分割比3更佳 所以当绳子大于4时,尽可能地以3分割;如果剩下的绳子刚好等于4,则将4分割成两个2 12345678910import mathclass Solution: def cuttingRope(self, n: int) -&gt; int: if n &lt;= 3: return n - 1 count3 = n // 3 # 如果存在4,要以2分割 if n - 3 * count3 == 1: count3 -= 1 count2 = (n - count3 * 3) // 2 return int(math.pow(3, count3)* math.pow(2, count2)) 2.1 复杂度分析时间复杂度: O(1) 空间复杂度:O(1) 总结做这道题之前,我只要看到最这个字,肯定都会联想到动态规划,做了这道题之后,哎,每一种题目,都还是会有自己特定的最合适的算法,而不是dp永远都是最好的.向之前有一道题322. 零钱兑换,我一开始就是想到了贪心算法,但其实正确的解法是动态规划。而且也要动脑子,这道题其实和数学的联系很大,如果理解了规律真的不难,还是要多刷多练,踏实地学习. 类似题目171. Excel表列序号 975. 奇偶跳","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://liangweijiang.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"动态规划","slug":"动态规划","permalink":"https://liangweijiang.github.io/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"},{"name":"数学","slug":"数学","permalink":"https://liangweijiang.github.io/tags/%E6%95%B0%E5%AD%A6/"},{"name":"贪心算法","slug":"贪心算法","permalink":"https://liangweijiang.github.io/tags/%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95/"}]},{"title":"每日一道算法之--寻找旋转排序数组中的最小值","slug":"algorithm-153","date":"2020-03-11T13:02:26.632Z","updated":"2020-03-11T13:03:39.316Z","comments":true,"path":"2020/03/11/algorithm-153/","link":"","permalink":"https://liangweijiang.github.io/2020/03/11/algorithm-153/","excerpt":"","text":"寻找旋转排序数组中的最小值力扣第153题:https://leetcode-cn.com/problems/find-minimum-in-rotated-sorted-array/ 假设按照升序排序的数组在预先未知的某个点上进行了旋转。 ( 例如，数组 [0,1,2,4,5,6,7] 可能变为 [4,5,6,7,0,1,2] )。 请找出其中最小的元素。 你可以假设数组中不存在重复元素。 示例 1: 输入: [3,4,5,1,2]输出: 1 示例 2: 输入: [4,5,6,7,0,1,2]输出: 0 这道题是剑指的原题的简单版,这里可以假设假设数组中不存在重复元素。找出其中最小的元素,直接遍历数组找到最小的元素不就好了吗,时间复杂度也为O(n),但是这样就和旋转数组没什么关系了,所以就算做出来了,面试也没什么效果。 二分查找思想我们知道在一个有序数组中，用二分法查找一个数，时间复杂度为O（logn），效率非常高。那么这道题也是有序数组的变形，应该也能用二分法。 可以看到，旋转数组分为了两部分，左右两部分也都是有序的 而且旋转数组的分界点，正是最小的元素 所以我们可以利用双指针，开始时分别指向数组的最前（左部分）和最后（右部分），然后去中间点，如果中间点的值比最前的点大，那么证明中间点在左部分，且最小元素在右部分，所以可以将左部分的指针指向中间点，这就是实现二分的思想。 123456789101112131415161718class Solution: def findMin(self, nums: List[int]) -&gt; int: if not nums: return -1 if len(nums) == 1: return nums[0] left, right = 0, len(nums) - 1 mid = left # 左边的元素比右边的大 while nums[left] &gt;= nums[right]: # 最小元素作为边界,即为右边区间的第一个元素 if right - left == 1: mid = right break mid = left + (right - left) //2 if nums[mid] &gt;= nums[left]: left = mid else: right = mid return nums[mid] 复杂度分析时间复杂度:二分法的时间复杂度为O(logn)空间复杂度:O(1), 没有用到额外的数据结构 相似题目33. 搜索旋转排序数组 154. 寻找旋转排序数组中的最小值 II","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://liangweijiang.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"数组","slug":"数组","permalink":"https://liangweijiang.github.io/tags/%E6%95%B0%E7%BB%84/"},{"name":"二分查找","slug":"二分查找","permalink":"https://liangweijiang.github.io/tags/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/"}]},{"title":"《趣谈网络协议》学习笔记之--通信协议综述","slug":"NetworkProtocol-01","date":"2020-03-10T08:56:43.896Z","updated":"2020-04-08T06:17:22.222Z","comments":true,"path":"2020/03/10/NetworkProtocol-01/","link":"","permalink":"https://liangweijiang.github.io/2020/03/10/NetworkProtocol-01/","excerpt":"","text":"学习自极客时间《趣谈网络协议》 作者：刘超 1. 为什么要学习网络协议?1.1 协议的三要素计算机语言作为程序员控制一台计算机工作的协议，具备了协议的三要素。 语法，就是这一段内容要符合一定的规则和格式。例如，括号要成对，结束要使用分号等。 语义，就是这一段内容要代表某种意义。例如数字减去数字是有意义的，数字减去文本一般来说就没有意义。 顺序，就是先干啥，后干啥。例如，可以先加上某个数值，然后再减去某个数值。会了计算机语言，你就能够教给一台计算机完成你的工作了。但是，要想打造互联网世界的通天塔，只教给一台机器做什么是不够的，你需要学会教给一大片机器做什么。这就需要网络协议。只有通过网络协议，才能使一大片机器互相协作、共同完成一件事。 1.2 HTTP协议的格式 HTTP/1.1 200 OKDate: Tue, 27 Mar 2018 16:50:26 GMTContent-Type: text/html;charset=UTF-8Content-Language: zh-CN 首先，符合语法，也就是说，只有按照上面那个格式来，浏览器才认。 例如，上来是状态，然后是首部，然后是内容。第二，符合语义，就是要按照约定的意思来。 例如，状态 200，表述的意思是网页成功返回。如果不成功，就是我们常见的“404”。 第三，符合顺序，你一点浏览器，就是发送出一个 HTTP 请求，然后才有上面那一串 HTTP 返回的东西。浏览器显然按照协议商定好的做了，最后一个五彩缤纷的页面就出现在你面前了 1.3 网络协议的种类 1.4 问题思考(1) 当网络包到达一个网关的时候，可以通过路由表得到下一个网关的 IP 地址，直接通过 IP 地址找就可以了，为什么还要通过本地的 MAC 地址呢？ 报文端到端的传输过程中，目的地址和源地址是不变的，但是每通过一个网关，源mac和目的mac一直在变。两个局域网之间的网关要通过mac互相传递报文。 (2) 1.MAC 地址可以修改吗？MAC（Media Access Control，介质访问控制）地址，也叫硬件地址，长度是 48 比特（6 字节），由 16 进制的数字组成，分为前 24 位和后 24 位。 前 24 位叫作组织唯一标志符（Organizationally Unique Identifier，OUI），是由 IEEE 的注册管理机构给不同厂家分配的代码，用于区分不同的厂家。 后 24 位是厂家自己分配的，称为扩展标识符。同一个厂家生产的网卡中 MAC 地址后 24 位是不同的。也就是说，MAC 本来设计为唯一性的，但是后来设备越来越多，而且还有虚拟化的设备和网卡，有很多工具可以修改，就很难保证不冲突了。但是至少应该保持一个局域网内是唯一的。 MAC 的设计，使得即便不能保证绝对唯一，但是能保证一个局域网内出现冲突的概率很小。这样，一台机器启动的时候，就能够在没有 IP 地址的情况下，先用 MAC 地址进行通信，获得 IP 地址。 MAC 地址是工作在一个局域网中的，因而即便出现了冲突，网络工程师也能够在自己的范围内很快定位并解决这个问题。这就像我们生成 UUID 或者哈希值，大部分情况下是不会冲突的，但是如果碰巧出现冲突了，采取一定的机制解决冲突就好。 2. 网络分层的真实含义是什么？2.1 网络为什么要分层？ 对于软件世界来说,在计算机领域，任何问题到了某个复杂的阶段，如果当前方法不能解决问题，必定可以通过加多一层来解决，如果加多一层都解决不了问题，那肯定是这个问题模型的层次已经到极限了。TCP 的实现将所有的逻辑耦合在了整个七层，不用 TCP 的可靠传输机制都没有办法。 1）各层之间相互独立：高层是不需要知道底层的功能是采取硬件技术来实现的，它只需要知道通过与底层的接口就可以获得所需要的服务；2）灵活性好：各层都可以采用最适当的技术来实现，例如某一层的实现技术发生了变化，用硬件代替了软件，只要这一层的功能与接口保持不变，实现技术的变化都并不会对其他各层以及整个系统的工作产生影响； 3）易于实现和标准化：由于采取了规范的层次结构去组织网络功能与协议，因此可以将计算机网络复杂的通信过程，划分为有序的连续动作与有序的交互过程，有利于将网络复杂的通信工作过程化解为一系列可以控制和实现的功能模块，使得复杂的计算机网络系统变得易于设计，实现和标准化 在网络协议中的分层。不仅仅是根据负责的功能来简单的划分层次，而且层与层之间会有不可缺少的的封装与传递。对于网络模型各层的封装是根据整个网络模型从上到下的工作流程来划分的。但是，每层之间会有一定的联系，不是独立工作的。 2.2 程序是如何工作的？ 2.3 层与层之间联系 只要是在网络上跑的包，都是完整的。可以有下层没上层，绝对不可能有上层没下层。 所以，对 TCP 协议来说，三次握手也好，重试也好，只要想发出去包，就要有 IP 层和 MAC 层，不然是发不出去的。 3. ifconfig：最熟悉又陌生的命令行 linux上查看ip地址的命令有ifconfig和ip addr两种 3.1 ifconfig 和 ip addr 的区别这是一个有关 net-tools 和 iproute2 的“历史”故事，net-tools起源于BSD的TCP/IP工具箱，后来成为老版本Linux内核中配置网络功能的工具，但自2001年起，Linux社区已经对其停止维护。同时，一些Linux发行版比如Arch Linux和CentOS/RHEL 7则已经完全抛弃了net-tools，只支持iproute2。 作为网络配置工具的一份子，iproute2是linux下管理控制TCP/IP网络和流量控制的新一代工具包，旨在替代老派的工具链net-tools，即大家比较熟悉的ifconfig，arp，route，netstat等命令。 net-tools通过procfs(/proc)和ioctl系统调用去访问和改变内核网络配置，而iproute2则通过netlink套接字接口与内核通讯。 抛开性能而言，net-tools的用法给人的感觉是比较乱，而iproute2的用户接口相对net-tools来说相对来说，更加直观。比如，各种网络资源（如link、IP地址、路由和隧道等）均使用合适的对象抽象去定义，使得用户可使用一致的语法去管理不同的对象，更重要的是，到目前为止，iproute2仍处在持续开发中，所以，net-tools和iproute2都需要去学习掌握了。 如果你仍在使用net-tools，而且尤其需要跟上新版Linux内核中的最新最重要的网络特性的话，那么是时候转到iproute2的阵营了。原因就在于使用iproute2可以做很多net-tools无法做到的事情。 3.2 ip地址的分类linux下输入ip addr 12345678910111213root@test:~# ip addr1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether fa:16:3e:c7:79:75 brd ff:ff:ff:ff:ff:ff inet 10.100.122.2/24 brd 10.100.122.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::f816:3eff:fec7:7975/64 scope link valid_lft forever preferred_lft forever 在 IP 地址的后面有个 scope，对于 eth0 这张网卡来讲，是 global，说明这张网卡是可以对外的，可以接收来自各个地方的包。对于 lo 来讲，是 host，说明这张网卡仅仅可以供本机相互通信。lo 全称是 loopback，又称环回接口，往往会被分配到 127.0.0.1 这个地址。这个地址用于本机通信，经过内核处理后直接返回，不会在任何网络中出现。 这个命令显示了这台机器上所有的网卡。大部分的网卡都会有一个 IP 地址，当然，这不是必须的。IP 地址是一个网卡在网络世界的通讯地址，相当于我们现实世界的门牌号码 以前人们根本想不到会有这么多的计算机,所以ip地址总共就32位,也就是我们所说的ipv4,可是ipv4很快就别分配完了,于是就出现了ipv6。 iPv4被分成了5类 这里面有个尴尬的事情，就是 C 类地址能包含的最大主机数量实在太少了，只有 254 个。当时设计的时候恐怕没想到，现在估计一个网吧都不够用吧。而 B 类地址能包含的最大主机数量又太多了。6 万多台机器放在一个网络下面，一般的企业基本达不到这个规模，闲着的地址就是浪费。 3.3 无类型域间选路（CIDR） 无类型域间选路(CIDR)基本思想是取消地址的分类结构，取而代之的是允许以可变长分界的方式分配网络数。它支持路由聚合，可限制Internet主干路由器中必要路由信息的增长。“无类型”的意思是选路决策是基于整个32位IP地址的掩码操作。而不管其IP地址是A类、B类或是C类。这样能够将路由表中的许多表项归并(summarization)成更少的数目。 伴随着 CIDR 存在的，一个是广播地址，10.100.122.255。如果发送这个地址，所有 10.100.122 网络里面的机器都可以收到。 另一个是子网掩码，255.255.255.0。将子网掩码和 IP 地址进行 AND 计算。前面三个 255，转成二进制都是 1。1 和任何数值取 AND，都是原来数值，因而前三个数不变，为 10.100.122。后面一个 0，转换成二进制是 0，0 和任何数值取 AND，都是 0，因而最后一个数变为 0，合起来就是 10.100.122.0。这就是网络号。将子网掩码和 IP 地址按位计算 AND，就可得到网络号。 3.4 公有 IP 地址和私有 IP 地址上图中可以看到私有地址的范围,私有地址就是平时我们看到的数据中心里，办公室、家里或学校的 IP 地址，一般都是私有 IP 地址段。因为这些地址允许组织内部的 IT 人员自己管理、自己分配，而且可以重复。因此，你学校的某个私有 IP 地址段和我学校的可以是一样的。就像你的小区有402号房间,我的小区也可以有402号房间,但是一旦出了小区,直接说402房间别人就找不到了。 公有 IP 地址有个组织统一分配，你需要去买。如果你搭建一个网站，给你学校的人使用，让你们学校的 IT 人员给你一个 IP 地址就行。但是假如你要做一个类似网易 163 这样的网站，就需要有公有 IP 地址，这样全世界的人才能访问。 3.5 MAC地址 MAC 地址更像是身份证，是一个唯一的标识。它的唯一性设计是为了组网的时候，不同的网卡放在一个网络里面的时候，可以不用担心冲突。从硬件角度，保证不同的网卡有不同的标识。但是光有mac地址是不够的的，就像光知道你的身份证是很难找得到你的人的，还需要你现在的居住地址（相当于ip地址） MAC地址只有很小范围的定位功能，就像大家在一个房间里，这时候大喊这个人的身份证，就会有对应的人回复你。 mac地址是唯一的，为什么可以修改?想想身份证，身份证号是唯一的，不能改变的，但是可以造价。mac地址全球唯一，它是固化在网卡里的。网卡毕竟是个硬件，需要软件支持，既操作系统识别。重点来了，操作系统识别出来的mac地址是可以更改的，它只不过是一个字符串。我们常说的修改mac指的是修改电脑中记录的既注册表中的记录。 有了mac地址为什么还要有ip地址。举个例子，身份证号是你的唯一标识，不会重复，一落户就有（网卡一出厂就有mac）。现在我要和你通信（写信给你），地址用你的姓名+身份证，信能送到你手上吗?明显不能！身份证号前六位能定位你出生的县。mac地址前几位也可以定位生产厂家。但是你出生后会离开这个县（哪怕在这个县，也不能具体找到你）。所以一般写个人信息就要有出生地和现居地址了 3.6 问题思考（1） 组播和广播的意义和原理是什么？ 广播和组播分为两个层面，其中 MAC 层有广播和组播对应的地址，IP 层也有自己的广播地址和组播地址。 广播相对比较简单，MAC 层的广播为 ff:ff:ff:ff:ff:ff，IP 层指向子网的广播地址为主机号为全 1 且有特定子网号的地址。 组播复杂一些，MAC 层中，当地址中最高字节的最低位设置为 1 时，表示该地址是一个组播地址，用十六进制可表示为 01:00:00:00:00:00。IP 层中，组播地址为 D 类 IP 地址，当 IP 地址为组播地址的时候，有一个算法可以计算出对应的 MAC 层地址。 多播进程将目的 IP 地址指明为多播地址，设备驱动程序将它转换为相应的以太网地址，然后把数据发送出去。这些接收进程必须通知它们的 IP 层，它们想接收的发给定多播地址的数据报，并且设备驱动程序必须能够接收这些多播帧。这个过程就是“加入一个多播组”。当多播跨越路由器的时候，需要通过 IGMP 协议告诉多播路由器，多播数据包应该如何转发。 4. DHCP与PXE：IP是怎么来的，又是怎么没的？4.1 配置ip地址使用 net-tools： $ sudo ifconfig eth1 10.0.0.1/24$ sudo ifconfig eth1 up 使用 iproute2： $ sudo ip addr add 10.0.0.1/24 dev eth1$ sudo ip link set up eth1 但是在手动配置ip地址时,不是自由的去配置的,要考虑很多因素,如两台电脑需要通信时,源IP 地址 16.158.23.6，目标 IP 地址 192.168.1.6,因为不是同一个网卡的话,Linux 默认的逻辑是，如果这是一个跨网段的调用，它便不会直接将包发送到网络上，而是企图将包发送到网关。需要经过多次网关才能达到,如果两台电脑是相邻的,如果直接配置成同一个网段的,直接发送ARP请求,节省很多资源. 4.2 动态主机配置协议（DHCP） 概要动态主机配置协议 (DHCP) 是由 RFC 1541（已被 RFC 2131 取代）定义的标准协议，允许服务器将 IP 地址和配置信息动态分发给客户端。正常情况下，DHCP 服务器至少会为客户端提供以下基本信息： IP 地址 子网掩码 默认网关 4.2.1 DHCP 的工作方式: 这里第二步,如果有多个DHCP服务器,新的客户端会收到多个ip地址,一般选择第一个到来的,客户端仍然使用 0.0.0.0 为源 IP 地址、255.255.255.255 为目标地址进行广播。在 BOOTP 里面，接受某个 DHCP Server 的分配的 IP。 4.2.2 IP 地址的收回和续客户机会在租期过去 50% 的时候，直接向为其提供 IP 地址的 DHCP Server 发送 DHCP request 消息包。客户机接收到该服务器回应的 DHCP ACK 消息包，会根据包中所提供的新的租期以及其他已经更新的 TCP/IP 参数，更新自己的配置。这样，IP 租用更新就完成了。 4.2.3 PXE 的工作过程网络管理员不仅能自动分配 IP 地址，还能帮你自动安装操作系统！我们安装操作系统的过程，只能插在 BIOS 启动之后了。因为没安装系统之前，连启动扇区都没有。因而这个过程叫做预启动执行环境（Pre-boot Execution Environment），简称 PXE。PXE 协议分为客户端和服务器端，由于还没有操作系统，只能先把客户端放在 BIOS 里面。当计算机启动时，BIOS 把 PXE 客户端调入内存里面，就可以连接到服务端做一些操作了。 4.2.4 DCHP总结 DHCP 协议主要是用来给客户租用 IP 地址，和房产中介很像，要商谈、签约、续租，广播还不能“抢单” DHCP 协议能给客户推荐“装修队”PXE，能够安装操作系统，这个在云计算领域大有用处 4.3 问题思考（1）PXE 协议可以用来安装操作系统，但是如果每次重启都安装操作系统，就会很麻烦。你知道如何使得第一次安装操作系统，后面就正常启动吗？ 现在一般电脑的网卡几乎都支持PXE启动， PXE client 就在网卡的 ROM 中，当计算机引导时，BIOS 把 PXE client 调入内存执行。安装完成后，将提示重新引导计算机。这个时候，在重新引导的过程中将BIOS修改回从硬盘启动就可以了。 服务器一般都提供 IPMI 接口，可以通过这个接口启动、重启、设置启动模式等等远程访问，这样就可以批量管理一大批机器。 （2）在 DHCP 网络里面，手动配置 IP 地址会冲突吗? 在一个 DHCP 网络里面，如果某一台机器手动配置了一个 IP 地址，并且在 DHCP 管理的网段里的话，DHCP 服务器是会将这个地址分配给其他机器的。一旦分配了，ARP 的时候，就会收到两个应答，IP 地址就冲突了 当发生这种情况的时候，应该怎么办呢？DHCP 的过程虽然没有明确如何处理，但是 DHCP 的客户端和服务器都可以添加相应的机制来检测冲突。 如果由客户端来检测冲突，一般情况是，客户端在接受分配的 IP 之前，先发送一个 ARP，看是否有应答，有就说明冲突了，于是发送一个 DHCPDECLINE，放弃这个 IP 地址。 如果由服务器来检测冲突，DHCP 服务器会发送 ping，来看某个 IP 是否已经被使用。如果被使用了，它就不再将这个 IP 分配给其他的客户端了。 （3）DHCP 的 Offer 和 ACK 应该是单播还是广播呢？正常情况下，一旦有了 IP 地址，DHCP Server 还是希望通过单播的方式发送 OFFER 和 ACK。但是不幸的是，有的客户端协议栈的实现，如果还没有配置 IP 地址，就使用单播。协议栈是不接收这个包的，因为 OFFER 和 ACK 的时候，IP 地址还没有配置到网卡上。所以，一切取决于客户端的协议栈的能力。 如果没配置好 IP，就不能接收单播的包，那就将 BROADCAST 设为 1，以广播的形式进行交互。 如果客户端的协议栈实现很厉害，即便是没有配置好 IP，仍然能够接受单播的包，那就将 BROADCAST 位设置为 0，就以单播的形式交互。 （4）DHCP 如何解决内网安全问题?其实 DHCP 协议的设计是基于内网互信的基础来设计的，而且是基于 UDP 协议。但是这里面的确是有风险的。例如一个普通用户无意地或者恶意地安装一台 DHCP 服务器，发放一些错误或者冲突的配置；再如，有恶意的用户发出很多的 DHCP 请求，让 DHCP 服务器给他分配大量的 IP。 对于第一种情况，DHCP 服务器和二层网络都是由网管管理的，可以在交换机配置只有来自某个 DHCP 服务器的包才是可信的，其他全部丢弃。如果有 SDN，或者在云中，非法的 DHCP 包根本就拦截到虚拟机或者物理机的出口。 对于第二种情况，一方面进行监控，对 DHCP 报文进行限速，并且异常的端口可以关闭，一方面还是 SDN 或者在云中，除了被 SDN 管控端登记过的 IP 和 MAC 地址，其他的地址是不允许出现在虚拟机和物理机出口的，也就无法模拟大量的客户端。 参考文章动态主机配置协议DHCP 关于ifconfig与ip addr 学习自极客时间 《趣谈网络协议》","categories":[{"name":"随笔","slug":"随笔","permalink":"https://liangweijiang.github.io/categories/%E9%9A%8F%E7%AC%94/"}],"tags":[{"name":"网络协议","slug":"网络协议","permalink":"https://liangweijiang.github.io/tags/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/"}]},{"title":"每日一道算法之--数组中的重复数字","slug":"algorithm-Interview-03","date":"2020-03-09T14:39:07.977Z","updated":"2020-03-09T14:40:55.208Z","comments":true,"path":"2020/03/09/algorithm-Interview-03/","link":"","permalink":"https://liangweijiang.github.io/2020/03/09/algorithm-Interview-03/","excerpt":"","text":"数组中重复的数字力扣面试题03 : https://leetcode-cn.com/problems/shu-zu-zhong-zhong-fu-de-shu-zi-lcof/ 找出数组中重复的数字。 在一个长度为 n 的数组 nums 里的所有数字都在 0～n-1 的范围内。数组中某些数字是重复的，但不知道有几个数字重复了，也不知道每个数字重复了几次。请找出数组中任意一个重复的数字。 示例 1： 输入：[2, 3, 1, 0, 2, 5, 3]输出：2 或 3 限制： 2 &lt;= n &lt;= 100000 这是剑指offer里面的原题,也是比较简单的一道题,但是,一开始我能想到的就是通过排序然后遍历就能够找到重复的数字,时间复杂度为O(logn),或者通过哈希表建立索引,从而使时间复杂度变为O(n),但是要牺牲额外的空间.然后作者给我们提供了一种很好的方法,我暂且叫做下标定位法.看看是怎么实现的吧 1. 排序实现直接看代码: 123456789class Solution: def findRepeatNumber(self, nums: List[int]) -&gt; int: if not nums: return -1 if len(nums) == 1: return nums.sort() for i in range(1, len(nums)): if nums[i] == nums[i - 1]: return nums[i] return -1 2. 利用哈希表实现12345678910class Solution: def findRepeatNumber(self, nums: List[int]) -&gt; int: if not nums: return -1 if len(nums) == 1: return -1 has_map = &#123;&#125; for num in nums: if num in has_map: return num has_map[num] = 1 return -1 3. 下标定位实现 阅读题目可以发现,所有数字都在 0～n-1 的范围内(哈哈我感觉其实救赎出题人有意而为), 这就可以假设如果数组中没有重复的数字, 那么排好序之后,数组每一个下标对应的数,其实就是刚好等于下标的,看到这句话,卧槽,牛逼牛逼!!如果有重复的数,则有些下标可能存在多个数,有的下标可能没有数。 现在可以重新对这个数组排序,从下标为i开始,这个数字为m,如果m不为i,则将a[i]与a[m]比较,如果不相等,则交换;如果相等,则证明找到了重复的数字。这样,下标为m的数它的值也为m了,相当于每个数最多两次交换就能找到它对应的下标. 12345678class Solution: def findRepeatNumber(self, nums: List[int]) -&gt; int: for i in range(len(nums)): while nums[i] != i: if nums[i] == nums[nums[i]]: return nums[i] nums[nums[i]], nums[i] = nums[i], nums[nums[i]] return -1 3.1 复杂度分析时间复杂度:虽然代码中有两层循环,但是每个数最多两次交换就能找到它对应的下标,所以时间复杂度还是为O(n) 空间复杂度:没有用到额外的数据结构,所以空间复杂度为O(1)","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://liangweijiang.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"数组","slug":"数组","permalink":"https://liangweijiang.github.io/tags/%E6%95%B0%E7%BB%84/"}]},{"title":"学习笔记之---负载均衡","slug":"LoadBalancing","date":"2020-03-09T07:16:29.244Z","updated":"2020-04-08T06:12:45.271Z","comments":true,"path":"2020/03/09/LoadBalancing/","link":"","permalink":"https://liangweijiang.github.io/2020/03/09/LoadBalancing/","excerpt":"","text":"负载均衡1 什么是负载均衡?百度百科是这样说的: 负载均衡建立在现有网络结构之上，它提供了一种廉价有效透明的方法扩展网络设备和服务器的带宽、增加吞吐量、加强网络数据处理能力、提高网络的灵活性和可用性。负载均衡（Load Balance）其意思就是分摊到多个操作单元上进行执行，例如Web服务器、FTP服务器、企业关键应用服务器和其它关键任务服务器等，从而共同完成工作任务。 互联网早期，业务流量比较小并且业务逻辑比较简单，单台服务器便可以满足基本的需求；但随着互联网的发展，业务流量越来越大并且业务逻辑也越来越复杂，单台机器的性能问题以及单点问题凸显了出来，因此需要多台机器来进行性能的水平扩展以及避免单点故障。但是要如何将不同的用户的流量分发到不同的服务器上面呢？ 这就是负载均衡所要解决的问题。回到上边请求页面的过程，这个请求此时会被一台专门的服务器来处理，这台服务器其实就是个集群的老大，他负责把这个请求派给下面哪个小弟（服务器）来处理，处理完之后将数据返回给用户。当有多个请求同时发生时，集群的老大可以将请求派给不同的小弟，这样处理的效率就会大幅提升，充分发挥集群的力量，至于哪个请求到底派给哪个小弟，这就是调度策略的问题了。 我的理解就是有一个老大哥给每一小弟安排工作,而且工作不能偏心,要能使每一个小弟都能很好的工作,这样他们的效率就会更高,不会轻易累死小弟… 1.1 负载均衡的特点负载均衡技术具有一下优势： 高性能：负载均衡技术将业务较均衡的分担到多台设备或链路上，从而提高了整个系统的性能； 可扩展性：负载均衡技术可以方便的增加集群中设备或链路的数量，在不降低业务质量的前提下满足不断增长的业务需求； 高可靠性：单个甚至多个设备或链路法神故障也不会导致业务中断，提高了整个系统的可靠性； 可管理性：大量的管理共组都集中在使用负载均衡技术的设备上，设备集群或链路集群只需要维护通过的配置即可； 透明性：对用户而言，集群等于一个或多个高可靠性、高性能的设备或链路，用户感知不到，也不关心具体的网络结构，增加或减少设备或链路数量都不会影响正常的业务。 1.2 负载均衡的分类负载均衡技术分类： 服务器负载均衡：在数据中心等组网环境中，可以采用服务器负载均衡，将网络服务分担给多台服务器进行处理，提高数据中心的业务处理能力； 链路负载均衡：在有多个运营商出接口的组网环境中，可以采用出方向多链路动态负载均衡，实现链路的动态选择，提高服务的可靠性； 防火墙负载均衡：在防火墙处理能力成为瓶颈的组网环境中，可以采用防火墙负载均衡，将网络流量分担给多台防火墙设备，提高防火桥的处理能力； 2. 负载均衡的算法 随机算法Random随机，按权重设置随机概率。在一个截面上碰撞的概率高，但调用量越大分布越均匀，而且按概率使用权重后也比较均匀，有利于动态调整提供者权重。 轮询及加权轮询轮询(Round Robbin)当服务器群中各服务器的处理能力相同时，且每笔业务处理量差异不大时，最适合使用这种算法。 轮循，按公约后的权重设置轮循比率。存在慢的提供者累积请求问题，比如：第二台机器很慢，但没挂，当请求调到第二台时就卡在那，久而久之，所有请求都卡在调到第二台上。加权轮询(Weighted Round Robbin)为轮询中的每台服务器附加一定权重的算法。比如服务器1权重1，服务器2权重2，服务器3权重3，则顺序为1-2-2-3-3-3-1-2-2-3-3-3- …… 最小连接及加权最小连接最少连接(Least Connections)在多个服务器中，与处理连接数(会话数)最少的服务器进行通信的算法。即使在每台服务器处理能力各不相同，每笔业务处理量也不相同的情况下，也能够在一定程度上降低服务器的负载。加权最少连接(Weighted Least Connection)为最少连接算法中的每台服务器附加权重的算法，该算法事先为每台服务器分配处理连接的数量，并将客户端请求转至连接数最少的服务器上。 哈希算法普通哈希一致性哈希一致性Hash，相同参数的请求总是发到同一提供者。当某一台提供者挂时，原本发往该提供者的请求，基于虚拟节点，平摊到其它提供者，不会引起剧烈变动。 IP地址散列通过管理发送方IP和目的地IP地址的散列，将来自同一发送方的分组(或发送至同一目的地的分组)统一转发到相同服务器的算法。当客户端有一系列业务需要处理而必须和一个服务器反复通信时，该算法能够以流(会话)为单位，保证来自相同客户端的通信能够一直在同一服务器中进行处理。 URL散列通过管理客户端请求URL信息的散列，将发送至相同URL的请求转发至同一服务器的算法。 3. 负载均衡的实现3.1 HTTP重定向实现负载均衡 HTTP重定向服务器就是一个普通的服务器，当用户访问时，其会根据一定的算法得到服务器集群的一个真实服务器的IP地址，将其放在HTTP响应头中，响应状态码为（302），当用户浏览器接收到这个响应时，会将得到的真实服务器的IP地址提出并重新访问。当浏览器收到响应消息后，解析Location字段，并向该URL发起请求，然后指定的服务器处理该用户的请求，最后将结果返回给用户。 在使用HTTP重定向来实现服务器集群负载均衡的过程中，需要一台服务器作为请求调度者。用户的一项操作需要发起两次HTTP请求，一次向调度服务器发送请求，获取后端服务器的IP，第二次向后端服务器发送请求，获取处理结果。 优点：采用HTTP重定向来实现服务器集群的负载均衡实现起来较为容易，逻辑比较简单。缺点: 这种方式需要用户浏览器访问两次，性能较差 HTTP重定向服务器会的处理能力会成为负载均衡的瓶颈由于不同用户的访问时间 HTTP重定向返回302，可能会使搜索引擎判定为SEO作弊，降低搜索排名,若分配给该用户的后端服务器出现故障，并且如果页面被浏览器缓存，那么当用户再次访问网站时，请求都会发给出现故障的服务器，从而导致访问失败 3.2 DNS负载均衡 当用户向我们的域名发起请求时，DNS服务器会自动地根据我们事先设定好的调度策略选一个合适的IP返回给用户，用户再向该IP发起请求 优点： 将负载均衡工作交给DNS，省略掉了网络管理的麻烦 DNS负载均衡最大的优点就是配置简单。服务器集群的调度工作完全由DNS服务器承担，那么我们就可以把精力放在后端服务器上，保证他们的稳定性与吞吐量。而且完全不用担心DNS服务器的性能，即便是使用了轮询策略，它的吞吐率依然卓越。 DNS负载均衡具有较强了扩展性，你完全可以为一个域名解析较多的IP，而且不用担心性能问题。 缺点： 由于把集群调度权交给了DNS服务器，从而我们没办法随心所欲地控制调度者，没办法定制调度策略。。 当我们发现某一台后端服务器发生故障时，即使我们立即将该服务器从域名解析中去除，但由于DNS服务器会有缓存，该IP仍然会在DNS中保留一段时间，那么就会导致一部分用户无法正常访问网站（可以用动态 DNS来解决）。事实上，大型网站总是部分使用DNS域名解析，作为第一级负载均衡手段，然后再在内部做第二级负载均衡。 3.3 数据链路层负载均衡(LVS)数据链路层负载均衡是指在通信协议的数据链路层修改mac地址进行负载均衡。 这种数据传输方式又称作三角传输模式，负载均衡数据分发过程中不修改IP地址，只修改目的的mac地址，通过配置真实物理服务器集群所有机器虚拟IP和负载均衡服务器IP地址一样，从而达到负载均衡，这种负载均衡方式又称为直接路由方式（DR）. 用户请求到达负载均衡服务器后，负载均衡服务器将请求数据的目的mac地址修改为真是WEB服务器的mac地址，并不修改数据包目标IP地址，因此数据可以正常到达目标WEB服务器，该服务器在处理完数据后可以经过网管服务器而不是负载均衡服务器直接到达用户浏览器。 使用三角传输模式的链路层负载均衡是目前大型网站所使用的最广的一种负载均衡手段。在linux平台上最好的链路层负载均衡开源产品是LVS(linux virtual server)。 3.4 IP层负载均衡先看一下ip层ip包的结构 可以看到结构中有原地址和目标地址这两个部分,这是实现ip层负载均衡的关键,我们就是通过修改这两个地址来达到“转发”目的 用户请求数据包到达负载均衡服务器后，负载均衡服务器在操作系统内核进行获取网络数据包，根据负载均衡算法计算得到一台真实的WEB服务器地址，然后将数据包的IP地址修改为真实的WEB服务器地址，不需要通过用户进程处理。真实的WEB服务器处理完毕后，相应数据包回到负载均衡服务器，负载均衡服务器再将数据包源地址修改为自身的IP地址发送给用户浏览器。 这里的关键在于真实WEB服务器相应数据包如何返回给负载均衡服务器，一种是负载均衡服务器在修改目的IP地址的同时修改源地址，将数据包源地址改为自身的IP，即源地址转换（SNAT），另一种方案是将负载均衡服务器同时作为真实物理服务器的网关服务器，这样所有的数据都会到达负载均衡服务器。 优点: IP负载均衡在内核进程完成数据分发，较反向代理均衡有更好的处理性能。缺点: 由于所有请求响应的数据包都需要经过负载均衡服务器，因此负载均衡的网卡带宽成为系统的瓶颈。 3.5 反向代理负载均衡(nginx) 代理与反向代理:VPN服务就是我们常用的一种代理（正向代理），用户将请教交给代理服务器，代理服务器访问网站获取数据，之后代理服务器再将数据返还给用户。在这个过程中，应用服务器并不知道用户的存在。只知道代理浏览器的访问。反向代理是指在服务器端的代理，代理服务器接收用户的请求，再转发给真实服务器，之后再返回给代理服务器再给用户，在这个过程中，用户并不知道真实服务器的存在。 传统代理服务器位于浏览器一端，代理浏览器将HTTP请求发送到互联网上。而反向代理服务器则位于网站机房一侧，代理网站web服务器接收http请求。 反向代理的作用是保护网站安全，所有互联网的请求都必须经过代理服务器，相当于在web服务器和可能的网络攻击之间建立了一个屏障。 除此之外，代理服务器也可以配置缓存加速web请求。当用户第一次访问静态内容的时候，静态内存就被缓存在反向代理服务器上，这样当其他用户访问该静态内容时，就可以直接从反向代理服务器返回，加速web请求响应速度，减轻web服务器负载压力。 另外，反向代理服务器也可以实现负载均衡的功能。 反向代理服务器管理了一组服务器，当用户访问时，代理服务器根据负载均衡算法将请求转发到真实服务器，真实服务器也通过反向代理服务器返还数据。内部服务器不对外部提供服务，所以不需要外部IP，而反向代理服务器需要两个网卡，一个IP用于外部用户访问使用，另外一个用于内部使用 优点： 隐藏后端服务器。与HTTP重定向相比，反向代理能够隐藏后端服务器，所有浏览器都不会与后端服务器直接交互，从而能够确保调度者的控制权，提升集群的整体性能。 故障转移。与DNS负载均衡相比，反向代理能够更快速地移除故障结点。当监控程序发现某一后端服务器出现故障时，能够及时通知反向代理服务器，并立即将其删除。 合理分配任务 。HTTP重定向和DNS负载均衡都无法实现真正意义上的负载均衡，也就是调度服务器无法根据后端服务器的实际负载情况分配任务。但反向代理服务器支持手动设定每台后端服务器的权重。我们可以根据服务器的配置设置不同的权重，权重的不同会导致被调度者选中的概率的不同。 缺点： 调度者压力过大 。由于所有的请求都先由反向代理服务器处理，那么当请求量超过调度服务器的最大负载时，调度服务器的吞吐率降低会直接降低集群的整体性能。 制约扩展。当后端服务器也无法满足巨大的吞吐量时，就需要增加后端服务器的数量，可没办法无限量地增加，因为会受到调度服务器的最大吞吐量的制约。 4. 四层和七层负载均衡的区别 源自:负载均衡基础知识:https://www.cnblogs.com/danbing/p/7459224.html作者: 金钟路上小码工 4.1 技术原理上的区别。 所谓四层负载均衡，也就是主要通过报文中的目标地址和端口，再加上负载均衡设备设置的服务器选择方式，决定最终选择的内部服务器。 以常见的TCP为例，负载均衡设备在接收到第一个来自客户端的SYN 请求时，即通过上述方式选择一个最佳的服务器，并对报文中目标IP地址进行修改(改为后端服务器IP），直接转发给该服务器。TCP的连接建立，即三次握手是客户端和服务器直接建立的，负载均衡设备只是起到一个类似路由器的转发动作。在某些部署情况下，为保证服务器回包可以正确返回给负载均衡设备，在转发报文的同时可能还会对报文原来的源地址进行修改。 所谓七层负载均衡，也称为“内容交换”，也就是主要通过报文中的真正有意义的应用层内容，再加上负载均衡设备设置的服务器选择方式，决定最终选择的内部服务器。 以常见的TCP为例，负载均衡设备如果要根据真正的应用层内容再选择服务器，只能先代理最终的服务器和客户端建立连接(三次握手)后，才可能接受到客户端发送的真正应用层内容的报文，然后再根据该报文中的特定字段，再加上负载均衡设备设置的服务器选择方式，决定最终选择的内部服务器。 负载均衡设备在这种情况下，更类似于一个代理服务器。负载均衡和前端的客户端以及后端的服务器会分别建立TCP连接。所以从这个技术原理上来看，七层负载均衡明显的对负载均衡设备的要求更高，处理七层的能力也必然会低于四层模式的部署方式。 4.2 应用场景的需求。 七层应用负载的好处，是使得整个网络更”智能化”, 例如访问一个网站的用户流量，可以通过七层的方式，将对图片类的请求转发到特定的图片服务器并可以使用缓存技术；将对文字类的请求可以转发到特定的文字服务器并可以使用压缩技术。 当然这只是七层应用的一个小案例，从技术原理上，这种方式可以对客户端的请求和服务器的响应进行任意意义上的修改，极大的提升了应用系统在网络层的灵活性。很多在后台，(例如Nginx或者Apache)上部署的功能可以前移到负载均衡设备上，例如客户请求中的Header重写，服务器响应中的关键字过滤或者内容插入等功能。 另外一个常常被提到功能就是安全性。网络中最常见的SYN Flood攻击，即黑客控制众多源客户端，使用虚假IP地址对同一目标发送SYN攻击，通常这种攻击会大量发送SYN报文，耗尽服务器上的相关资源，以达到Denial of Service(DoS)的目的。 从技术原理上也可以看出，四层模式下这些SYN攻击都会被转发到后端的服务器上；而七层模式下这些SYN攻击自然在负载均衡设备上就截止，不会影响后台服务器的正常运营。另外负载均衡设备可以在七层层面设定多种策略，过滤特定报文，例如SQL Injection等应用层面的特定攻击手段，从应用层面进一步提高系统整体安全。 现在的7层负载均衡，主要还是着重于应用广泛的HTTP协议，所以其应用范围主要是众多的网站或者内部信息平台等基于B/S开发的系统。 4层负载均衡则对应其他TCP应用，例如基于C/S开发的ERP等系统。 参考文章几种常见的负载均衡 负载均衡基础知识 关于负载均衡的详细介绍（通俗易懂） 负载均衡","categories":[{"name":"随笔","slug":"随笔","permalink":"https://liangweijiang.github.io/categories/%E9%9A%8F%E7%AC%94/"}],"tags":[{"name":"负载均衡","slug":"负载均衡","permalink":"https://liangweijiang.github.io/tags/%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/"},{"name":"计算机网络","slug":"计算机网络","permalink":"https://liangweijiang.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}]},{"title":"每日一道算法之--组合总和","slug":"algorithm-39","date":"2020-03-05T12:55:27.140Z","updated":"2020-03-05T12:56:39.571Z","comments":true,"path":"2020/03/05/algorithm-39/","link":"","permalink":"https://liangweijiang.github.io/2020/03/05/algorithm-39/","excerpt":"","text":"组合总和力扣第39题:https://leetcode-cn.com/problems/combination-sum/参考文章回溯算法 + 剪枝 极客时间-数据结构与算法之美 给定一个无重复元素的数组 candidates 和一个目标数 target ，找出 candidates 中所有可以使数字和为 target 的组合。 candidates 中的数字可以无限制重复被选取。 说明： 所有数字（包括 target）都是正整数。 解集不能包含重复的组合。示例 1: 输入: candidates = [2,3,6,7], target = 7,所求解集为:[[7],[2,2,3]] 示例 2: 输入: candidates = [2,3,5], target = 8,所求解集为:[ [2,2,2,2], [2,3,3], [3,5]] 回溯思想当我第一眼看到这个题的时候,我第一时间想到就是DP,但是仔细阅读题目的时候就可以发现,这道题是求所有可能的总和,而动态规划只能解决最有问题,所以以后遇到求总和问题的时候,就应该先想到回溯的思想去解决问题. 回溯的处理思想，有点类似枚举搜索。我们枚举所有的解，找到满足期望的解。为了有规律地枚举所有可能的解，避免遗漏和重复，我们把问题求解的过程分为多个阶段。每个阶段，我们都会面对一个岔路口，我们先随意选一条路走，当发现这条路走不通的时候（不符合期望的解），就回退到上一个岔路口，另选一种走法继续走。 相信看过明日边缘和蝴蝶效应这些电影的人应该很容易去理解回溯,下面就来用回溯思想去分析一下这道题. 可以看到,以蓝色点递归可以找到所有的组合 但是有一些节点是重复的,因为递归时没有加限制条件,更深一层的递归又会重复去考虑之前已经计算过的组合,所以需要去排序. 排好序之后,当候选数组的元素比目标数还大时,该元素后面的数可以都不用去考虑了. 123456789101112131415161718class Solution: def combinationSum(self, candidates: List[int], target: int) -&gt; List[List[int]]: candidates.sort() n = len(candidates) res = [] def helper(start, cur_num, tmp): if cur_num == 0: res.append(tmp) return # 这里是去重, 排序之后,start前面的元素都不用去考虑了 for i in range(start, n): # 剪枝,该元素比目标数还大,则直接跳出 # 这里是自下往上组合,上图中是自上往下组合,原理都一样 if cur_num - candidates[i] &lt; 0: break helper(i, cur_num - candidates[i], tmp + [candidates[i]]) helper(0, target, []) return res 复杂度分析时间复杂度:每一次递归遍历考虑所有可能的组合,所以时间复杂度为$$O(n^2)$$ 空间复杂度:O(n) 总结回溯思想可能是一个时间复杂度较高的算法,很多时候可以用dp去优化,但是在枚举所有可能的结果的时候,就是发挥它的特点的时候了,不要因为复杂度高就不去考虑,所以还是的脚踏实地去学习编程吧!!! 相似题目 组合总和II 组合总和III 全排列 N皇后","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://liangweijiang.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"数组","slug":"数组","permalink":"https://liangweijiang.github.io/tags/%E6%95%B0%E7%BB%84/"},{"name":"回溯思想","slug":"回溯思想","permalink":"https://liangweijiang.github.io/tags/%E5%9B%9E%E6%BA%AF%E6%80%9D%E6%83%B3/"}]},{"title":"python单例模式的实现","slug":"python-02","date":"2020-03-04T08:13:02.751Z","updated":"2020-03-04T08:18:12.492Z","comments":true,"path":"2020/03/04/python-02/","link":"","permalink":"https://liangweijiang.github.io/2020/03/04/python-02/","excerpt":"","text":"单例模式什么是单例模式 单例模式（Singleton Pattern）是一种常用的软件设计模式，该模式的主要目的是确保某一个类只有一个实例存在。当你希望在整个系统中，某个类只能出现一个实例时，单例对象就能派上用场。 比如，某个服务器程序的配置信息存放在一个文件中，客户端通过一个 AppConfig 的类来读取配置文件的信息。如果在程序运行期间，有很多地方都需要使用配置文件的内容，也就是说，很多地方都需要创建 AppConfig 对象的实例，这就导致系统中存在多个 AppConfig 的实例对象，而这样会严重浪费内存资源，尤其是在配置文件内容很多的情况下。事实上，类似 AppConfig 这样的类，我们希望在程序运行期间只存在一个实例对象。 单例模式的实现python实现单例模式有多种实现方法，下面一一来解释说明 1. 基于装饰器的单例模式的实现12345678910111213141516171819202122232425def singleton(cls): # 创建一个字典用来保存类的实例对象 _instance = &#123;&#125; def _singleton(*args, **kwargs): # 先判断这个类有没有对象 if cls not in _instance: _instance[cls] = cls(*args, **kwargs) # 创建一个对象,并保存到字典当中 # 将实例对象返回 return _instance[cls] return _singleton@singletonclass A(object): a = 1 def __init__(self, x=0): self.x = x print('这是A的类的初始化方法')a1 = A(2)a2 = A(3)print(a1.x, a2.x)print(id(a1), id(a2)) 可以看到打印结果为: 2. 基于类方法的单例模式的实现123456789101112131415161718192021class A: def __init__(self, x, y): self.x = x self.y = y @classmethod def singleton(cls, *args, **kwargs): if not hasattr(cls, '__instance'): cls.__instance = cls(*args, **kwargs) return cls.__instanceobj1 = A(1, 2)obj2 = A(3, 4)print(obj1.x, obj1.y, obj2.x, obj2.y)print(id(obj1), id(obj2))obj3 = A.singleton(1, 2)obj4 = A.singleton(3, 4)print(obj3.x, obj3.y, obj4.x, obj4.y)print(id(obj3), id(obj4) 打印结果如下: 但是这个实现方法在多线程上会出现堵塞现象,所以需要加锁1234567891011121314151617181920212223242526import threadingclass A: _instance_lock = threading.Lock() def __init__(self, x, y): self.x = x self.y = y @classmethod def singleton(cls, *args, **kwargs): if not hasattr(cls, '__instance'): with cls._instance_lock: cls.__instance = cls(*args, **kwargs) return cls.__instanceobj1 = A(1, 2)obj2 = A(3, 4)print(obj1.x, obj1.y, obj2.x, obj2.y)print(id(obj1), id(obj2))obj3 = A.singleton(1, 2)obj4 = A.singleton(3, 4)print(obj3.x, obj3.y, obj4.x, obj4.y)print(id(obj3), id(obj4)) 可以发现,使用类方法创建单例模式时,必须要调用该方法,否则直接调用类得到的并不是单例 3. 基于new方法实现单例模式(推荐) 一个对象的实例化过程是先执行类的new方法,如果我们没有写,默认会调用object的new方法,返回一个实例化对象,然后再调用init方法,对这个对象进行初始化,我们可以根据这个实现单例. 在一个类的new方法中先判断是不是存在实例,如果存在实例,就直接返回,如果不存在实例就创建. 1234567891011121314151617import threadingclass A: _instance_lock = threading.Lock() def __new__(cls, *args, **kwargs): if not hasattr(cls, '__instance'): with cls._instance_lock: if not hasattr(cls, '__instance'): cls.__instance = super().__new__(cls) return cls.__instanceobj1 = A()obj2 = A()print(obj1, obj2)print(id(obj1), id(obj2)) 45.基于metaclass方式实现 类由type创建，创建类时，type的init方法自动执行，类() 执行type的 call方法(类的new方法,类的init方法) 对象由类创建，创建对象时，类的init方法自动执行，对象()执行类的 call 方法 1234567891011class Foo: def __init__(self): pass def __call__(self, *args, **kwargs): passobj = Foo()# 执行type的 __call__ 方法，调用 Foo类（是type的对象）的 __new__方法，用于创建对象，然后调用 Foo类（是type的对象）的 __init__方法，用于对对象初始化。obj() # 执行Foo的 __call__ 方法 元类的使用 1234567891011121314151617181920class SingletonType(type): #只有继承了type类才能称之为一个元类，否则就是一个普通的自定义类 def __init__(self,*args,**kwargs): super(SingletonType,self).__init__(*args,**kwargs) def __call__(cls, *args, **kwargs): # 这里的cls，即Foo类 print('cls',cls) # __new__()创建对象 obj = cls.__new__(cls,*args, **kwargs) # __init__()实例化对象 cls.__init__(obj,*args, **kwargs) # Foo.__init__(obj) return objclass Foo(metaclass=SingletonType): # 指定创建Foo的type为SingletonType def __init__(self，name): self.name = name def __new__(cls, *args, **kwargs): return object.__new__(cls)obj = Foo('xx') 实现单例模式 1234567891011121314151617181920import threadingclass SingletonType(type): _instance_lock = threading.Lock() def __call__(cls, *args, **kwargs): if not hasattr(cls, \"_instance\"): with SingletonType._instance_lock: if not hasattr(cls, \"_instance\"): cls._instance = super(SingletonType,cls).__call__(*args, **kwargs) return cls._instanceclass Foo(metaclass=SingletonType): def __init__(self,name): self.name = nameobj1 = Foo('name')obj2 = Foo('name')print(obj1,obj2)","categories":[{"name":"python","slug":"python","permalink":"https://liangweijiang.github.io/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"https://liangweijiang.github.io/tags/python/"},{"name":"设计模式","slug":"设计模式","permalink":"https://liangweijiang.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"python与其他语言的对比","slug":"python-01","date":"2020-03-04T05:12:40.670Z","updated":"2020-03-08T15:10:45.420Z","comments":true,"path":"2020/03/04/python-01/","link":"","permalink":"https://liangweijiang.github.io/2020/03/04/python-01/","excerpt":"","text":"python与其他语言的对比1.C语言 它既有高级语言的特点，又具有汇编语言的特点，它是结构式语言。C语言应用指针：可以直接进行靠近硬件的操作，但是C的指针操作不做保护，也给它带来了很多不安全的因素。C++在这方面做了改进，在保留了指针操作的同时又增强了安全性，受到了一些用户的支持，但是，由于这些改进增加语言的复杂度，也为另一部分所诟病。Java则吸取了C++的教训，取消了指针操作，也取消了C++改进中一些备受争议的地方，在安全性和适合性方面均取得良好的效果，但其本身解释在虚拟机中运行，运行效率低于C++/C。一般而言，C，C++，java被视为同一系的语言，它们长期占据着程序使用榜的前三名。 C语言的优点：简洁紧凑、灵活方便；运算符丰富；数据类型丰富；表达方式灵活实用；允许直接访问物理地址，对硬件进行操作；生成目标代码质量高，程序执行效率高；可移植性好；表达力强； C语言的缺点：C语言的缺点主要表现在数据的封装性上，这一点使得C在数据的安全性上有很大缺陷，这也是C和C++的一大区别。 C语言的语法限制不太严格，对变量的类型约束不严格，影响程序的安全性，对数组下标越界不作检查等。从应用的角度，C语言比其他高级语言较难掌握。也就是说，对用C语言的人，要求对程序设计更熟练一些。 2.c++语言 C++是C语言的继承，它既可以进行C语言的过程化程序设计，又可以进行以抽象数据类型为特点的基于对象的程序设计，还可以进行以继承和多态为特点的面向对象的程序设计。C++擅长面向对象程序设计的同时，还可以进行基于过程的程序设计，因而C++就适应的问题规模而论，大小由之。 C++不仅拥有计算机高效运行的实用性特征，同时还致力于提高大规模程序的编程质量与程序设计语言的问题描述能力。 C++语言的程序因为要体现高性能，所以都是编译型的。但其开发环境，为了方便测试，将调试环境做成解释型的。即开发过程中，以解释型的逐条语句执行方式来进行调试，以编译型的脱离开发环境而启动运行的方式来生成程序最终的执行代码。 生成程序是指将源码（C++语句）转换成一个可以运行的应用程序的过程。如果程序的编写是正确的，那么通常只需按一个功能键，即可搞定这个过程。该过程实际上分成两个步骤。 第一步是对程序进行编译，这需要用到编译器（compiler）。编译器将C++语句转换成机器码(也称为目标码)；如果这个步骤成功，下一步就是对程序进行链接，这需要用到链接器（linker）。链接器将编译获得机器码与C++库中的代码进行合并。C++库包含了执行某些常见任务的函数（“函数”是子程序的另一种称呼）。例如，一个C++库中包含标准的平方根函数sqrt，所以不必亲自计算平方根。C++库中还包含一些子程序，它们把数据发送到显示器，并知道如何读写硬盘上的数据文件。 3. C#语言 C#是微软公司发布的一种面向对象的、运行于.NET Framework之上的高级程序设计语言。C#看起来与Java有着惊人的相似；它包括了诸如单一继承、接口、与Java几乎同样的语法和编译成中间代码再运行的过程。但是C#与Java有着明显的不同，它借鉴了Delphi的一个特点，与COM（组件对象模型）是直接集成的，而且它是微软公司 .NET windows网络框架的主角。首先，C# 和JAVA一样，简直就是照搬了C++的部分语法，因此，对于数量众多的C++程序员学习起来很容易上手，另外，对于新手来说，比C++要简单一些。其次，Windows是占垄断地位的平台，而开发Windows应用，当然微软的声音是不能忽略的。最重要的是，相对于C++，用C# 开发应用软件可以大大缩短开发周期，同时可以利用原来除用户界面代码之外的C++代码。 4. Java语言 Java是一种可以撰写跨平台应用软件的面向对象的程序设计语言，是由Sun Microsystems公司于1995年5月推出的Java程序设计语言和Java平台（即JavaSE, JavaEE, JavaME）的总称。Java 技术具有卓越的通用性、高效性、平台移植性和安全性，广泛应用于个人PC、数据中心、游戏控制台、科学超级计算机、移动电话和互联网，同时拥有全球最大的开发者专业社群。在全球云计算和移动互联网的产业环境下，Java更具备了显著优势和广阔前景。 Java的优势，与传统程序不同，Sun 公司在推出 Java 之际就将其作为一种开放的技术。全球数以万计的 Java 开发公司被要求所设计的 Java软件必须相互兼容。“Java 语言靠群体的力量而非公司的力量”是Sun公司的口号之一，并获得了广大软件开发商的认同。这与微软公司所倡导的注重精英和封闭式的模式完全不同。Sun 公司对 Java 编程语言的解释是：Java 编程语言是个简单、面向对象、分布式、解释性、健壮、安全与系统无关、可移植、高性能、多线程和动态的语言。 5.php语言 PHP（PHP: Hypertext Preprocessor的缩写，中文名：“PHP：超文本预处理器”）是一种通用开源脚本语言。语法吸收了C语言、Java和Perl的特点，入门门槛较低，易于学习，使用广泛，主要适用于Web开发领域。 的执行动态网页——动态页面方面，与其他的编程语言相比，PHP是将程序嵌入到HTML文档中去执行，执行效率比完全生成htmL标记的CGI要高许多，PHP具有非常强大的功能，所有的CGI的功能PHP都能实现； PHP支持几乎所有流行的数据库以及操作系统；最重要的是PHP可以用C、C++进行程序的扩展。 6.python语言 python是一种面向对象、直译式计算机程序设计语言，Python语法简洁而清晰，具有丰富和强大的类库。它常被昵称为胶水语言，它能够很轻松的把用其他语言制作的各种模块（尤其是C/C++）轻松地联结在一起。常见的一种应用情形是，使用python快速生成程序的原型（有时甚至是程序的最终界面），然后对其中有特别要求的部分，用更合适的语言改写。 Python是完全面向对象的语言。函数、模块、数字、字符串都是对象。并且完全支持继承、重载、派生、多继承，有益于增强源代码的复用性。Python支持重载运算符和动态类型。相对于Lisp这种传统的函数式编程语言，Python对函数式设计只提供了有限的支持。有两个标准库(functools, itertools)提供了Haskell和Standard ML中久经考验的函数式程序设计工具。Python本身被设计为可扩充的。并非所有的特性和功能都集成到语言核心。Python提供了丰富的API和工具，以便程序员能够轻松地使用C语言、C++、Cython来编写扩充模块。Python编译器本身也可以被集成到其它需要脚本语言的程序内。因此，很多人还把Python作为一种“胶水语言”（glue language）使用。使用Python将其他语言编写的程序进行集成和封装。 编译型语言和解释型语言 编译性语言(需要编译器,相当于翻译) 只须编译一次就可以把源代码编译成机器语言，后面的执行无须重新编译，直接使用之前的编译结果就可 以；因此其执行的效率比较高； 编译性语言代表：C、C++、Pascal/Object Pascal（Delphi）； 程序执行效率比较高，但比较依赖编译器，调试麻烦, 因此跨平台性差一些； 不同平台对编译器影响较大。如：（1）16位系统下int是2个字节（16位），而32位系统下int占4个字节（32位）；（2）32位系统下long类型占4字节，而64位系统下long类型占8个字节； 解释型语言（需要解释器，相当于同声传译） 源代码不能直接翻译成机器语言，而是先翻译成中间代码，再由解释器对中间代码进行解释运行； 程序不需要编译，程序在运行时才翻译成机器语言，每执行一次都要翻译一次； 解释型跨平台好，一份代码，到处使用 解释性语言代表：Python、JavaScript、Shell、Ruby、MATLAB等； 运行效率一般相对比较低，依赖解释器，跨平台性好； python解释器的种类和特点CPythonCPython是使用最广且被的Python解释器。本教程以CPython为准。当我们从Python官方网站下载并安装好Python 2.7后，我们就直接获得了一个官方版本的解释器：CPython。这个解释器是用C语言开发的，所以叫CPython。在命令行下运行python就是启动CPython解释器。 IPythonIPython是基于CPython之上的一个交互式解释器，也就是说，IPython只是在交互方式上有所增强，但是执行Python代码的功能和CPython是完全一样的。好比很多国产浏览器虽然外观不同，但内核其实都是调用了IE。CPython用&gt;&gt;&gt;作为提示符，而IPython用In [序号]:作为提示符。 PyPyPyPy是另一个Python解释器，它的目标是执行速度。PyPy采用JIT技术，对Python代码进行动态编译（注意不是解释），所以可以显著提高Python代码的执行速度。 绝大部分Python代码都可以在PyPy下运行，但是PyPy和CPython有一些是不同的，这就导致相同的Python代码在两种解释器下执行可能会有不同的结果。如果你的代码要放到PyPy下执行，就需要了解PyPy和CPython的不同点。 JythonJython是运行在Java平台上的Python解释器，可以直接把Python代码编译成Java字节码执行。 IronPythonIronPython和Jython类似，只不过IronPython是运行在微软.Net平台上的Python解释器，可以直接把Python代码编译成.Net的字节码。","categories":[{"name":"python","slug":"python","permalink":"https://liangweijiang.github.io/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"https://liangweijiang.github.io/tags/python/"}]},{"title":"python的垃圾回收机制","slug":"python-GC","date":"2020-03-04T04:07:35.310Z","updated":"2020-03-05T04:11:27.598Z","comments":true,"path":"2020/03/04/python-GC/","link":"","permalink":"https://liangweijiang.github.io/2020/03/04/python-GC/","excerpt":"","text":"python的垃圾回收机制 垃圾回收机制是自动帮助我们管理内存，清理垃圾的一种工具1）、引用计数当一个对象的引用被创建或者复制时，对象的引用计数加1；当一个对象的引用被销毁时，对象的引用计数减1；当对象的引用计数减少为0时，就意味着对象已经没有被任何人使用了，可以将其所占用的内存释放了。2）、标记-清除标记-清除不改动真实的引用计数，而是将集合中对象的引用计数复制一份副本，改动该对象引用的副本。对于副本做任何的改动，都不会影响到对象生命走起的维护。3）、分代回收将系统中的所有内存块根据其存活时间划分为不同的集合，每一个集合就成为一个“代”，垃圾收集的频率随着“代”的存活时间的增大而减小。也就是说，活得越长的对象，就越不可能是垃圾，就应该减少对它的垃圾收集频率。那么如何来衡量这个存活时间：通常是利用几次垃圾收集动作来衡量，如果一个对象经过的垃圾收集次数越多，可以得出：该对象存活时间就越长。 1. 引用计数python一切皆对象,每一个对象都保存了一个称为引用计数的整数值，来追踪到底有多少引用指向了这个对象。无论何时，如果我们程序中的一个变量或其他对象引用了目标对象，Python将会增加这个计数值。 对象被创建 a=14 对象被引用 b=a 对象被作为参数,传到函数中 func(a) 对象作为一个元素，存储在容器中 List={a,”a”,”b”,2} 而当程序停止使用这个对象，则Python会减少这个计数值。一旦计数值被减到零，Python将会释放这个对象以及回收相关内存空间。 当该对象的别名被显式销毁时 del a 当该对象的引别名被赋予新的对象， a=26 一个对象离开它的作用域，例如 func函数执行完毕时，函数里面的局部变量的引用计数器就会减一（但是全局变量不会） 将该元素从容器中删除时，或者容器被销毁时。 当指向该对象的内存的引用计数器为0的时候，该内存将会被Python虚拟机销毁 1.1 引用计数的优点可以看出,引用计数有以下几个优点: 高效 运行期没有停顿 可以类比一下Ruby的垃圾回收机制，也就是 实时性：一旦没有引用，内存就直接释放了。不用像其他机制等到特定时机。实时性还带来一个好处：处理回收内存的时间分摊到了平时。 对象有确定的生命周期 易于实现,简单直观 但是针对引用计数这种算法来说，如果一个数据结构引用了它自身，即如果这个数据结构是一个循环数据结构，那么某些引用计数值是肯定无法变成零的。为了更好地理解这个问题，让我们举个例子。 123456a = &#123; &#125; #对象A的引用计数为 1b = &#123; &#125; #对象B的引用计数为 1a['b'] = b #B的引用计数增1b['a'] = a #A的引用计数增1del a #A的引用减 1，最后A对象的引用为 1del b #B的引用减 1, 最后B对象的引用为 1 在这个例子中程序执行完del语句后，A、B对象已经没有任何引用指向这两个对象，但是这两个对象各包含一个对方对象的引用，虽然最后两个对象都无法通过其它变量来引用这两个对象了，这对GC来说就是两个非活动对象或者说是垃圾对象，但是他们的引用计数并没有减少到零。因此如果是使用引用计数法来管理这两对象的话，他们并不会被回收，它会一直驻留在内存中，就会造成了内存泄漏（内存空间在使用完毕后未释放）。 1.2 引用计数的缺点虽然引用计数有着非常简单高效的优点,但是他也有这致命的缺点 维护引用计数消耗资源，维护引用计数的次数和引用赋值成正比，而不像mark and sweep等基本与回收的内存数量有关。 无法解决循环引用的问题 为了解决对象的循环引用问题，Python引入了标记-清除和分代回收两种GC机制。 2. 标记清除 『标记清除（Mark—Sweep）』算法是一种基于追踪回收（tracing GC）技术实现的垃圾回收算法。它分为两个阶段：第一阶段是标记阶段，GC会把所有的『活动对象』打上标记，第二阶段是把那些没有标记的对象『非活动对象』进行回收。那么GC又是如何判断哪些是活动对象哪些是非活动对象的呢？ 对象之间通过引用（指针）连在一起，构成一个有向图，对象构成这个有向图的节点，而引用关系构成这个有向图的边。从根对象（root object）出发，沿着有向边遍历对象，可达的（reachable）对象标记为活动对象，不可达的对象就是要被清除的非活动对象。根对象就是全局变量、调用栈、寄存器。 标记阶段，遍历所有的GC Roots对象(栈区中的所有内容或者线程都可以作为GC Roots对象），然后将所有 GC Roots的对象可以直接或间接访问到的对象标记为存活的对象，其余的均为非存活对象，应该被清除; 清除阶段，再次遍历对象，如果发现某个对象没有标记为可达，则就将其回收。 在上图中，我们把小黑圈视为全局变量，也就是把它作为root object，从小黑圈出发，对象1可直达，那么它将被标记，对象2、3可间接到达也会被标记，而4和5不可达，那么1、2、3就是活动对象，4和5是非活动对象会被GC回收。 标记清除算法作为Python的辅助垃圾收集技术主要处理的是一些容器对象，比如list、dict、tuple，instance等，因为对于字符串、数值对象是不可能造成循环引用问题。Python使用一个双向链表将这些容器对象组织起来。 不过，这种简单粗暴的标记清除算法也有明显的缺点：清除非活动的对象前它必须顺序扫描整个堆内存，哪怕只剩下小部分活动对象也要扫描所有对象，而且会暂停整个应用程序，等待标记清除结束后才会恢复应用程序的运行 3. 分代回收分代回收是一种以空间换时间的操作方式，Python将内存根据对象的存活时间划分为不同的集合，每个集合称为一个代，Python将内存分为了3“代”，分别为年轻代（第0代）、中年代（第1代）、老年代（第2代），他们对应的是3个链表，它们的垃圾收集频率与对象的存活时间的增大而减小。新创建的对象都会分配在年轻代，年轻代链表的总数达到上限时，Python垃圾收集机制就会被触发，把那些可以被回收的对象回收掉，而那些不会回收的对象就会被移到中年代去，依此类推，老年代中的对象是存活时间最久的对象，甚至是存活于整个系统的生命周期内。同时，分代回收是建立在标记清除技术基础之上。分代回收同样作为Python的辅助垃圾收集技术处理那些容器对象。 新定义的变量，放到新生代这个等级中，假设每隔1分钟扫描新生代一次，如果发现变量依然被引用，那么该对象的权重（权重本质就是个整数）加一，当变量的权重大于某个设定得值（假设为3），会将它移动到更高一级的青春代，青春代的gc扫描的频率低于新生代（扫描时间间隔更长），假设5分钟扫描青春代一次，这样每次gc需要扫描的变量的总个数就变少了，节省了扫描的总时间，接下来，青春代中的对象，也会以同样的方式被移动到老年代中。也就是等级（代）越高，被垃圾回收机制扫描的频率越低 分代回收是基于这样的一个统计事实，对于程序，存在一定比例的内存块的生存周期比较短；而剩下的内存块，生存周期会比较长，甚至会从程序开始一直持续到程序结束。生存期较短对象的比例通常在 80%～90% 之间，这种思想简单点说就是：对象存在时间越长，越可能不是垃圾，应该越少去收集。这样在执行标记-清除算法时可以有效减小遍历的对象数，从而提高垃圾回收的速度。 Python默认定义了三代对象集合，索引数越大，对象存活时间越长12345678910#define NUM_GENERATIONS 3#define GEN_HEAD(n) (&amp;generations[n].head)/* linked lists of container objects */static struct gc_generation generations[NUM_GENERATIONS] = &#123; /* PyGC_Head, threshold, count */ &#123;&#123;&#123;GEN_HEAD(0), GEN_HEAD(0), 0&#125;&#125;, 700, 0&#125;, &#123;&#123;&#123;GEN_HEAD(1), GEN_HEAD(1), 0&#125;&#125;, 10, 0&#125;, &#123;&#123;&#123;GEN_HEAD(2), GEN_HEAD(2), 0&#125;&#125;, 10, 0&#125;,&#125;; 新生成的对象会被加入第0代，前面_PyObject_GC_Malloc中省略的部分就是Python GC触发的时机。每新生成一个对象都会检查第0代有没有满，如果满了就开始着手进行垃圾回收.1234567891011g-&gt;gc.gc_refs = GC_UNTRACKED; generations[0].count++; /* number of allocated GC objects */ if (generations[0].count &gt; generations[0].threshold &amp;&amp; enabled &amp;&amp; generations[0].threshold &amp;&amp; !collecting &amp;&amp; !PyErr_Occurred()) &#123; collecting = 1; collect_generations(); collecting = 0; &#125; 3.1 在Python中的零代(Generation Zero)很多时候你的代码也许会在不经意间包含循环引用并且你并未意识到。事实上，当你的Python程序运行的时候它将会建立一定数量的“浮点数垃圾”，Python的GC不能够处理未使用的对象因为应用计数值不会到零。就是为什么Python要引入Generational GC算法的原因！正如Ruby使用一个链表(free list)来持续追踪未使用的、自由的对象一样，Python使用一种不同的链表来持续追踪活跃的对象。而不将其称之为“活跃列表”，Python的内部C代码将其称为零代(Generation Zero)。每次当你创建一个对象或其他什么值的时候，Python会将其加入零代链表。请注意到这并不是一个真正的列表，并不能直接在你的代码中访问，事实上这个链表是一个完全内部的Python运行体 3.2 Python中的GC阈值随着你的程序运行，Python解释器保持对新创建的对象，以及因为引用计数为零而被释放掉的对象的追踪。从理论上说，这两个值应该保持一致，因为程序新建的每个对象都应该最终被释放掉。 当然，事实并非如此。因为循环引用的原因，并且因为你的程序使用了一些比其他对象存在时间更长的对象，从而被分配对象的计数值与被释放对象的计数值之间的差异在逐渐增长。一旦这个差异累计超过某个阈值，则Python的收集机制就启动了，并且触发上边所说到的零代算法，释放“浮动的垃圾”，并且将剩下的对象移动到一代列表。 随着时间的推移，程序所使用的对象逐渐从零代列表移动到一代列表。而Python对于一代列表中对象的处理遵循同样的方法，一旦被分配计数值与被释放计数值累计到达一定阈值，Python会将剩下的活跃对象移动到二代列表。 通过这种方法，你的代码所长期使用的对象，那些你的代码持续访问的活跃对象，会从零代链表转移到一代再转移到二代。通过不同的阈值设置，Python可以在不同的时间间隔处理这些对象。Python处理零代最为频繁，其次是一代然后才是二代。 3.3 弱代假说来看看代垃圾回收算法的核心行为：垃圾回收器会更频繁的处理新对象。一个新的对象即是你的程序刚刚创建的，而一个来的对象则是经过了几个时间周期之后仍然存在的对象。Python会在当一个对象从零代移动到一代，或是从一代移动到二代的过程中提升(promote)这个对象。 为什么要这么做？这种算法的根源来自于弱代假说(weak generational hypothesis)。这个假说由两个观点构成：首先是年亲的对象通常死得也快，而老对象则很有可能存活更长的时间。 假定现在我用Python或是Ruby创建一个新对象 n1=”ABC”： 根据假说，我的代码很可能仅仅会使用ABC很短的时间。这个对象也许仅仅只是一个方法中的中间结果，并且随着方法的返回这个对象就将变成垃圾了。大部分的新对象都是如此般地很快变成垃圾。然而，偶尔程序会创建一些很重要的，存活时间比较长的对象-例如web应用中的session变量或是配置项。 通过频繁的处理零代链表中的新对象，Python的垃圾收集器将把时间花在更有意义的地方：它处理那些很快就可能变成垃圾的新对象。同时只在很少的时候，当满足阈值的条件，收集器才回去处理那些老变量。 分代回收的逻辑分配内存-&gt; 发现超过阈值了-&gt; 触发垃圾回收-&gt; 将所有可收集对象链表放到一起-&gt; 遍历, 计算有效引用计数-&gt; 分成 有效引用计数=0 和 有效引用计数 &gt; 0 两个集合-&gt; 大于0的, 放入到更老一代-&gt; =0的, 执行回收-&gt; 回收遍历容器内的各个元素, 减掉对应元素引用计数(破掉循环引用)-&gt; 执行-1的逻辑, 若发现对象引用计数=0, 触发内存回收-&gt; python底层内存管理机制回收内存 总结总体来说，在Python中，主要通过引用计数进行垃圾回收；通过 “标记-清除” 解决容器对象可能产生的循环引用问题；通过 “分代回收” 以空间换时间的方法提高垃圾回收效率。 参考文章：一文搞定Python垃圾回收机制Python垃圾回收机制–完美讲解!Python中的垃圾回收机制PYTHON 源码阅读 - 垃圾回收机制《垃圾回收的算法与实现》","categories":[{"name":"python","slug":"python","permalink":"https://liangweijiang.github.io/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"https://liangweijiang.github.io/tags/python/"},{"name":"垃圾回收","slug":"垃圾回收","permalink":"https://liangweijiang.github.io/tags/%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/"}]},{"title":"每日一道算法之--有效的括号","slug":"algorithm-20","date":"2020-03-03T11:41:23.245Z","updated":"2020-03-03T11:42:43.389Z","comments":true,"path":"2020/03/03/algorithm-20/","link":"","permalink":"https://liangweijiang.github.io/2020/03/03/algorithm-20/","excerpt":"","text":"有效的括号力扣第20题 : https://leetcode-cn.com/problems/valid-parentheses/ 给定一个只包括 ‘(‘，’)’，’{‘，’}’，’[‘，’]’ 的字符串，判断字符串是否有效。 有效字符串需满足： 左括号必须用相同类型的右括号闭合。左括号必须以正确的顺序闭合。注意空字符串可被认为是有效字符串。 示例 1: 输入: “()”输出: true 示例 2: 输入: “()[]{}”输出: true 示例 3: 输入: “(]”输出: false 题目分析题目不难理解,就是括号要一一对应,换句话来说,只要出现了右括号,name就必定有一个左括号与之对应,而且左括号和右括号必须是对称的,刚好栈这种数据结构可以很好地实现这一目的. 要想一一对应,字符串长度必须是偶数 遍历这个数组,每当遇到左括号时,直接入栈 当遇到有括号时,就取出栈顶元素,然后与该括号配对,若配对失败,则证明字符串无效,若配对成功,则继续遍历 直到数组遍历完成并且栈里没有元素以后,则证明字符串有效 翻译成代码如下: 123456789101112class Solution: def isValid(self, s: str) -&gt; bool: has_map = &#123;')':'(', '&#125;':'&#123;', ']':'['&#125; stack = [] if len(s) % 2 != 0: return False for char in s: if char in has_map: top_ele = stack.pop() if stack else '' if top_ele != has_map[char]: return False else: stack.append(char) return not stack 复杂度分析时间复杂度：因为我们一次只遍历给定的字符串中的一个字符并在栈上进行 O(1) 的推入和弹出操作,所以时间复杂度为O(n)。 空间复杂度：在最糟糕的情况下，我们最终要把所有括号推到栈上。例如 ((((((((((, 而如果是有效字符串,则也要将n/2的括号入栈,所以空间复杂度为O(n)。","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://liangweijiang.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"栈","slug":"栈","permalink":"https://liangweijiang.github.io/tags/%E6%A0%88/"},{"name":"数组","slug":"数组","permalink":"https://liangweijiang.github.io/tags/%E6%95%B0%E7%BB%84/"}]},{"title":"每日一道算法之--二叉树的层次遍历","slug":"algorithm-102","date":"2020-03-03T09:46:49.296Z","updated":"2020-03-03T10:35:03.161Z","comments":true,"path":"2020/03/03/algorithm-102/","link":"","permalink":"https://liangweijiang.github.io/2020/03/03/algorithm-102/","excerpt":"","text":"二叉树的层次遍历力扣第102题 : https://leetcode-cn.com/problems/binary-tree-level-order-traversal/ 给定一个二叉树，返回其按层次遍历的节点值。 （即逐层地，从左到右访问所有节点）。 例如: 给定二叉树: [3,9,20,null,null,15,7], 3 / 9 20 / 15 7 返回其层次遍历结果： [ [3], [9,20], [15,7]] 1.递归一开始看到这题时,因为之前学过二叉树的知识,所以我知道这道题的第一思路就是递归但是这道层次遍历的主要问题就是,数是分左右节点的,如果直接递归,那么同一层的左右节点就不会给同时读取.所以,能不能先定义一个变量,专门记录该节点的层级.然后分析题目的返回值,它是一个子元素是列表的列表,不难发现,子元素的索引,刚好就是数的层级,所以可以直接定义一个列表levels = [],每递归一层,如果大列表中没有该层级的子列表,就往大列表中添加一个字列表,然后往该层级的子列表中添加该节点的值代码如下: 12345678910111213141516171819202122# Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def levelOrder(self, root: TreeNode) -&gt; List[List[int]]: levels= [] if not root : return levels def foo(node, level): n = len(levels) if n == level: levels.append([]) levels[level].append(node.val) if node.left: foo(node.left, level + 1) if node.right: foo(node.right, level + 1) foo(root, 0) return levels 1.1 复杂度分析时间复杂度:假设有n个节点,恰好会递归到每一个节点,所以时间复杂度为O(n) 空间复杂度:假设有n个节点,数组存储的刚好是每一个节点,所以空间复杂度为O(n) 2.迭代实现了递归的方式后,迭代的思想节本和递归差不多,只是要加多一个栈的数据结构 12345678910111213141516class Solution: def levelOrder(self, root: TreeNode) -&gt; List[List[int]]: levels = [] stack = [] if not root: return levels stack.append((root, 0)) while stack: node, level = stack.pop() if level == len(levels): levels.append([]) levels[level].append(node.val) if node.right: stack.append((node.right, level + 1)) if node.left: stack.append((node.left, level + 1)) return levels 2.1 复杂度分析时间复杂度：假设有n个节点,恰好会递归到每一个节点,因为每个节点恰好会被运算一次,所以时间复杂度为O(n) 空间复杂度：假设有n个节点,数组存储的刚好是每一个节点,栈的存储也是每一个节点,所以空间复杂度为O(n)+O(n) = O(n) 总结这道题考察的就是树的遍历,和前后序遍历差不错,但是就是加多了一个变量来记录层级,还是那句话,多加练习,多加练习!!!!!!","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://liangweijiang.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"递归","slug":"递归","permalink":"https://liangweijiang.github.io/tags/%E9%80%92%E5%BD%92/"},{"name":"树的遍历","slug":"树的遍历","permalink":"https://liangweijiang.github.io/tags/%E6%A0%91%E7%9A%84%E9%81%8D%E5%8E%86/"}]},{"title":"每日一道算法之--链表反转","slug":"algorithm-206","date":"2020-03-02T10:10:34.816Z","updated":"2020-03-02T10:32:34.192Z","comments":true,"path":"2020/03/02/algorithm-206/","link":"","permalink":"https://liangweijiang.github.io/2020/03/02/algorithm-206/","excerpt":"","text":"反转链表力扣第206题:https://leetcode-cn.com/problems/reverse-linked-list/submissions/ 反转一个单链表。 示例: 输入: 1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;NULL输出: 5-&gt;4-&gt;3-&gt;2-&gt;1-&gt;NULL 进阶:你可以迭代或递归地反转链表。你能否用两种方法解决这道题？ 1.递归解决1.1 创建新的节点一开始的时候我的想法是,既然是反转链表,那就可以创建一个新的节点来重新存储,然后直接递归到最后的节点,在不断地回溯中将新节点指向回溯的节点. 123456789101112131415161718192021# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: new_node = ListNode(0) n = new_node def reverseList(self, head: ListNode) -&gt; ListNode: if not head:return head def foo(head): if not head.next: return head node = foo(head.next) if node: self.n.next = ListNode(node.val) self.n = self.n.next return head foo(head) self.n.next = ListNode(head.val) return self.new_node.next 在测试了几个例子都行的通,我就很兴奋的提交了,结果哦吼,提交时间超时…… 1.2 在原链表处理我就在找原因,有可能是因为创建了新节点,处理时间太长,能不能不用新的节点,直接在原本的链表中去改变指针的指向,当获取到最后的节点时直接处理该节点 逐步递归,知道获取到最后的节点 递归回退的时候,一次改变节点的指针,使该节点的下下个指针指向自己,即node.next.next = node 注意的是,要将该节点的下一个指针删除,一开始我没注意,看了官方题解才知道 1234567class Solution: def reverseList(self, head: ListNode) -&gt; ListNode: if not head or not head.next: return head node = self.reverseList(head.next) head.next.next = head; head.next = None; return node 1.3 复杂度分析时间复杂度：O(n)，假设 n 是列表的长度，要层层递归,那么时间复杂度为 O(n)。 空间复杂度：O(n)，由于使用递归，将会使用隐式栈空间。递归深度可能会达到 nn 层。 2.迭代实现1.1 用栈实现因为栈是先进先出的数据结构,所以先用栈存储每一个节点,然后在依次取出操作 12345678910111213class Solution: def reverseList(self, head: ListNode) -&gt; ListNode: if not head or not head.next: return head stack = [] while head: stack.append(head) head = head.next n = stack.pop() while stack: node = stack.pop() node.next.next = node node.next = None return n 1.1.1 复杂度分析时间复杂度:假设列表长度为n, 进栈的时间复杂度为O(n),出栈的时间复杂度同样为O(n),所以时间复杂度为2*O(n) = O(n) 空间复杂度为:运用了栈的数据结构,所以空间复杂度为O(n)可以看到效率还是不错的 2.2 官方的双指针解法当我们要反转一个链表时,只需要改变每一个节点的前驱和后继指针 12345678910class Solution: def reverseList(self, head: ListNode) -&gt; ListNode: if not head: return None prev &#x3D; None cur &#x3D; head while cur: cur.next&#x3D; prev prev &#x3D; cur cur &#x3D; cur.next return prev 2.2.1 复杂度分析时间复杂度:O(n) 空间复杂度为:O(n)","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://liangweijiang.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"链表","slug":"链表","permalink":"https://liangweijiang.github.io/tags/%E9%93%BE%E8%A1%A8/"},{"name":"递归","slug":"递归","permalink":"https://liangweijiang.github.io/tags/%E9%80%92%E5%BD%92/"}]},{"title":"每日一道算法之--股票问题总结","slug":"algorithm-stock","date":"2020-03-02T07:31:24.558Z","updated":"2020-03-02T10:32:55.199Z","comments":true,"path":"2020/03/02/algorithm-stock/","link":"","permalink":"https://liangweijiang.github.io/2020/03/02/algorithm-stock/","excerpt":"","text":"股票问题总结最近在leetcode刷题的时候发现了一个很好的解决股票问题的题解,所以收藏了这个链接,他主要运用了状态机的思想,从而非常有效的解决股票等dp问题 文章如下: 本文参考自英文版LeetCode：https://leetcode.com/problems/best-time-to-buy-and-sell-stock-with-transaction-fee/discuss/108870/Most-consistent-ways-of-dealing-with-the-series-of-stock-problems 很多读者抱怨股票系列问题奇技淫巧太多，如果面试真的遇到这类问题，基本不会想到那些巧妙的办法，怎么办？所以本文拒绝奇技淫巧，而是稳扎稳打，只用一种通用方法解决所用问题，以不变应万变。 这篇文章用状态机的技巧来解决，可以全部提交通过。不要觉得这个名词高大上，文学词汇而已，实际上就是 DP table，看一眼就明白了。 先随便抽出一道题，看看别人的解法： 123456789101112int maxProfit(vector&lt;int&gt;&amp; prices) &#123; if(prices.empty()) return 0; int s1=-prices[0],s2=INT_MIN,s3=INT_MIN,s4=INT_MIN; for(int i=1;i&lt;prices.size();++i) &#123; s1 = max(s1, -prices[i]); s2 = max(s2, s1+prices[i]); s3 = max(s3, s2-prices[i]); s4 = max(s4, s3+prices[i]); &#125; return max(0,s4);&#125; 能看懂吧？会做了吗？不可能的，你看不懂，这才正常。就算你勉强看懂了，下一个问题你还是做不出来。为什么别人能写出这么诡异却又高效的解法呢？因为这类问题是有框架的，但是人家不会告诉你的，因为一旦告诉你，你五分钟就学会了，该算法题就不再神秘，变得不堪一击了。 本文就来告诉你这个框架，然后带着你一道一道秒杀。 这 6 道股票买卖问题是有共性的，我们通过对第四题（限制最大交易次数为 k）的分析一道一道解决。因为第四题是一个最泛化的形式，其他的问题都是这个形式的简化。 第一题是只进行一次交易，相当于 k = 1；第二题是不限交易次数，相当于 k = +infinity（正无穷）；第三题是只进行 2 次交易，相当于 k = 2；剩下两道也是不限次数，但是加了交易「冷冻期」和「手续费」的额外条件，其实就是第二题的变种，都很容易处理。 一、穷举框架首先，还是一样的思路：如何穷举？这里的穷举思路和上篇文章递归的思想不太一样。 递归其实是符合我们思考的逻辑的，一步步推进，遇到无法解决的就丢给递归，一不小心就做出来了，可读性还很好。缺点就是一旦出错，你也不容易找到错误出现的原因。比如上篇文章的递归解法，肯定还有计算冗余，但确实不容易找到。 而这里，我们不用递归思想进行穷举，而是利用「状态」进行穷举。我们具体到每一天，看看总共有几种可能的「状态」，再找出每个「状态」对应的「选择」。我们要穷举所有「状态」，穷举的目的是根据对应的「选择」更新状态。听起来抽象，你只要记住「状态」和「选择」两个词就行，下面实操一下就很容易明白了。 1234for 状态1 in 状态1的所有取值： for 状态2 in 状态2的所有取值： for ... dp[状态1][状态2][...] = 择优(选择1，选择2...) 比如说这个问题，每天都有三种「选择」：买入、卖出、无操作，我们用 buy, sell, rest 表示这三种选择。但问题是，并不是每天都可以任意选择这三种选择的，因为 sell 必须在 buy 之后，buy 必须在 sell 之后。那么 rest 操作还应该分两种状态，一种是 buy 之后的 rest（持有了股票），一种是 sell 之后的 rest（没有持有股票）。而且别忘了，我们还有交易次数 k 的限制，就是说你 buy 还只能在 k &gt; 0 的前提下操作。 很复杂对吧，不要怕，我们现在的目的只是穷举，你有再多的状态，老夫要做的就是一把梭全部列举出来。这个问题的「状态」有三个，第一个是天数，第二个是允许交易的最大次数，第三个是当前的持有状态（即之前说的 rest 的状态，我们不妨用 1 表示持有，0 表示没有持有）。然后我们用一个三维数组就可以装下这几种状态的全部组合： 123456789dp[i][k][0 or 1]0 &lt;= i &lt;= n-1, 1 &lt;= k &lt;= Kn 为天数，大 K 为最多交易数此问题共 n × K × 2 种状态，全部穷举就能搞定。for 0 &lt;= i &lt; n: for 1 &lt;= k &lt;= K: for s in &#123;0, 1&#125;: dp[i][k][s] = max(buy, sell, rest) 而且我们可以用自然语言描述出每一个状态的含义，比如说 dp[3][2][1] 的含义就是：今天是第三天，我现在手上持有着股票，至今最多进行 2 次交易。再比如 dp[2][3][0] 的含义：今天是第二天，我现在手上没有持有股票，至今最多进行 3 次交易。很容易理解，对吧？ 我们想求的最终答案是 dp[n - 1][K][0]，即最后一天，最多允许 K 次交易，最多获得多少利润。读者可能问为什么不是 dp[n - 1][K][1]？因为 [1] 代表手上还持有股票，[0] 表示手上的股票已经卖出去了，很显然后者得到的利润一定大于前者。 记住如何解释「状态」，一旦你觉得哪里不好理解，把它翻译成自然语言就容易理解了。 二、状态转移框架现在，我们完成了「状态」的穷举，我们开始思考每种「状态」有哪些「选择」，应该如何更新「状态」。只看「持有状态」，可以画个状态转移图。 123456789101112131415通过这个图可以很清楚地看到，每种状态（0 和 1）是如何转移而来的。根据这个图，我们来写一下状态转移方程：dp[i][k][0] = max(dp[i-1][k][0], dp[i-1][k][1] + prices[i]) max( 选择 rest , 选择 sell )解释：今天我没有持有股票，有两种可能：要么是我昨天就没有持有，然后今天选择 rest，所以我今天还是没有持有；要么是我昨天持有股票，但是今天我 sell 了，所以我今天没有持有股票了。dp[i][k][1] = max(dp[i-1][k][1], dp[i-1][k-1][0] - prices[i]) max( 选择 rest , 选择 buy )解释：今天我持有着股票，有两种可能：要么我昨天就持有着股票，然后今天选择 rest，所以我今天还持有着股票；要么我昨天本没有持有，但今天我选择 buy，所以今天我就持有股票了。 这个解释应该很清楚了，如果 buy，就要从利润中减去 prices[i]，如果 sell，就要给利润增加 prices[i]。今天的最大利润就是这两种可能选择中较大的那个。而且注意 k 的限制，我们在选择 buy 的时候，把 k 减小了 1，很好理解吧，当然你也可以在 sell 的时候减 1，一样的。 现在，我们已经完成了动态规划中最困难的一步：状态转移方程。如果之前的内容你都可以理解，那么你已经可以秒杀所有问题了，只要套这个框架就行了。不过还差最后一点点，就是定义 base case，即最简单的情况。 12345678dp[-1][k][0] = 0解释：因为 i 是从 0 开始的，所以 i = -1 意味着还没有开始，这时候的利润当然是 0 。dp[-1][k][1] = -infinity解释：还没开始的时候，是不可能持有股票的，用负无穷表示这种不可能。dp[i][0][0] = 0解释：因为 k 是从 1 开始的，所以 k = 0 意味着根本不允许交易，这时候利润当然是 0 。dp[i][0][1] = -infinity解释：不允许交易的情况下，是不可能持有股票的，用负无穷表示这种不可能。 把上面的状态转移方程总结一下： 12345678base case：dp[-1][k][0] = dp[i][0][0] = 0dp[-1][k][1] = dp[i][0][1] = -infinity状态转移方程：dp[i][k][0] = max(dp[i-1][k][0], dp[i-1][k][1] + prices[i])dp[i][k][1] = max(dp[i-1][k][1], dp[i-1][k-1][0] - prices[i])读者可能会问，这个数组索引是 -1 怎么编程表示出来呢，负无穷怎么表示呢？这都是细节问题，有很多方法实现。现在完整的框架已经完成，下面开始具体化。 三、秒杀题目第一题，k = 1直接套状态转移方程，根据 base case，可以做一些化简： 123456789dp[i][1][0] = max(dp[i-1][1][0], dp[i-1][1][1] + prices[i])dp[i][1][1] = max(dp[i-1][1][1], dp[i-1][0][0] - prices[i]) = max(dp[i-1][1][1], -prices[i])解释：k = 0 的 base case，所以 dp[i-1][0][0] = 0。现在发现 k 都是 1，不会改变，即 k 对状态转移已经没有影响了。可以进行进一步化简去掉所有 k：dp[i][0] = max(dp[i-1][0], dp[i-1][1] + prices[i])dp[i][1] = max(dp[i-1][1], -prices[i]) 直接写出代码： 1234567int n = prices.length;int[][] dp = new int[n][2];for (int i = 0; i &lt; n; i++) &#123; dp[i][0] = Math.max(dp[i-1][0], dp[i-1][1] + prices[i]); dp[i][1] = Math.max(dp[i-1][1], -prices[i]);&#125;return dp[n - 1][0]; 显然 i = 0 时 dp[i-1] 是不合法的。这是因为我们没有对 i 的 base case 进行处理。可以这样处理： 12345678910111213141516171819for (int i = 0; i &lt; n; i++) &#123; if (i - 1 == -1) &#123; dp[i][0] = 0; // 解释： // dp[i][0] // = max(dp[-1][0], dp[-1][1] + prices[i]) // = max(0, -infinity + prices[i]) = 0 dp[i][1] = -prices[i]; //解释： // dp[i][1] // = max(dp[-1][1], dp[-1][0] - prices[i]) // = max(-infinity, 0 - prices[i]) // = -prices[i] continue; &#125; dp[i][0] = Math.max(dp[i-1][0], dp[i-1][1] + prices[i]); dp[i][1] = Math.max(dp[i-1][1], -prices[i]);&#125;return dp[n - 1][0]; 第一题就解决了，但是这样处理 base case 很麻烦，而且注意一下状态转移方程，新状态只和相邻的一个状态有关，其实不用整个 dp 数组，只需要一个变量储存相邻的那个状态就足够了，这样可以把空间复杂度降到 O(1): 12345678910111213// k == 1int maxProfit_k_1(int[] prices) &#123; int n = prices.length; // base case: dp[-1][0] = 0, dp[-1][1] = -infinity int dp_i_0 = 0, dp_i_1 = Integer.MIN_VALUE; for (int i = 0; i &lt; n; i++) &#123; // dp[i][0] = max(dp[i-1][0], dp[i-1][1] + prices[i]) dp_i_0 = Math.max(dp_i_0, dp_i_1 + prices[i]); // dp[i][1] = max(dp[i-1][1], -prices[i]) dp_i_1 = Math.max(dp_i_1, -prices[i]); &#125; return dp_i_0;&#125; 两种方式都是一样的，不过这种编程方法简洁很多。但是如果没有前面状态转移方程的引导，是肯定看不懂的。后续的题目，我主要写这种空间复杂度 O(1) 的解法。 第二题，k = +infinity如果 k 为正无穷，那么就可以认为 k 和 k - 1 是一样的。可以这样改写框架： 1234567dp[i][k][0] = max(dp[i-1][k][0], dp[i-1][k][1] + prices[i])dp[i][k][1] = max(dp[i-1][k][1], dp[i-1][k-1][0] - prices[i]) = max(dp[i-1][k][1], dp[i-1][k][0] - prices[i])我们发现数组中的 k 已经不会改变了，也就是说不需要记录 k 这个状态了：dp[i][0] = max(dp[i-1][0], dp[i-1][1] + prices[i])dp[i][1] = max(dp[i-1][1], dp[i-1][0] - prices[i]) 直接翻译成代码： 12345678910int maxProfit_k_inf(int[] prices) &#123; int n = prices.length; int dp_i_0 = 0, dp_i_1 = Integer.MIN_VALUE; for (int i = 0; i &lt; n; i++) &#123; int temp = dp_i_0; dp_i_0 = Math.max(dp_i_0, dp_i_1 + prices[i]); dp_i_1 = Math.max(dp_i_1, temp - prices[i]); &#125; return dp_i_0;&#125; 第三题，k = +infinity with cooldown每次 sell 之后要等一天才能继续交易。只要把这个特点融入上一题的状态转移方程即可： 1234567891011121314151617dp[i][0] = max(dp[i-1][0], dp[i-1][1] + prices[i])dp[i][1] = max(dp[i-1][1], dp[i-2][0] - prices[i])解释：第 i 天选择 buy 的时候，要从 i-2 的状态转移，而不是 i-1 。翻译成代码：int maxProfit_with_cool(int[] prices) &#123; int n = prices.length; int dp_i_0 = 0, dp_i_1 = Integer.MIN_VALUE; int dp_pre_0 = 0; // 代表 dp[i-2][0] for (int i = 0; i &lt; n; i++) &#123; int temp = dp_i_0; dp_i_0 = Math.max(dp_i_0, dp_i_1 + prices[i]); dp_i_1 = Math.max(dp_i_1, dp_pre_0 - prices[i]); dp_pre_0 = temp; &#125; return dp_i_0;&#125; 第四题，k = +infinity with fee每次交易要支付手续费，只要把手续费从利润中减去即可。改写方程： 12345678910111213141516dp[i][0] = max(dp[i-1][0], dp[i-1][1] + prices[i])dp[i][1] = max(dp[i-1][1], dp[i-1][0] - prices[i] - fee)解释：相当于买入股票的价格升高了。在第一个式子里减也是一样的，相当于卖出股票的价格减小了。直接翻译成代码：int maxProfit_with_fee(int[] prices, int fee) &#123; int n = prices.length; int dp_i_0 = 0, dp_i_1 = Integer.MIN_VALUE; for (int i = 0; i &lt; n; i++) &#123; int temp = dp_i_0; dp_i_0 = Math.max(dp_i_0, dp_i_1 + prices[i]); dp_i_1 = Math.max(dp_i_1, temp - prices[i] - fee); &#125; return dp_i_0;&#125; 第五题，k = 2k = 2 和前面题目的情况稍微不同，因为上面的情况都和 k 的关系不太大。要么 k 是正无穷，状态转移和 k 没关系了；要么 k = 1，跟 k = 0 这个 base case 挨得近，最后也没有存在感。 这道题 k = 2 和后面要讲的 k 是任意正整数的情况中，对 k 的处理就凸显出来了。我们直接写代码，边写边分析原因。 原始的动态转移方程，没有可化简的地方 12dp[i][k][0] = max(dp[i-1][k][0], dp[i-1][k][1] + prices[i])dp[i][k][1] = max(dp[i-1][k][1], dp[i-1][k-1][0] - prices[i]) 按照之前的代码，我们可能想当然这样写代码（错误的）： 12345678int k = 2;int[][][] dp = new int[n][k + 1][2];for (int i = 0; i &lt; n; i++) if (i - 1 == -1) &#123; /* 处理一下 base case*/ &#125; dp[i][k][0] = Math.max(dp[i-1][k][0], dp[i-1][k][1] + prices[i]); dp[i][k][1] = Math.max(dp[i-1][k][1], dp[i-1][k-1][0] - prices[i]);&#125;return dp[n - 1][k][0]; 为什么错误？我这不是照着状态转移方程写的吗？ 还记得前面总结的「穷举框架」吗？就是说我们必须穷举所有状态。其实我们之前的解法，都在穷举所有状态，只是之前的题目中 k 都被化简掉了。这道题由于没有消掉 k 的影响，所以必须要对 k 进行穷举： 12345678910111213141516int max_k = 2;int[][][] dp = new int[n][max_k + 1][2];for (int i = 0; i &lt; n; i++) &#123; for (int k = max_k; k &gt;= 1; k--) &#123; if (i - 1 == -1) &#123; /* 处理 base case */ dp[i][k][0] = 0; dp[i][k][1] = -prices[i]; continue; &#125; dp[i][k][0] = max(dp[i-1][k][0], dp[i-1][k][1] + prices[i]); dp[i][k][1] = max(dp[i-1][k][1], dp[i-1][k-1][0] - prices[i]); &#125;&#125;// 穷举了 n × max_k × 2 个状态，正确。return dp[n - 1][max_k][0]; 如果你不理解，可以返回第一点「穷举框架」重新阅读体会一下。 这里 k 取值范围比较小，所以可以不用 for 循环，直接把 k = 1 和 2 的情况手动列举出来也可以： 12345678910111213141516dp[i][2][0] = max(dp[i-1][2][0], dp[i-1][2][1] + prices[i])dp[i][2][1] = max(dp[i-1][2][1], dp[i-1][1][0] - prices[i])dp[i][1][0] = max(dp[i-1][1][0], dp[i-1][1][1] + prices[i])dp[i][1][1] = max(dp[i-1][1][1], -prices[i])int maxProfit_k_2(int[] prices) &#123; int dp_i10 = 0, dp_i11 = Integer.MIN_VALUE; int dp_i20 = 0, dp_i21 = Integer.MIN_VALUE; for (int price : prices) &#123; dp_i20 = Math.max(dp_i20, dp_i21 + price); dp_i21 = Math.max(dp_i21, dp_i10 - price); dp_i10 = Math.max(dp_i10, dp_i11 + price); dp_i11 = Math.max(dp_i11, -price); &#125; return dp_i20;&#125; 有状态转移方程和含义明确的变量名指导，相信你很容易看懂。其实我们可以故弄玄虚，把上述四个变量换成 a, b, c, d。这样当别人看到你的代码时就会一头雾水，大惊失色，不得不对你肃然起敬。 第六题，k = any integer有了上一题 k = 2 的铺垫，这题应该和上一题的第一个解法没啥区别。但是出现了一个超内存的错误，原来是传入的 k 值会非常大，dp 数组太大了。现在想想，交易次数 k 最多有多大呢？ 一次交易由买入和卖出构成，至少需要两天。所以说有效的限制 k 应该不超过 n/2，如果超过，就没有约束作用了，相当于 k = +infinity。这种情况是之前解决过的。 直接把之前的代码重用： 12345678910111213141516171819int maxProfit_k_any(int max_k, int[] prices) &#123; int n = prices.length; if (max_k &gt; n / 2) return maxProfit_k_inf(prices); int[][][] dp = new int[n][max_k + 1][2]; for (int i = 0; i &lt; n; i++) for (int k = max_k; k &gt;= 1; k--) &#123; if (i - 1 == -1) &#123; /* 处理 base case */ dp[i][k][0] = 0; dp[i][k][1] = -prices[i]; continue; &#125; dp[i][k][0] = max(dp[i-1][k][0], dp[i-1][k][1] + prices[i]); dp[i][k][1] = max(dp[i-1][k][1], dp[i-1][k-1][0] - prices[i]); &#125; return dp[n - 1][max_k][0];&#125; 至此，6 道题目通过一个状态转移方程全部解决。 四、最后总结本文给大家讲了如何通过状态转移的方法解决复杂的问题，用一个状态转移方程秒杀了 6 道股票买卖问题，现在想想，其实也不算难对吧？这已经属于动态规划问题中较困难的了。 关键就在于列举出所有可能的「状态」，然后想想怎么穷举更新这些「状态」。一般用一个多维 dp 数组储存这些状态，从 base case 开始向后推进，推进到最后的状态，就是我们想要的答案。想想这个过程，你是不是有点理解「动态规划」这个名词的意义了呢？ 具体到股票买卖问题，我们发现了三个状态，使用了一个三维数组，无非还是穷举 + 更新，不过我们可以说的高大上一点，这叫「三维 DP」，怕不怕？这个大实话一说，立刻显得你高人一等，名利双收有没有。 所以，大家不要被各种高大上的名词吓到，再多的困难问题，奇技淫巧，也不过是基本套路的不断升级组合产生的。只要把住算法的底层原理，即可举一反三，逐个击破。 买卖股票的最佳时机 买卖股票的最佳时机 II 买卖股票的最佳时机 III 买卖股票的最佳时机 IV 最佳买卖股票时机含冷冻期 买卖股票的最佳时机含手续费 作者：labuladong链接：https://leetcode-cn.com/problems/best-time-to-buy-and-sell-stock/solution/yi-ge-fang-fa-tuan-mie-6-dao-gu-piao-wen-ti-by-l-3/来源：力扣（LeetCode）著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://liangweijiang.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"动态规划","slug":"动态规划","permalink":"https://liangweijiang.github.io/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"},{"name":"状态机","slug":"状态机","permalink":"https://liangweijiang.github.io/tags/%E7%8A%B6%E6%80%81%E6%9C%BA/"}]},{"title":"每日一道算法之--无重复的最长的子字符串","slug":"algorithm-3","date":"2020-02-29T08:59:16.674Z","updated":"2020-03-11T12:30:29.188Z","comments":true,"path":"2020/02/29/algorithm-3/","link":"","permalink":"https://liangweijiang.github.io/2020/02/29/algorithm-3/","excerpt":"","text":"无重复的最长的子字符串力扣第3题:https://leetcode-cn.com/problems/longest-substring-without-repeating-characters/ 1 暴力解决暴力方法就是将所有的肯恩列举出来,在会超过时间限制,这里就不列举了 2 滑动窗口这道题是我第一道碰到的滑动窗口的题目,当时想了很久都想不出来,后来去看一一下别人的题解恍然大悟,滑动窗口确实是解决字符串问题的一非常好的方法 滑动窗口的顾名思义就是可以滑动的数据结构,遍历整个字符串,当滑动窗口里没有该字符时,就将该字符加进滑动窗口 当遇到有重复字符时,要将滑动窗口该字符前面的的字符(包括该字符)全部去除,然后在将该字符加进滑动窗口 代码如下: 1234567891011121314151617181920class Solution: def lengthOfLongestSubstring(self, s: str) -&gt; int: # 集合可以保证唯一性 silde_win = set() # 设置滑动窗口的值 left = 0 # 记录最长子串的长度 max_len = 0 # 记录当前的子串长度 cur_len = 0 for i in range(len(s)): cur_len += 1 # 设置滑动窗口的值 while s[i] in silde_win: silde_win.remove(s[left]) left += 1 cur_len -= 1 silde_win.add(s[i]) max_len = max(max_len, cur_len) return max_len 2.1 复杂度分析时间复杂度:遍历了一次数组,时间复杂度为$$0(N)$$, 但是每次滑动窗口的都有重复字符,如字符串为’aaaaaaaaaaaaaaaaaaa’,会浪费没必要的时间,但是均摊时间复杂度依然为$$O(N)$$ 空间复杂度:用了集合,所以空间复杂度为$$O(N)$$ 滑动窗口的优化可以看到提交的成绩不是很理想,所以我一直在想着怎么优化,想到了几个优化方法 如果字符串有相邻的子串,直接忽视123456789101112131415161718192021222324class Solution: def lengthOfLongestSubstring(self, s: str) -&gt; int: if not s :return 0 silde_win = set() left = 0 max_len = 0 cur_len = 0 for i in range(len(s)): # 如果字符串有相邻的子串,直接忽视 if i &gt; 0 and s[i-1] == s[i]: silde_win = set() silde_win.add(s[i]) left = i cur_len = 1 continue cur_len += 1 # 设置滑动窗口的值 while s[i] in silde_win: silde_win.remove(s[left]) left += 1 cur_len -= 1 silde_win.add(s[i]) max_len = max(max_len, cur_len) return max_len 可以发现成绩有了提升,但不是很明显 然后在看看我么你写的代码,silde_win这个数据结构其实是有很多操作的,话费了很多时间,换个思路想一想,我们能又能用更好地数据结构来代替这个集合呢,其实这个滑动窗口只是一个辅助的数据结构,只是用来暂时存储字符串的字串的,那为什么不直接用分片呢 123456789101112131415161718192021class Solution: def lengthOfLongestSubstring(self, s: str) -&gt; int: if not s :return 0 # 设置滑动窗口的值 left = 0 # 记录最长子串的长度 max_len = 0 # 记录当前的子串长度 cur_len = 0 for i in range(len(s)): if i &gt; 0 and s[i-1] == s[i]: left = i cur_len = 1 continue cur_len += 1 # 设置滑动窗口的值 while s[i] in s[left:i]: left += 1 cur_len -= 1 max_len = max(max_len, cur_len) return max_len 相似题目76.最小覆盖子串 159. 至多包含两个不同字符的最长子串 340. 至多包含 K 个不同字符的最长子串","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://liangweijiang.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"滑动窗口","slug":"滑动窗口","permalink":"https://liangweijiang.github.io/tags/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3/"}]},{"title":"每日一道算法之--买卖股票的最佳时机","slug":"algorithm-121","date":"2020-02-29T07:25:08.775Z","updated":"2020-03-02T10:29:59.799Z","comments":true,"path":"2020/02/29/algorithm-121/","link":"","permalink":"https://liangweijiang.github.io/2020/02/29/algorithm-121/","excerpt":"","text":"买卖股票的最佳时机力扣第121题：https://leetcode-cn.com/problems/best-time-to-buy-and-sell-stock/给定一个数组，它的第 i 个元素是一支给定股票第 i 天的价格。 如果你最多只允许完成一笔交易（即买入和卖出一支股票），设计一个算法来计算你所能获取的最大利润。 注意你不能在买入股票前卖出股票。 示例 1: 输入: [7,1,5,3,6,4]输出: 5解释: 在第 2 天（股票价格 = 1）的时候买入，在第 5 天（股票价格 = 6）的时候卖出，最大利润 = 6-1 = 5 。 注意利润不能是 7-1 = 6, 因为卖出价格需要大于买入价格。 示例 2： 输入: [7,6,4,3,1]输出: 0解释: 在这种情况下, 没有交易完成, 所以最大利润为 0 1 暴力方法暴力方法简单容易理解，只是两次循环把所有的可能一一列举出来。然后在取其中的最大值 12345678class Solution: def maxProfit(self, prices: List[int]) -&gt; int: max_profit = 0 for i in range(len(prices) - 1): for j in range(i + 1, len(prices)): profit = prices[j] - prices[i] max_profit = max(max_profit, profit) return max_profit 1.1 复杂度分析时间复杂度:循环里嵌套另一个循环,所里时间复杂度为$$O(N^2)$$ 空间复杂度:只使用了max_profit和profit两个变量,所以空间复杂度为$$O(1)$$ 2 动态规划遍历一次 由题意不难理解,当前最大的收益就是你当前的价格减去之前的最小价格,所以定义一个变量min_price存储之前的最小值,在定义一个变量max_profit存储最大的收益值,便可以达到遍历一次 状态转移方程为dp[i] = max(dp[i-1]+diff[i], 0), dp[i])指以i元素结尾的子数组的最大和 12345678class Solution: def maxProfit(self, prices: List[int]) -&gt; int: min_price = float('inf') max_profit = 0 for i in range(len(prices)): min_price = min(prices[i], min_price) max_profit = max(max_profit, prices[i]-min_price) return max_profit 2.1 复杂度分析时间复杂度:遍历了一次数组,所以时间复杂度为$$O(N)$$ 空间复杂度:只使用了min_price和max_profit两个变量,所以空间复杂度为$$O(1)$$","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://liangweijiang.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"动态规划","slug":"动态规划","permalink":"https://liangweijiang.github.io/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"}]},{"title":"DRF源码解析6--权限组件的封装和初始化","slug":"DRF-06","date":"2020-02-26T09:55:17.825Z","updated":"2020-02-26T09:56:50.508Z","comments":true,"path":"2020/02/26/DRF-06/","link":"","permalink":"https://liangweijiang.github.io/2020/02/26/DRF-06/","excerpt":"","text":"1.权限组件的初始化在前面的版本控制和认证组件里提到的initia初始化函数, 同时也对权限组件进行了初始化 1234567def initial(self, request, *args, **kwargs): # 认证组件 self.perform_authentication(request) # 权限组件 self.check_permissions(request) # 频率 self.check_throttles(request) 1.1 check_permissions可以看到,check_permissions函数就是初始化权限组件的函数了 123456789 def check_permissions(self, request): for permission in self.get_permissions():# permission_denied函数抛出异常# 说明权限组件中必须包含has_permission的方法# 同时说明这是DRF框架给我们提供的自定义权限组件的钩子 if not permission.has_permission(request, self): self.permission_denied( request, message=getattr(permission, 'message', None) ) 1.2 get_permissions可以看出,权限组件在self.get_permissions方法中获取 12def get_permissions(self): return [permission() for permission in self.permission_classes] 可以看到,这个和认证组件的获取方法是一样的,只是没有认证组件那么绕 2. 权限组件的类型DRF提供的权限组件在rest_framework.permissions中,如下图可以看到, has_permission方法返回的是布尔类型 #总结DRF的权限组件比较简单,和认证组件大致一样","categories":[{"name":"django-rest-framework","slug":"django-rest-framework","permalink":"https://liangweijiang.github.io/categories/django-rest-framework/"}],"tags":[{"name":"DRF","slug":"DRF","permalink":"https://liangweijiang.github.io/tags/DRF/"}]},{"title":"每日一道算法之--乘积最大子序和","slug":"algorithm-152","date":"2020-02-26T09:08:00.357Z","updated":"2020-03-02T10:32:21.176Z","comments":true,"path":"2020/02/26/algorithm-152/","link":"","permalink":"https://liangweijiang.github.io/2020/02/26/algorithm-152/","excerpt":"","text":"乘积最大子序和力扣第152题:https://leetcode-cn.com/problems/maximum-product-subarray/给定一个整数数组 nums ，找出一个序列中乘积最大的连续子序列（该序列至少包含一个数）。 示例 1: 输入: [2,3,-2,4]输出: 6解释: 子数组 [2,3] 有最大乘积 6。 示例 2: 输入: [-2,0,-1]输出: 0解释: 结果不能为 2, 因为 [-2,-1] 不是子数组。 1. 类似指针的解法参考力扣第53题最大子序和的其中一种双指针解法,这道题同样可以用类似指针的解法 首先定义一个res记录最大的子序和,cur_pos记录当前的乘积 然后cur_pos依次累乘, 每一次的结果都有三种情况: cur_pos等于0, 这时候要重新将指针偏移后一位 cur_pos大于0, 这时候要更新res的结果,就是和cur_pos比较大小 cur_pos小于0, 负负得正,需要找到指针前面最大的负数,相除就变为最大 12345678910111213141516171819202122232425262728class Solution: def maxProduct(self, nums: List[int]) -&gt; int: if not nums: return 0 # 目前的累乘 cur_pro = 1 # 前面最大的负数 max_neg = float(\"-inf\") # 结果 res = float(\"-inf\") for num in nums: cur_pro *= num # 考虑三种情况 # 大于0 if cur_pro &gt; 0: res = max(res, cur_pro) # 小于0 elif cur_pro &lt; 0: if max_neg != float(\"-inf\"): res = max(res, cur_pro // max_neg) else: res = max(res, num) max_neg = max(max_neg, cur_pro) # 等于0 else: cur_pro = 1 max_neg = float(\"-inf\") res = max(res, num) return res 1.1 复杂度分析时间复杂度:遍历整个数组的时间复杂度为O(N)空间复杂度:没有用到额外的数据结构,所以空间复杂度为O(1) 2. 动态规划解决不难发现,每一个元素的乘积,最大值只可能在自身或者自身与上一次的累乘之中公式如下: dp_max[i] = Math.max(nums[i-1],dp_max[i-1]*nums[i-1]) 因为存在负数,所以当这个元素为负数是,其最大乘积可能是与上一次最小乘积相乘所以不仅要定义一个变量存储最大当前的最大累乘,还要定义一个变量存储当前的最小累乘 12345678910111213141516class Solution: def maxProduct(self, nums: List[int]) -&gt; int: if not nums: return 0 # 存储最后的结果 res = nums[0] # 存储最大当前的最大累乘 res_max = nums[0] # 存储当前的最小累乘 res_min = nums[0] for num in nums[1:]: cur_max = max(res_max*num, res_min*num, num) cur_min = min(res_max*num, res_min*num, num) res = max(res, cur_max) res_max = cur_max res_min = cur_min return res 2.1 复杂度分析时间复杂度:遍历整个数组的时间复杂度为O(N)空间复杂度:没有用到额外的数据结构,所以空间复杂度为O(1)","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://liangweijiang.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"动态规划","slug":"动态规划","permalink":"https://liangweijiang.github.io/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"},{"name":"指针","slug":"指针","permalink":"https://liangweijiang.github.io/tags/%E6%8C%87%E9%92%88/"}]},{"title":"DRF源码解析5--认证组件的封装和初始化","slug":"DRF-05","date":"2020-02-24T12:59:20.730Z","updated":"2020-02-26T09:41:53.685Z","comments":true,"path":"2020/02/24/DRF-05/","link":"","permalink":"https://liangweijiang.github.io/2020/02/24/DRF-05/","excerpt":"","text":"1.认证组件的初始化在对DRF的版本认证里说到,initial()不仅对版本认证组件进行了初始化,同时还初始化了认证组件,现在来分析一下DRF是怎么样初始化认证方法的 12def perform_authentication(self, request): request.user 可以看到,perform_authentication()方法返回了request(相当于封装的Request),进入request中找到uesr方法 1.1 截取Request部分代码如下:123456789101112131415161718192021222324252627class Request: def __init__(self, request, parsers=None, authenticators=None, negotiator=None, parser_context=None): assert isinstance(request, HttpRequest), ( 'The `request` argument must be an instance of ' '`django.http.HttpRequest`, not `&#123;&#125;.&#123;&#125;`.' .format(request.__class__.__module__, request.__class__.__name__) ) self._request = request self.parsers = parsers or () self.authenticators = authenticators or () self.negotiator = negotiator or self._default_negotiator() @property def user(self): if not hasattr(self, '_user'): with wrap_attributeerrors(): self._authenticate() return self._user @user.setter def user(self, value): # 当调用user时触发, 如user = 'xxx', 则value = 'xxx' self._user = value self._request.user = value 可以看到,user执行了self._authenticate方法,返回的_user为当前用户 截取self._authenticate的代码如下:12345678910111213 def _authenticate(self): for authenticator in self.authenticators: try:# 这里是调用默认的认证组件里的authenticate方法 user_auth_tuple = authenticator.authenticate(self) except exceptions.APIException: self._not_authenticated() raise if user_auth_tuple is not None: self._authenticator = authenticator self.user, self.auth = user_auth_tuple return self.authenticators为Request的属性,所以在init中可以找到 123def __init__(self, request, parsers=None, authenticators=None, negotiator=None, parser_context=None): self.authenticators = authenticators or () authenticators是作为参数传进来的,所以回到实例化Request的initialize_request方法中 12345678910def initialize_request(self, request, *args, **kwargs): parser_context = self.get_parser_context(request) return Request( request, parsers=self.get_parsers(), authenticators=self.get_authenticators(), negotiator=self.get_content_negotiator(), parser_context=parser_context ) 可以看到,authenticators传进了一个self.get_authenticators的方法 1.2 截取self.get_authenticators代码如下12def get_authenticators(self): return [auth() for auth in self.authentication_classes] 可以看到,get_authenticators方法返回了self.authentication_classes中所有auth的执行结果的一个列表, 而self.authentication_classes则为DRF中默认的认证组件 authenticate回到_authenticate方法中,现在已经知道authenticator是DRF默认的认证组件,则进去其authenticate方法中 12345678class ForcedAuthentication: def __init__(self, force_user, force_token): self.force_user = force_user self.force_token = force_token def authenticate(self, request): return (self.force_user, self.force_token) 其返回了self.force_user, self.force_token,分别为用户和验证用户信息的的token码,说明了认证组件都带有authenticate方法并且返回用户认证的信息 1.3 思路总结DRF认证组件的初始化主要分为两部分: 2. 认证组件的类型DRF提供的认证组件在rest_framework.authentication中 2.1 BaseAuthentication1234567class BaseAuthentication: def authenticate(self, request): raise NotImplementedError(\".authenticate() must be overridden.\") def authenticate_header(self, request): pass 可以看出,authenticate方法就是自定制认证组件的钩子函数 2.2 SessionAuthentication1234567891011121314151617181920212223242526class SessionAuthentication(BaseAuthentication): def authenticate(self, request): # 重写了authenticate方法并获取当前的用户信息 # Get the session-based user from the underlying HttpRequest object user = getattr(request._request, 'user', None) # Unauthenticated, CSRF validation not required if not user or not user.is_active: return None # 执行enforce_csrf方法判断用户的csrf_token是否正确 self.enforce_csrf(request) # CSRF passed with authenticated user return (user, None) def enforce_csrf(self, request): check = CSRFCheck() # populates request.META['CSRF_COOKIE'], which is used in process_view() # 中间件的process_request方法,拿取用户的CSRF_COOKIE check.process_request(request) # 认证用户的CSRF_COOKIE是否正确 reason = check.process_view(request, None, (), &#123;&#125;) if reason: # CSRF failed, bail with explicit error message raise exceptions.PermissionDenied('CSRF Failed: %s' % reason) 总结DRF的认证组件就是通过中间件等方法,获取当前用户的认证信息,在与本地的信息进行对比,从而达到认证用户的目的。","categories":[{"name":"django-rest-framework","slug":"django-rest-framework","permalink":"https://liangweijiang.github.io/categories/django-rest-framework/"}],"tags":[{"name":"DRF","slug":"DRF","permalink":"https://liangweijiang.github.io/tags/DRF/"}]},{"title":"每日一道算法之--三数之和","slug":"algorithm-15","date":"2020-02-24T09:25:51.544Z","updated":"2020-03-02T10:30:37.376Z","comments":true,"path":"2020/02/24/algorithm-15/","link":"","permalink":"https://liangweijiang.github.io/2020/02/24/algorithm-15/","excerpt":"","text":"三数之和力扣第15题：https://leetcode-cn.com/problems/3sum/给定一个包含 n 个整数的数组 nums，判断 nums 中是否存在三个元素 a，b，c ，使得 a + b + c = 0 ？找出所有满足条件且不重复的三元组。 注意：答案中不可以包含重复的三元组。 示例： 给定数组 nums = [-1, 0, 1, 2, -1, -4]，满足要求的三元组集合为：[ [-1, 0, 1], [-1, -1, 2]] 1. 排序 + 双指针解法题目中的要求是不能含有相同的三元组，所以在算法中我们需要去重，排序是一个很好的方法，因为排序可以让相同的数连载一起，方便判断去重。让后在通过双指针对数组一一检查，找到所有合适的三元组。 1234567891011121314151617181920212223242526272829class Solution: def threeSum(self, nums: List[int]) -&gt; List[List[int]]: # 先将nums排序 nums.sort() res = [] for i in range(len(nums) - 2): # 设置两个指针 l, r = i + 1, len(nums) - 1 if nums[i] &gt; 0 : return res # 去除重复的判断,因为nums[i -1]已经包含了nums[i]的所有组合的可能性 if i &gt; 0 and nums[i] == nums[i - 1] : continue while l &lt; r: s = nums[i] + nums[l] + nums[r] if s &lt; 0: # 如果和小于0，证明左边的数太小了，需要往后移 l += 1 # 去除重复的判断，但是前提条件为l&lt;r while l &lt; r and nums[l] == nums[l - 1]: l += 1 elif s &gt; 0: r -= 1 # 如果和大于0，证明右边的数太大了，需要往前移 while l &lt; r and nums[r] == nums[r + 1]: r -= 1 else: res.append([nums[i], nums[l], nums[r]]) l += 1 r -= 1 while l &lt; r and nums[l] == nums[l - 1]: l += 1 while l &lt; r and nums[r] == nums[r + 1]: r -= 1 return res 1.1 时间复杂度分析 时间复杂度：排序的时间复杂度为 O(NlogN), 遍历数组为O(N),双指针的遍历为O(N),所以总的时间复杂度为$$O(NlogN)+0(N)*O(N) = O(N^2)$$ 空间复杂度：没有用到额外的数据结构，所以空间复杂度为$$O(1)$$ 2.哈希索引的方法（空间换时间）参考两数之和，我们可以构建哈希表的方法使查找效率变得更高。 12345678910111213141516171819202122232425262728class Solution: def threeSum(self, nums: List[int]) -&gt; List[List[int]]: if len(nums) &lt; 3: return [] nums.sort() # 因为a+b = 0等价于a = -b target_hash = &#123;-x: i for i, x in enumerate(nums)&#125; res = [] res_hash = &#123;&#125; # 从零开始检索,到倒数第二位结束 for i, first in enumerate(nums[:-1]): if nums[i] &gt; 0: return res if i &gt; 0 and first == nums[i - 1]: continue #从第一个指针的下一位开始搜索 for j, second in enumerate(nums[i + 1:]): # 检查两数之和是否存在于哈希表target_hash中 if first + second in target_hash: target_index = target_hash[first + second] if target_index == i or target_index == i + j + 1: continue # 将找到的结果存入另一个哈希表中, 避免包含重复结果 row = sorted([first, second, nums[target_index]]) key = \",\".join([str(x) for x in row]) if key not in res_hash: res.append(row) res_hash[key] = True return res","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://liangweijiang.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"双指针","slug":"双指针","permalink":"https://liangweijiang.github.io/tags/%E5%8F%8C%E6%8C%87%E9%92%88/"},{"name":"哈希表","slug":"哈希表","permalink":"https://liangweijiang.github.io/tags/%E5%93%88%E5%B8%8C%E8%A1%A8/"},{"name":"排序","slug":"排序","permalink":"https://liangweijiang.github.io/tags/%E6%8E%92%E5%BA%8F/"}]},{"title":"DRF源码解析4--版本控制组件的封装","slug":"DRF-04","date":"2020-02-22T14:09:24.712Z","updated":"2020-02-22T14:11:09.294Z","comments":true,"path":"2020/02/22/DRF-04/","link":"","permalink":"https://liangweijiang.github.io/2020/02/22/DRF-04/","excerpt":"","text":"一.DRF中APIView的initial()方法初始化版本信息在讲到DRF对ruquest的封装时,返回新封装的Request后,DRF执行了initial()方法对组件的初始化 12345678910111213def initial(self, request, *args, **kwargs): # 版本控制组件 version, scheme = self.determine_version(request, *args, **kwargs) # 将版本空值信息写入request中 request.version, request.versioning_scheme = version, scheme # Ensure that the incoming request is permitted # 认证组件 self.perform_authentication(request) # 权限组件 self.check_permissions(request) # 频率 self.check_throttles(request) 1.1重写determine_version()自定制版本信息12345678910 def determine_version(self, request, *args, **kwargs): \"\"\" If versioning is being used, then determine any API version for the incoming request. Returns a two-tuple of (version, versioning_scheme) \"\"\" if self.versioning_class is None: return (None, None)# self.versioning_class是drf配置默认的类,说明我们自定义版本时要重写这个方法 scheme = self.versioning_class() return (scheme.determine_version(request, *args, **kwargs), scheme) 二.rest_framework.versioning提供的版本控制方法 2.1 BaseVersioning1234567891011121314151617181920212223class BaseVersioning: # default_version(None), allowed_versions(None), version_param可以在settings中重新配置 default_version = api_settings.DEFAULT_VERSION allowed_versions = api_settings.ALLOWED_VERSIONS version_param = api_settings.VERSION_PARAM def determine_version(self, request, *args, **kwargs): # 在继承该类时要重写determine_version(方法) msg = '&#123;cls&#125;.determine_version() must be implemented.' raise NotImplementedError(msg.format( cls=self.__class__.__name__ )) def reverse(self, viewname, args=None, kwargs=None, request=None, format=None, **extra): # 和django.urls.reverse大致相同, # 但可以选择接受一个请求并返回使用请求获取基本URL的标准URL。 return _reverse(viewname, args, kwargs, request, format, **extra) def is_allowed_version(self, version): if not self.allowed_versions: return True return ((version is not None and version == self.default_version) or (version in self.allowed_versions)) 2.2 URLPathVersioning1234567891011121314151617181920212223242526272829303132333435363738class URLPathVersioning(BaseVersioning): \"\"\" To the client this is the same style as `NamespaceVersioning`. The difference is in the backend - this implementation uses Django's URL keyword arguments to determine the version. An example URL conf for two views that accept two different versions. 可以看到版本信息就在url中 urlpatterns = [ url(r'^(?P&lt;version&gt;[v1|v2]+)/users/$', users_list, name='users-list'), url(r'^(?P&lt;version&gt;[v1|v2]+)/users/(?P&lt;pk&gt;[0-9]+)/$', users_detail, name='users-detail') ] GET /1.0/something/ HTTP/1.1 Host: example.com Accept: application/json \"\"\" invalid_version_message = _('Invalid version in URL path.') def determine_version(self, request, *args, **kwargs): # self.version_param,self.default_version是父类中的属性,可以自定义 version = kwargs.get(self.version_param, self.default_version) if version is None: # 如果没有自己配置默认为DRF默认的版本信息 version = self.default_version if not self.is_allowed_version(version): raise exceptions.NotFound(self.invalid_version_message) return version def reverse(self, viewname, args=None, kwargs=None, request=None, format=None, **extra): if request.version is not None: kwargs = &#123;&#125; if (kwargs is None) else kwargs # self.version_param就是版本的key kwargs[self.version_param] = request.version return super().reverse( viewname, args, kwargs, request, format, **extra 其他的类基本配置都差不多","categories":[{"name":"django-rest-framework","slug":"django-rest-framework","permalink":"https://liangweijiang.github.io/categories/django-rest-framework/"}],"tags":[{"name":"DRF","slug":"DRF","permalink":"https://liangweijiang.github.io/tags/DRF/"}]},{"title":"DRF源码解析3--routers对路由的封装","slug":"DRF-03","date":"2020-02-20T15:15:11.102Z","updated":"2020-02-20T15:16:50.088Z","comments":true,"path":"2020/02/20/DRF-03/","link":"","permalink":"https://liangweijiang.github.io/2020/02/20/DRF-03/","excerpt":"","text":"当我们想最方便的使用drf的路由的时候,只需要用DefaultRoute实例化一个路由实例,然后注册,再讲注册后的组件加入urlpatterns即可 123456from rest_framework.routers import DefaultRouterrouter = DefaultRouter()# 其中BookModelView继承了viewsets.ModelViewSetrouter.register(r'^book', views.BookModelView)urlpatterns += router.urls 到底routers底层是怎么样实现的呢 routersDefaultRoute是routers组件里的一个类,相当于帮助我们在路由里构建好了如{‘list’:”create”}的对应关系,它继承了SimpleRouter,截取源码如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102class SimpleRouter(BaseRouter): routes = [ # List route. Route( url=r'^&#123;prefix&#125;&#123;trailing_slash&#125;$', # 这里构建好了对应关系 mapping=&#123; 'get': 'list', 'post': 'create' &#125;, name='&#123;basename&#125;-list', detail=False, initkwargs=&#123;'suffix': 'List'&#125; ), def __init__(self, trailing_slash=True): self.trailing_slash = '/' if trailing_slash else '' super().__init__() def get_default_basename(self, viewset): \"\"\" If `basename` is not specified, attempt to automatically determine it from the viewset. \"\"\" queryset = getattr(viewset, 'queryset', None) assert queryset is not None, '`basename` argument not specified, and could ' \\ 'not automatically determine the name from the viewset, as ' \\ 'it does not have a `.queryset` attribute.' return queryset.model._meta.object_name.lower() def get_routes(self, viewset): # 获取对应关系 known_actions = list(flatten([route.mapping.values() for route in self.routes if isinstance(route, Route)])) # 找出viewsets里所有的方法 extra_actions = viewset.get_extra_actions() # checking action names against the known actions list # 判断路由的方法对应的viewsets的方法 not_allowed = [ action.__name__ for action in extra_actions if action.__name__ in known_actions ] if not_allowed: msg = ('Cannot use the @action decorator on the following ' 'methods, as they are existing routes: %s') raise ImproperlyConfigured(msg % ', '.join(not_allowed)) # partition detail and list actions detail_actions = [action for action in extra_actions if action.detail] list_actions = [action for action in extra_actions if not action.detail] routes = [] for route in self.routes: if isinstance(route, DynamicRoute) and route.detail: routes += [self._get_dynamic_route(route, action) for action in detail_actions] elif isinstance(route, DynamicRoute) and not route.detail: routes += [self._get_dynamic_route(route, action) for action in list_actions] else: routes.append(route) return routes def get_urls(self): \"\"\" Use the registered viewsets to generate a list of URL patterns. \"\"\" ret = [] # 这里的self.registry是父类BaseRouter的一个属性 for prefix, viewset, basename in self.registry: lookup = self.get_lookup_regex(viewset) routes = self.get_routes(viewset) # 这里是对路径进行了不同形式的封装 for route in routes: # Only actions which actually exist on the viewset will be bound mapping = self.get_method_map(viewset, route.mapping) if not mapping: continue # Build the url pattern regex = route.url.format( prefix=prefix, lookup=lookup, trailing_slash=self.trailing_slash ) if not prefix and regex[:2] == '^/': regex = '^' + regex[2:] initkwargs = route.initkwargs.copy() initkwargs.update(&#123; 'basename': basename, 'detail': route.detail, &#125;) view = viewset.as_view(mapping, **initkwargs) name = route.name.format(basename=basename) ret.append(url(regex, view, name=name)) return ret 进入BaseRouter中,截取源码如下: 1234567891011121314151617181920212223242526272829303132333435class BaseRouter: def __init__(self): # SimpleRouter的get_urls中的需要的属性 self.registry = [] def register(self, prefix, viewset, basename=None): # 如果没有basename,则self.get_default_basename()方法 # get_default_basename()方法就是报错要求basename必须有, # 从而看得出来要想使用router组件必须执行register()方法 if basename is None: basename = self.get_default_basename(viewset) self.registry.append((prefix, viewset, basename)) # invalidate the urls cache if hasattr(self, '_urls'): del self._urls def get_default_basename(self, viewset): \"\"\" If `basename` is not specified, attempt to automatically determine it from the viewset. \"\"\" raise NotImplementedError('get_default_basename must be overridden') def get_urls(self): \"\"\" Return a list of URL patterns, given the registered viewsets. \"\"\" raise NotImplementedError('get_urls must be overridden') @property def urls(self): if not hasattr(self, '_urls'): self._urls = self.get_urls() return self._urls DefaultRoute类进行了更好的优化 123456789101112131415161718192021222324252627282930313233343536373839404142class DefaultRouter(SimpleRouter): \"\"\" The default router extends the SimpleRouter, but also adds in a default API root view, and adds format suffix patterns to the URLs. \"\"\" include_root_view = True include_format_suffixes = True root_view_name = 'api-root' default_schema_renderers = None APIRootView = APIRootView APISchemaView = SchemaView SchemaGenerator = SchemaGenerator def __init__(self, *args, **kwargs): if 'root_renderers' in kwargs: self.root_renderers = kwargs.pop('root_renderers') else: self.root_renderers = list(api_settings.DEFAULT_RENDERER_CLASSES) super().__init__(*args, **kwargs) def get_api_root_view(self, api_urls=None): \"\"\" Return a basic root view. \"\"\" api_root_dict = OrderedDict() list_name = self.routes[0].name for prefix, viewset, basename in self.registry: api_root_dict[prefix] = list_name.format(basename=basename) return self.APIRootView.as_view(api_root_dict=api_root_dict) def get_urls(self): urls = super().get_urls() if self.include_root_view: view = self.get_api_root_view(api_urls=urls) root_url = url(r'^$', view, name=self.root_view_name) urls.append(root_url) if self.include_format_suffixes: urls = format_suffix_patterns(urls) return urls 最后路由组件创建了怎么样的url?","categories":[{"name":"django-rest-framework","slug":"django-rest-framework","permalink":"https://liangweijiang.github.io/categories/django-rest-framework/"}],"tags":[{"name":"DRF","slug":"DRF","permalink":"https://liangweijiang.github.io/tags/DRF/"}]},{"title":"DRF源码解析2--viewsets.ModelViewSet对视图的封装","slug":"DRF-02","date":"2020-02-19T13:47:47.427Z","updated":"2020-02-19T15:08:42.664Z","comments":true,"path":"2020/02/19/DRF-02/","link":"","permalink":"https://liangweijiang.github.io/2020/02/19/DRF-02/","excerpt":"","text":"viewsetsdrf中 viewsets 对 view 进行了更加深层的封装,在CBV编程中减少代码的冗余,截取viewsets源码如下 12345678class ModelViewSet(mixins.CreateModelMixin, mixins.RetrieveModelMixin, mixins.UpdateModelMixin, mixins.DestroyModelMixin, mixins.ListModelMixin, GenericViewSet):# 可以看到viewsets中的ModelViewSet只是继承了各种类,每一个类对应CBV的每一个方法,如CreateModelMixin则对应post方法 pass 进入GenericViewSet的源码中,不难发现,GenericViewSet这个类主要是获取我们的数据和处理数据的组件 123class GenericViewSet(ViewSetMixin, generics.GenericAPIView): # GenericViewSet同样是继承了两个类 pass 进入generics.GenericAPIView 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657class GenericAPIView(views.APIView): # queryset就是获取的ORM对象 queryset = None # serializer_class是序列化的类 serializer_class = None lookup_field = 'pk' lookup_url_kwarg = None # The filter backend classes to use for queryset filtering filter_backends = api_settings.DEFAULT_FILTER_BACKENDS # The style to use for queryset pagination. # pagination_class就是和分页组件相关的类,默认为drf配置中的组件 pagination_class = api_settings.DEFAULT_PAGINATION_CLASS def get_queryset(self): # 这里断言就是为了说明在继承这个类时,一定要有queryset属性或覆盖这个方法 assert self.queryset is not None, ( \"'%s' should either include a `queryset` attribute, \" \"or override the `get_queryset()` method.\" % self.__class__.__name__ ) # 获取ORM对象 queryset = self.queryset if isinstance(queryset, QuerySet): # Ensure queryset is re-evaluated on each request. queryset = queryset.all() return queryset def get_serializer(self, *args, **kwargs): serializer_class = self.get_serializer_class() kwargs['context'] = self.get_serializer_context() # serializer_class接收的参数可能不同 return serializer_class(*args, **kwargs) def get_serializer_class(self): # 继承这个类时,一定要有serializer_classt属性或覆盖这个方法 assert self.serializer_class is not None, ( \"'%s' should either include a `serializer_class` attribute, \" \"or override the `get_serializer_class()` method.\" % self.__class__.__name__ ) return self.serializer_class def get_serializer_context(self): return &#123; 'request': self.request, 'format': self.format_kwarg, 'view': self &#125; def filter_queryset(self, queryset): for backend in list(self.filter_backends): queryset = backend().filter_queryset(self.request, queryset, self) return queryset 接着回到ModelViewSet中,进入到mixins.CreateModelMixin的源码中 1234567891011121314151617181920class CreateModelMixin: \"\"\" Create a model instance. \"\"\" def create(self, request, *args, **kwargs): # 这里调用的就是GenericAPIView中的get_serializer serializer = self.get_serializer(data=request.data) serializer.is_valid(raise_exception=True) self.perform_create(serializer) headers = self.get_success_headers(serializer.data) return Response(serializer.data, status=status.HTTP_201_CREATED, headers=headers) def perform_create(self, serializer): serializer.save() def get_success_headers(self, data): try: return &#123;'Location': str(data[api_settings.URL_FIELD_NAME])&#125; except (TypeError, KeyError): return &#123;&#125; 现在我们只需要继承ModelViewSet,然后重写queryset和处理数据所需要的组件类如序列化的类等,但是怎么将self.post对应上self.list方法呢,GenericViewSet还继承了ViewSetMixin,看一下ViewSetMixin的源码的截取 1234567891011121314151617181920212223242526272829303132333435class ViewSetMixin: @classonlymethod # 对as_view()方法又一次封装,action表示可以接收参数了 def as_view(cls, actions=None, **initkwargs): if not actions: raise TypeError(\"The `actions` argument must be provided when \" \"calling `.as_view()` on a ViewSet. For example \" \"`.as_view(&#123;'get': 'list'&#125;)`\") def view(request, *args, **kwargs): self = cls(**initkwargs) # We also store the mapping of request methods to actions, # so that we can later set the action attribute. # eg. `self.action = 'list'` on an incoming GET request. self.action_map = actions # Bind methods to actions # This is the bit that's different to a standard view # method, action退出action为一个字典,&#123;'get':'list'&#125; for method, action in actions.items(): # handler为实例中的各个方法 handler = getattr(self, action) # 这里setattr相当于metmod == handler setattr(self, method, handler) if hasattr(self, 'get') and not hasattr(self, 'head'): self.head = self.get self.request = request self.args = args self.kwargs = kwargs # And continue as usual return self.dispatch(request, *args, **kwargs) return csrf_exempt(view) 奉献一张图来看下我们的继承顺序","categories":[{"name":"django-rest-framework","slug":"django-rest-framework","permalink":"https://liangweijiang.github.io/categories/django-rest-framework/"}],"tags":[{"name":"DRF","slug":"DRF","permalink":"https://liangweijiang.github.io/tags/DRF/"}]},{"title":"DRF源码解析1--APIView对request的重新封装","slug":"DRF-01","date":"2020-02-19T08:58:37.586Z","updated":"2020-02-19T10:08:43.094Z","comments":true,"path":"2020/02/19/DRF-01/","link":"","permalink":"https://liangweijiang.github.io/2020/02/19/DRF-01/","excerpt":"","text":"截取APIView12345678910111213141516171819202122232425262728293031323334353637383940414243444546@classmethodclass APIView(View): def as_view(cls, **initkwargs): if isinstance(getattr(cls, 'queryset', None), models.query.QuerySet): def force_evaluation(): raise RuntimeError( 'Do not evaluate the `.queryset` attribute directly, ' 'as the result will be cached and reused between requests. ' 'Use `.all()` or call `.get_queryset()` instead.' ) cls.queryset._fetch_all = force_evaluation # 继承了django中View类的的as_view()方法,然后重写了dispatch方法 view = super().as_view(**initkwargs) view.cls = cls view.initkwargs = initkwargs return csrf_exempt(view) def dispatch(self, request, *args, **kwargs): self.args = args self.kwargs = kwargs # 在传入request是,apiview对request进行了一次包装 request = self.initialize_request(request, *args, **kwargs) # 此时self.request是initialize_request方法返回的Request self.request = request self.headers = self.default_response_headers # deprecate? try: # 相比于View类的dispatch方法,Apiview在这里进行了一次初始化的方法 self.initial(request, *args, **kwargs) # Get the appropriate handler method if request.method.lower() in self.http_method_names: handler = getattr(self, request.method.lower(), self.http_method_not_allowed) else: handler = self.http_method_not_allowed response = handler(request, *args, **kwargs) except Exception as exc: response = self.handle_exception(exc) self.response = self.finalize_response(request, response, *args, **kwargs) return self.response 进入self.initalize_request()12345678910def initialize_request(self, request, *args, **kwargs): parser_context = self.get_parser_context(request) # 可以看到, request已经被封装成Request返回,而request则被封装成Request的一个属性 return Request( request, parsers=self.get_parsers(), authenticators=self.get_authenticators(), negotiator=self.get_content_negotiator(), parser_context=parser_context ) 截取Ruquest123456789101112131415161718192021222324252627282930class Request: def __init__(self, request, parsers=None, authenticators=None, negotiator=None, parser_context=None): assert isinstance(request, HttpRequest), ( 'The `request` argument must be an instance of ' '`django.http.HttpRequest`, not `&#123;&#125;.&#123;&#125;`.' .format(request.__class__.__module__, request.__class__.__name__) ) # request被封装了私有属性 self._request = request @property def content_type(self): meta = self._request.META # 封装了content_type方法获取传输类型 return meta.get('CONTENT_TYPE', meta.get('HTTP_CONTENT_TYPE', '')) @property def query_params(self): \"\"\" More semantically correct name for request.GET. \"\"\" # django的view中的request.Get属性封装成了query_params属性 return self._request.GET @property def data(self): if not _hasattr(self, '_full_data'): self._load_data_and_files() # django的view中的request.Post属性封装成了data属性 return self._full_data 我们可以看出~CBV在内部做了一个分发,本质和FBV是一样的。 以后做接口开发的时候，就要用CBV，学习了restful规范，现在就很容易理解我们为什么用CBV了。","categories":[{"name":"django-rest-framework","slug":"django-rest-framework","permalink":"https://liangweijiang.github.io/categories/django-rest-framework/"}],"tags":[{"name":"DRF","slug":"DRF","permalink":"https://liangweijiang.github.io/tags/DRF/"}]}]}